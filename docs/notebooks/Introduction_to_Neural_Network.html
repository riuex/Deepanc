

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3. ニューラルネットワークの基礎 &mdash; メディカルAI専門コース オンライン講義資料  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="4. Deep Learningフレームワークの基礎" href="Introduction_to_Chainer.html" />
    <link rel="prev" title="2. 機械学習ライブラリの基礎" href="Introduction_to_ML_libs.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 機械学習ライブラリの基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. ニューラルネットワークの基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ニューラルネットワークの構造">3.1. ニューラルネットワークの構造</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#線形変換">3.1.1. 線形変換</a></li>
<li class="toctree-l3"><a class="reference internal" href="#非線形変換">3.1.2. 非線形変換</a></li>
<li class="toctree-l3"><a class="reference internal" href="#数値を見ながら計算の流れを確認">3.1.3. 数値を見ながら計算の流れを確認</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#目的関数">3.2. 目的関数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#平均二乗誤差">3.2.1. 平均二乗誤差</a></li>
<li class="toctree-l3"><a class="reference internal" href="#交差エントロピー">3.2.2. 交差エントロピー</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#補足：交差エントロピーについて">3.2.2.1. 補足：交差エントロピーについて</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ニューラルネットワークの最適化">3.3. ニューラルネットワークの最適化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#パラメータ更新量の算出">3.3.1. パラメータ更新量の算出</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#パラメータ-{\bf-w}_2-の更新量">3.3.1.1. パラメータ <span class="math notranslate nohighlight">\({\bf w}_2\)</span> の更新量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#学習率（learning-rate）について">3.3.1.2. 学習率（learning rate）について</a></li>
<li class="toctree-l4"><a class="reference internal" href="#パラメータ-{\bf-w}_1-の更新量">3.3.1.3. パラメータ <span class="math notranslate nohighlight">\({\bf w}_1\)</span> の更新量</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#誤差逆伝播法（バックプロパゲーション）">3.4. 誤差逆伝播法（バックプロパゲーション）</a></li>
<li class="toctree-l2"><a class="reference internal" href="#勾配消失">3.5. 勾配消失</a></li>
<li class="toctree-l2"><a class="reference internal" href="#「レイヤ」が指すもの">3.6. 「レイヤ」が指すもの</a></li>
<li class="toctree-l2"><a class="reference internal" href="#様々なレイヤタイプ">3.7. 様々なレイヤタイプ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#畳み込み層（convolution-layer）">3.7.1. 畳み込み層（convolution layer）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#プーリング層（pooling-layer）">3.7.2. プーリング層（pooling layer）</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 実践編: ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>3. ニューラルネットワークの基礎</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Introduction_to_Neural_Network.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Introduction_to_Neural_Network.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="ニューラルネットワークの基礎">
<h1>3. ニューラルネットワークの基礎<a class="headerlink" href="#ニューラルネットワークの基礎" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>ここでは，ニューラルネットワーク (Neural Network)
についてその概要を紹介していきます．画像認識などに用いられる
Convolutional Neural Network (CNN) や，自然言語処理などに用いられる
Recurrent Neural Network (RNN)
といった手法は，ニューラルネットワークの一種です．</p>
<p>ここではまず，最もシンプルな全結合型と呼ばれるニューラルネットワークの構造について説明を行ったあと，複数の入力データと望ましい出力の組からなる学習用データセットを準備したとき，どうやってニューラルネットワークを学習させればよいのか（教師あり学習の仕組み）について解説を行います．</p>
<p>ニューラルネットワークによって表現される複雑な関数を，現実的な時間で学習するための誤差逆伝播法（バックプロパゲーション）と呼ばれるアルゴリズムについても紹介します．</p>
<p>まずはニューラルネットワークをブラックボックスとして扱ってしまうのではなく，一つ一つ内部で行われる計算を丁寧に調べます．そして，パラメータで特徴づけられた関数で表される線形変換とそれに続く非線形変換を組み合わせて，全体として微分可能な一つの関数を表していることを理解していきます．</p>
<div class="section" id="ニューラルネットワークの構造">
<h2>3.1. ニューラルネットワークの構造<a class="headerlink" href="#ニューラルネットワークの構造" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まずはニューラルネットワークの構造を図式化して見てみましょう．入力変数が{年数，アルコール度数，色合い，匂い}の4変数，出力変数が{白ワイン，赤ワイン}の2変数の場合を示します．</p>
<div class="figure" id="id16">
<img alt="ニューラルネットワークの基本構造" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/01.png" />
<p class="caption"><span class="caption-text">ニューラルネットワークの基本構造</span></p>
</div>
<p>この図のひとつひとつの丸い部分のことを<strong>ノード</strong>もしくは<strong>ユニット</strong>と呼び，その縦方向の集まりを<strong>層</strong>と呼びます．そして，一番初めの層を<strong>入力層（input
layer）</strong>，最後の層を<strong>出力層（output
layer）</strong>，そしてその間を<strong>中間層（intermediate
layer）</strong>もしくは<strong>隠れ層（hidden
layer）</strong>と呼びます．このモデルは入力層，中間層，出力層の３層の構造となっていますが，中間層の数を増やすことでさらに多層のニューラルネットワークを定義することもできます．この例では各層間の全てのノードが互いに結合されているため，<strong>全結合型のニューラルネットワーク</strong>とも呼び，ニューラルネットワークの最も基礎的な構造です．</p>
<p>入力変数は前章までと同様ですが，出力変数の扱い方がこれまでと異なります．例えば，上図では出力層の各ノードがそれぞれ白ワインと赤ワインに対応しており，カテゴリの数だけ出力の変数があるということになります．なぜこのような構造となっているのでしょうか．</p>
<div class="figure" id="id17">
<img alt="ニューラルネットワークの出力値" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/02.png" />
<p class="caption"><span class="caption-text">ニューラルネットワークの出力値</span></p>
</div>
<p>まず，最終層にどのような値が入るのか，具体例を見てみましょう．例えば，年数が3年物でアルコール度数が14度，色合いが0.2，匂いが0.8で表されるワインがあるとします．内部の計算は後述するとして，このようなデータをニューラルネットワークに与えたときに結果として得られる値に着目してみましょう．上図では，白ワイン
<span class="math notranslate nohighlight">\(y_{1} = 0.15\)</span>, 赤ワイン <span class="math notranslate nohighlight">\(y_{2}= 0.85\)</span>
となっています．このとき，出力値の中で最も大きな値となっている変数に対応するクラス，すなわち今回の例では「赤ワイン」をこの分類問題におけるこのニューラルネットワークの<strong>予測結果</strong>とすることができます．</p>
<p>ここで出力層のすべての値を合計してみると，1になっていることに気づきます．これは偶然ではなく，そうなるように出力層の値を計算しているためです*
．つまり，出力層のそれぞれのノードが持つ数値は，入力がそれぞれのクラスに属している確率を表していたのでした．そのため，カテゴリ数と同じ数だけ出力層にはノードが必要となります．</p>
<p>それでは，ここからニューラルネットワークの内部で行われる計算を詳しく見ていきましょう．ニューラルネットワークの各層は，前の層の値に線形変換と非線形変換を順番に施すことで計算されています．まずは，ここで言う線形変換とは何を表すのか，から見ていきましょう．</p>
<p>*
具体的には，Softmax関数という活性化関数（これも後述します）をニューラルネットワークの出力ベクトルに適用することで，出力層における全ノードの値の合計が1になるようにします．</p>
<div class="section" id="線形変換">
<h3>3.1.1. 線形変換<a class="headerlink" href="#線形変換" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここでは，ニューラルネットワークの各層で行われる線形変換について説明します．</p>
<div class="figure" id="id18">
<img alt="ニューラルネットワークの線形変換" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/linear_transformation.png" />
<p class="caption"><span class="caption-text">ニューラルネットワークの線形変換</span></p>
</div>
<p>ここで言う線形変換* とは，重み行列 (<span class="math notranslate nohighlight">\(w\)</span>) <span class="math notranslate nohighlight">\(\times\)</span>
入力ベクトル (<span class="math notranslate nohighlight">\(h\)</span>) <span class="math notranslate nohighlight">\(+\)</span> バイアスベクトル (<span class="math notranslate nohighlight">\(b\)</span>)
のような計算のことを指しています．このとき，この変換の入力が<span class="math notranslate nohighlight">\(h\)</span>，パラメータが<span class="math notranslate nohighlight">\(w\)</span>と<span class="math notranslate nohighlight">\(b\)</span>となります．ここでの掛け算（<span class="math notranslate nohighlight">\(\times\)</span>）は行列の掛け算であることに注意してください．また，これからは，<span class="math notranslate nohighlight">\(h\)</span>
が文字としてよく登場しますが，これは隠れ層 (hidden layer) の頭文字である
<span class="math notranslate nohighlight">\(h\)</span>
から来ています．ただし，表記を簡潔にするため以下では入力層（上図における
<span class="math notranslate nohighlight">\(x_1, x_2, x_3, x_4\)</span>）を<strong>0層目の隠れ層と考える</strong>ことにして，
<span class="math notranslate nohighlight">\(h_{01}, h_{02}, h_{03}, h_{04}\)</span>
と表記します．では上図で表される計算を数式で記述してみましょう．</p>
<p>（* 通常数学では線形変換とは <span class="math notranslate nohighlight">\({\bf w} \times {\bf h}\)</span>
のことを指し，この変換は厳密には「アファイン変換（もしくは
アフィン変換）」と呼ばれるものです．しかし，深層学習の文脈ではこの変換も線形変換と呼ぶことも多いです．）</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
u_{11}&amp;=w_{11}h_{01}+w_{12}h_{02}+w_{13}h_{03}+w_{14}h_{04}+b_{1} \\
u_{12}&amp;=w_{21}h_{01}+w_{22}h_{02}+w_{23}h_{03}+w_{24}h_{04}+b_{2} \\
u_{13}&amp;=w_{31}h_{01}+w_{32}h_{02}+w_{33}h_{03}+w_{34}h_{04}+b_{3}
\end{aligned}\end{split}\]</div>
<p>バイアス（<span class="math notranslate nohighlight">\(b_1, b_2, b_3\)</span>）は上図では省略されていることに注意してください．さて，以上の4つの式は，ベクトルと行列の計算として以下のように書き直すことができます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
u_{11} \\
u_{12} \\
u_{13}
\end{bmatrix}&amp;=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} \\
w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} \\
w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}
\end{bmatrix}\begin{bmatrix}
h_{01} \\
h_{02} \\
h_{03} \\
h_{04}
\end{bmatrix}+\begin{bmatrix}
b_{1} \\
b_{2} \\
b_{3}
\end{bmatrix}\\
{\bf u}_{1}&amp;={\bf W}{\bf h}_{0}+{\bf b}
\end{aligned}\end{split}\]</div>
<p>本来は <span class="math notranslate nohighlight">\({\bf W}\)</span> や <span class="math notranslate nohighlight">\({\bf b}\)</span>
にも，どの層とどの層の間の計算に用いられるものなのかを表す添え字をつけるべきですが，ここでは簡単のため省略しています．</p>
</div>
<div class="section" id="非線形変換">
<h3>3.1.2. 非線形変換<a class="headerlink" href="#非線形変換" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>次に，非線形変換について説明します．線形変換のみでは，下図右のように入力と出力の間が非線形な関係である場合，両者の間の関係を適切に表現することができません．</p>
<div class="figure" id="id19">
<img alt="入力と出力の関係" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/04.png" />
<p class="caption"><span class="caption-text">入力と出力の関係</span></p>
</div>
<p>そこで，ニューラルネットワークでは各層で線形変換に引き続いて非線形変換を施すことで，全体の関数が非線形性を持つようにしています．この非線形変換を行う関数を，ニューラルネットワークの文脈においては
<strong>活性化関数</strong> と呼びます．</p>
<p>上図の線形変換の結果 <span class="math notranslate nohighlight">\(u_{11}, u_{12}, u_{13}\)</span>
に活性化関数を使って非線形変換を行った結果を
<span class="math notranslate nohighlight">\(h_{11}, h_{12}, h_{13}\)</span>
と書き，これらを活性値（activation）と呼びます（下図参照）．これが次の層への入力となります．</p>
<div class="figure" id="id20">
<img alt="活性値" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/activation.png" />
<p class="caption"><span class="caption-text">活性値</span></p>
</div>
<p>活性化関数の具体例としては，下図に示す
<strong>ロジスティックシグモイド関数</strong>（以下シグモイド関数）</p>
<div class="figure" id="id21">
<img alt="シグモイド関数" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/05.png" />
<p class="caption"><span class="caption-text">シグモイド関数</span></p>
</div>
<p>が従来，よく用いられてきました．しかし近年，層数が多いニューラルネットワークではシグモイド関数は活性化関数としてほとんど用いられていません．その理由の一つは，シグモイド関数を活性化関数に採用することで
<strong>勾配消失</strong>
という現象が起きやすくなり，学習が進行しなくなる問題が発生することがあったためです．これは後で詳述します．これを回避するために，<strong>Rectified
Linear Unit (ReLU)</strong>
という関数がよく用いられます．これは，以下のような形をした関数です．</p>
<div class="figure" id="id22">
<img alt="ReLU関数" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/06.png" />
<p class="caption"><span class="caption-text">ReLU関数</span></p>
</div>
<p>ここで， <span class="math notranslate nohighlight">\({\rm max}(0, u)\)</span>
は，<span class="math notranslate nohighlight">\(0\)</span>と<span class="math notranslate nohighlight">\(u\)</span>を比べて大きな方を返す関数です．すなわち，ReLUは入力が負の値の場合には出力は0で一定であり，正の値の場合は入力をそのまま出力するという関数です．シグモイド関数では，入力が小さな，もしくは大きな値をとった際に，勾配がどんどん小さくなってしまうだろうことがプロットからも見て取れます．それに対し，ReLU関数は入力の値がいくら大きくなっても，一定の勾配が発生します．これがのちほど紹介する勾配消失という問題に有効に働きます．</p>
</div>
<div class="section" id="数値を見ながら計算の流れを確認">
<h3>3.1.3. 数値を見ながら計算の流れを確認<a class="headerlink" href="#数値を見ながら計算の流れを確認" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここで，下図に書き込まれた具体的な数値を使って，入力
<span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span> から出力 <span class="math notranslate nohighlight">\(y\)</span>
が計算される過程を確認してみましょう．今は計算を簡略化するためバイアス
<span class="math notranslate nohighlight">\({\bf b}\)</span>
の計算は省略します（バイアスが全て0であるとします）．数値例として，<span class="math notranslate nohighlight">\({\bf x} = \begin{bmatrix} 2 &amp; 3 &amp; 1 \end{bmatrix}^T\)</span>
が与えられた時の出力 <span class="math notranslate nohighlight">\(y\)</span>
の計算手順を一つ一つ追いかけてみましょう．</p>
<div class="figure" id="id23">
<img alt="出力までの計算例" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/output.png" />
<p class="caption"><span class="caption-text">出力までの計算例</span></p>
</div>
<p>前章で解説した重回帰分析では，目的関数のパラメータについての導関数を0とおいて解析的に最適なパラメータを計算できましたが，ニューラルネットワークでは一般的に，解析的にパラメータを解くことはできません．その代わり，この導関数の値（勾配）を利用した別の方法でパラメータを逐次的に最適化していきます．</p>
<p>このため，ニューラルネットワークの場合は，<strong>まずパラメータを乱数で初期化し，ひとまずデータを入力して目的関数の値を計算します</strong>．次にその関数の勾配を計算して，それを利用してパラメータを更新し，その更新後の新しいパラメータを使って再度入力データを処理して目的関数の値を計算し…といったことを繰り返し行っていくことになります．</p>
<p>今，パラメータを初期化した結果，上の図のグラフの枝に与えられているような数値になった状態で，入力層の値に線形変換を施すところまでを考えてみましょう．この計算は，以下のようになります．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
u_{11}&amp;=3\times 2+1\times 3+2\times 1=11\\
u_{12}&amp;=-2\times 2-3\times 3-1\times 1=-11
\end{aligned}\end{split}\]</div>
<p>次に非線形変換を行う活性化関数としてReLU関数を採用し，以下のように中間層の値を計算してみましょう．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h_{11} &amp;= \max(0, 11) = 11 \\
h_{12} &amp;= \max(0, -11)  = 0
\end{aligned}\end{split}\]</div>
<p>同様に，出力層の <span class="math notranslate nohighlight">\(y\)</span> の値までを計算すると，</p>
<div class="math notranslate nohighlight">
\[y = 3 \times 11 + 2 \times 0 = 33\]</div>
<p>となります．</p>
<p>さて，次節からは，パラメータを，どうやって更新していくかを見てみましょう．</p>
</div>
</div>
<div class="section" id="目的関数">
<h2>3.2. 目的関数<a class="headerlink" href="#目的関数" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ニューラルネットワークでも，微分可能でさえあれば解きたいタスクに合わせて様々な目的関数を利用することができます．</p>
<div class="section" id="平均二乗誤差">
<h3>3.2.1. 平均二乗誤差<a class="headerlink" href="#平均二乗誤差" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>例えば，出力層に<span class="math notranslate nohighlight">\(N\)</span>個の値を持つニューラルネットワークで回帰問題を解く場合を考えてみましょう．<span class="math notranslate nohighlight">\(N\)</span>個の出力それぞれ（<span class="math notranslate nohighlight">\(y_n (n=1, 2, \dots, N)\)</span>）に対して望ましい出力（<span class="math notranslate nohighlight">\(t_n (n=1, 2, \dots, N)\)</span>）が与えられたとき，目的関数をそれぞれの出力（<span class="math notranslate nohighlight">\(y_n\)</span>）と対応する正解（<span class="math notranslate nohighlight">\(t_n\)</span>）の間の
<strong>平均二乗誤差（mean squared error）</strong>
とすることで，回帰問題を解くことができます．</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \dfrac{1}{N} \sum_{n=1}^{N}(t_{n} - y_{n})^{2}\]</div>
<p>これを最小にするようにニューラルネットワーク中のパラメータを決定するわけです．例えば，上図の例で正解として
<span class="math notranslate nohighlight">\(t = 20\)</span> が与えられたときの目的関数の値は，</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \dfrac{1}{1} (20 - 33)^2 = 169\]</div>
<p>です．これを小さくするような重み行列の値を探せばよいということです．</p>
</div>
<div class="section" id="交差エントロピー">
<h3>3.2.2. 交差エントロピー<a class="headerlink" href="#交差エントロピー" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>一方，分類問題の場合はしばしば <strong>交差エントロピー（cross entropy）</strong>
が目的関数として利用されます．</p>
<p>例として，<span class="math notranslate nohighlight">\(N\)</span>クラスの分類問題を考えてみましょう．ある入力
<span class="math notranslate nohighlight">\(x\)</span> が与えられたとき，ニューラルネットワークの出力層に <span class="math notranslate nohighlight">\(N\)</span>
個のノードがあり，それぞれがこの入力が <span class="math notranslate nohighlight">\(n\)</span>
番目のクラスに属する確率 <span class="math notranslate nohighlight">\(y_n = p(y=n|x)\)</span>
を表しているとします．これは，入力 <span class="math notranslate nohighlight">\(x\)</span>
が与えられたという条件のもとで，予測クラスを意味する <span class="math notranslate nohighlight">\(y\)</span> が
<span class="math notranslate nohighlight">\(n\)</span> であるような確率ということです．</p>
<p>ここで，<span class="math notranslate nohighlight">\(x\)</span> が所属するクラスについての正解が，
<span class="math notranslate nohighlight">\({\bf t} = \begin{bmatrix} t_1 &amp; t_2 &amp; \dots &amp; t_N \end{bmatrix}^T\)</span>
というベクトルで与えられているとします．ただし，このベクトルは
<span class="math notranslate nohighlight">\(t_n (n=1, 2, \dots, N)\)</span>
のいずれか1つだけが1であり，それ以外は0であるようなベクトルであるとします．
これを <strong>1-hotベクトル</strong>
と呼びます．そして，この1つだけ値が1となっている要素は，その要素のインデックスに対応したクラスが正解であることを意味します．例えば，<span class="math notranslate nohighlight">\(t_3 = 1\)</span>であれば3というインデックスに対応するクラスが正解であるということになります．</p>
<p>さて，このような準備を行うと，交差エントロピーは以下のように計算できるものとして記述することができます．</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = - \frac{1}{N} \sum_{n=1}^{N}t_{n}\log y_{n}\]</div>
<div class="section" id="補足：交差エントロピーについて">
<h4>3.2.2.1. 補足：交差エントロピーについて<a class="headerlink" href="#補足：交差エントロピーについて" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>以下は，交差エントロピーの定義について知りたい方だけ参考にしてください．情報理論などで交差エントロピーの定義を知っている方は上の式で表されるものが交差エントロピーとは違うようにみえるかもしれません．しかしこれは，以下のように説明できます．今，<span class="math notranslate nohighlight">\(q(y|x)\)</span>をニューラルネットワークのモデルが定義する条件付き確率とし，<span class="math notranslate nohighlight">\(p(y|x)\)</span>を実データの条件付き確率とします．ここで，<span class="math notranslate nohighlight">\(p(y|x)\)</span>は実際には未知であるため，代わりに学習データの経験分布</p>
<div class="math notranslate nohighlight">
\[\hat{p}(y|x) = \frac{1}{N} \sum_{n=1}^N I(x =x_n, y=y_n)\]</div>
<p>を用いることとします．ただし<span class="math notranslate nohighlight">\(I\)</span>はディラック関数とよばれ，その等号が成立する時，値が<span class="math notranslate nohighlight">\(\infty\)</span>，それ以外では<span class="math notranslate nohighlight">\(0\)</span>であるような関数で，その定義域全体にわたる積分は1になるものです．この時，確率分布
<span class="math notranslate nohighlight">\(\hat{p}(y|x)\)</span> と <span class="math notranslate nohighlight">\(q(y|x)\)</span> の間の交差エントロピーは</p>
<div class="math notranslate nohighlight">
\[\int_{x, y} \hat{p}(y|x) \log \frac{q(y|x)}{\hat{p}(y|x)}   dx dy\]</div>
<p>と定義されます．ここでディラックのデルタ関数の定義を用い，また<span class="math notranslate nohighlight">\(\hat{p}\)</span>だけに依存する項を除くと，先程の交差エントロピーの目的関数が導出されます．</p>
</div>
</div>
</div>
<div class="section" id="ニューラルネットワークの最適化">
<h2>3.3. ニューラルネットワークの最適化<a class="headerlink" href="#ニューラルネットワークの最適化" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>目的関数の値を最小にするようなパラメータの値を決定することが，ニューラルネットワークの学習の目的であるとわかりました．では，どのようにしてそのパラメータを探し当てればよいのでしょうか．ある目的関数が与えられたもとで，その目的関数が望ましい値をとるようにニューラルネットワークのパラメータを決定することを，ニューラルネットワークの最適化といいます．</p>
<p>まず最適化の方法を考える前に，まず最適化の対象とはなんであったか，再度確認しましょう．「ニューラルネットワークを最適化する」とは，すなわち「ニューラルネットワークが内部で用いている全てのパラメータの値を適切に決定する」という意味です．では，ニューラルネットワークにおけるパラメータとは，何だったでしょうか．それは，ここまで紹介したシンプルな全結合型ニューラルネットワークの場合，各層の線形変換に用いられていた
<span class="math notranslate nohighlight">\({\bf W}\)</span> と <span class="math notranslate nohighlight">\({\bf b}\)</span> のことを指します．</p>
<p>ニューラルネットワークの各パラメータを，目的関数に対する勾配を0とおいて解析的に解くことは，一般的には困難です．しかし，実データをニューラルネットワークに入力すれば，その入力の値における目的関数の勾配を数値的に求めることは可能です．この値が分かれば，パラメータをどのように変化させれば目的関数の値を小さくすることができるのかが分かります．そこで，この勾配を使ってパラメータを繰り返し少しずつ更新していくことで，ニューラルネットワークの最適化を行うことができるのです．この方法について順を追って考えていきましょう．</p>
<p>まず，以下の図を見てください．図中の点線は，パラメータ <span class="math notranslate nohighlight">\(w\)</span>
を変化させた際の目的関数 <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
の値を表しています．この例では簡単のため二次関数の形になっていますが，ニューラルネットワークの目的関数は実際には多次元で，かつもっと複雑な形をしていることがほとんどでしょう．しかし，ここでは説明のためこのようにシンプルな形を想像してみましょう．さて，この目的関数が最小値を与えるような
<span class="math notranslate nohighlight">\(w\)</span> は，どのようにして発見できるでしょうか．</p>
<div class="figure" id="id24">
<img alt="パラメータと目的関数の関係（イメージ）" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/13.png" />
<p class="caption"><span class="caption-text">パラメータと目的関数の関係（イメージ）</span></p>
</div>
<p>前節で説明したように，ニューラルネットワークのパラメータはまず乱数で初期化されます．ここでは，例として
<span class="math notranslate nohighlight">\(w=3\)</span>
という初期化が行われたと考えてみましょう．そうすると，<span class="math notranslate nohighlight">\(w=3\)</span>における<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>の勾配
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w}\)</span>
が求まります．ニューラルネットワークの目的関数は，全てのパラメータについて微分可能である*
ためです．さて，ここでは仮に <span class="math notranslate nohighlight">\(w=3\)</span> における
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w}\)</span> が <span class="math notranslate nohighlight">\(3\)</span>
であったとしましょう（このことを<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w} |_{w=3} = 3\)</span>と書きます）．すると，以下の図のように，この
<span class="math notranslate nohighlight">\(3\)</span> という値は <span class="math notranslate nohighlight">\(w=3\)</span> における <span class="math notranslate nohighlight">\(\mathcal{L}(w)\)</span>
という関数の接線の傾き（勾配; gradient）を表しています．</p>
<p>（*
厳密には損失関数に微分不可能な点が存在する可能性はあります．例えばReLUは
<span class="math notranslate nohighlight">\(x=0\)</span>
の点で微分不可能なため，ReLUを含んだニューラルネットワークには微分不可能な点があります．しかし，通常使うニューラルネットワークの場合，そのような微分不可能な点はわずかしかないため，以下に説明する最適化の方法の中では，無視できます．）</p>
<div class="figure" id="id25">
<img alt="目的関数の接線の傾き" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/11.png" />
<p class="caption"><span class="caption-text">目的関数の接線の傾き</span></p>
</div>
<p>傾きとは，<span class="math notranslate nohighlight">\(w\)</span> を増加させた際に <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
が増加する方向を意味しているので，今は <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
の値を小さくしたいわけですから，この傾きの逆方向へ <span class="math notranslate nohighlight">\(w\)</span>
を変化させる，すなわち <span class="math notranslate nohighlight">\(w\)</span> <strong>から</strong>
<span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial w\)</span> <strong>を引けばよい</strong>
ことになります．</p>
<p>これがニューラルネットワークのパラメータを目的関数の勾配を用いて更新していく際の基本的な考え方です．このときの
<span class="math notranslate nohighlight">\(w\)</span> のステップサイズ（更新量）のスケールを調整するために，勾配に
<strong>学習率 (learning rate)</strong> と呼ばれる値を乗じるのが一般的です．</p>
<p>例えば，今学習率を <span class="math notranslate nohighlight">\(0.5\)</span>
に設定してみます．そうすると，<span class="math notranslate nohighlight">\(w\)</span>の更新量は <strong>学習率</strong>
<span class="math notranslate nohighlight">\(\times\)</span> <strong>勾配</strong> で決まるので，<span class="math notranslate nohighlight">\(0.5 \times 3 = 1.5\)</span>
となります．現在 <span class="math notranslate nohighlight">\(w=3\)</span> なので，<strong>この値を引いて</strong>
<span class="math notranslate nohighlight">\(w \leftarrow w - 1.5\)</span> と更新した後は， <span class="math notranslate nohighlight">\(w=1.5\)</span>
となります．上の図は，この1度の更新を行ったあとの状態を表しています．</p>
<p>1度目の更新を行って，<span class="math notranslate nohighlight">\(w\)</span> が <span class="math notranslate nohighlight">\(w = 1.5\)</span>
の位置に移動しました．そこで，再度この点においても勾配を求めてみます．今度は
<span class="math notranslate nohighlight">\(-1\)</span> になっていたとしましょう．すると <strong>学習率</strong> <span class="math notranslate nohighlight">\(\times\)</span>
<strong>勾配</strong> は <span class="math notranslate nohighlight">\(0.5 \times -1 = -0.5\)</span>
となります．これを再び用いて，<span class="math notranslate nohighlight">\(w \leftarrow w - (-0.5)\)</span>
と2度目の更新を行うと，今度は <span class="math notranslate nohighlight">\(w = 2\)</span>
の位置にくるでしょう．このようにして，2回更新したあとは，以下の図のようになります．</p>
<div class="figure" id="id26">
<img alt="パラメータの更新" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/12.png" />
<p class="caption"><span class="caption-text">パラメータの更新</span></p>
</div>
<p>徐々に<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>が最小値をとるときの<span class="math notranslate nohighlight">\(w\)</span>の値に近づいていっていることが見て取れます．</p>
<p>こうして，<strong>学習率</strong> <span class="math notranslate nohighlight">\(\times\)</span> <strong>勾配</strong>
を更新量としてパラメータを変化させていくと，パラメータ <span class="math notranslate nohighlight">\(w\)</span>
を求めたい <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> の最小値を与える <span class="math notranslate nohighlight">\(w\)</span>
に徐々に近づけていくことができます．このような勾配を用いた目的関数の最小化手法を
<strong>勾配降下法</strong> と呼びます．ニューラルネットワークは，基本的に
<strong>微分可能な関数のみを層間をつなぐ関数として用いて</strong>
設計されるため，登場する関数はすべて微分可能であり，学習データセットを用いて勾配降下法によってパラメータを最適化する方法が適用可能なのです．</p>
<p>ただし，通常ニューラルネットワークを勾配降下法で最適化する場合は，データを一つ一つ用いてパラメータを更新するのではなく，いくつかのデータをまとめて入力し，それぞれの勾配を計算したあと，その勾配の平均値を用いてパラメータの更新を行う方法がよく行われます．これを
<strong>ミニバッチ学習</strong>
と呼びます．これは，学習データセットから一様ランダムに <span class="math notranslate nohighlight">\(k (&gt;0)\)</span>
個のデータを抽出し，その <span class="math notranslate nohighlight">\(k\)</span>
個のデータに対する目的関数の平均の値を小さくするようパラメータを更新することを，異なる
<span class="math notranslate nohighlight">\(k\)</span>
個のデータの組み合わせに対して繰り返し行う方法です．結果的にはデータセットに含まれる全てのデータを使用していきますが，1度の更新に用いるデータは
<span class="math notranslate nohighlight">\(k\)</span>
個ずつということになります．実際の実装では，データセット内のサンプルのインデックスをまずランダムにシャッフルして並べた配列を作り，その配列の先頭から
<span class="math notranslate nohighlight">\(k\)</span>
個ずつインデックスを取り出し，対応するデータを使ってミニバッチを構成します．こうして，全てのインデックスを使い切ること，すなわちデータセット内のデータを1度ずつ，すべてパラメータ更新に用い終えることを
<strong>1エポックの学習</strong> と呼びます．そして，この <span class="math notranslate nohighlight">\(k\)</span>
をバッチサイズもしくはミニバッチサイズと呼び，このような学習方法を指して，<strong>確率的勾配降下法
(SGD: Stocastic Gradient Descent)</strong>
という名前が用いられます．現在ほとんどすべてのニューラルネットワークの最適化手法はこのSGDをベースとした手法となっています．SGDを用いると，全体の計算時間が劇的に少なくできるだけでなく，下図のように目的関数が凸関数でなかったとしても，適当な条件のもとで“ほとんど確実に”局所最適解に収束することが知られています．</p>
<div class="figure" id="id27">
<img alt="局所最適解と大域最適解" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/14.png" />
<p class="caption"><span class="caption-text">局所最適解と大域最適解</span></p>
</div>
<div class="section" id="パラメータ更新量の算出">
<h3>3.3.1. パラメータ更新量の算出<a class="headerlink" href="#パラメータ更新量の算出" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは今，下図のような3層の全結合型ニューラルネットワークを考え，1層目と2層目の間の線形変換が
<span class="math notranslate nohighlight">\({\bf w}_1, {\bf b}_1\)</span>，2層目と3層目の間の線形変換が
<span class="math notranslate nohighlight">\({\bf w}_2, {\bf b}_2\)</span>
というパラメータによって表されているとします（図ではバイアス
<span class="math notranslate nohighlight">\({\bf b}_1, {\bf b}_2\)</span>
は省略されています）．また，これらをまとめて <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>
と表すことにします．</p>
<div class="figure" id="id28">
<img alt="パラメータ更新の例" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/08.png" />
<p class="caption"><span class="caption-text">パラメータ更新の例</span></p>
</div>
<p>入力ベクトルは <span class="math notranslate nohighlight">\({\bf x}\)</span>，ニューラルネットワークの出力は
<span class="math notranslate nohighlight">\({\bf y} \in \mathbb{R}^N\)</span>（<span class="math notranslate nohighlight">\(N\)</span>
次元実数ベクトルという意味）とし，入力 <span class="math notranslate nohighlight">\({\bf x}\)</span>
に対応した“望ましい出力”である教師ベクトルを <span class="math notranslate nohighlight">\({\bf t}\)</span>
とします．ここで，目的関数には前述の平均二乗誤差関数を用いることとしましょう．</p>
<p>さて，パラメータをそれぞれ適当な乱数で初期化したあと，入力
<span class="math notranslate nohighlight">\({\bf x}\)</span>
が与えられたときの目的関数の各パラメータについての勾配を計算して，それぞれのパラメータについて更新量を算出してみましょう．</p>
<p>まず，目的関数を改めてベクトル表記を用いて書き下すと，以下のようになります．</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}({\bf y}, {\bf t}) = \frac{1}{N} || {\bf t} - {\bf y} ||_2^2\]</div>
<p><span class="math notranslate nohighlight">\(|| {\bf t} - {\bf y} ||_2^2\)</span>はここでは<span class="math notranslate nohighlight">\(({\bf t} - {\bf y})^T({\bf t} - {\bf y})\)</span>と同等の意味となります．さらに，ニューラルネットワーク全体を
<span class="math notranslate nohighlight">\(f\)</span> と書くことにすると，出力 <span class="math notranslate nohighlight">\({\bf y}\)</span> は</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\bf y} &amp;= f({\bf x}; \boldsymbol{\Theta}) \\
&amp;= a_2 ( {\bf w}_2 a_1({\bf w}_1 {\bf x} + {\bf b}_1) + {\bf b}_2 )
\end{aligned}\end{split}\]</div>
<p>と書くことができます．ここで，<span class="math notranslate nohighlight">\(a_1, a_2\)</span>
はそれぞれ，1層目と2層目の，および2層目と3層目の間で線形変換のあとに施される非線形変換（活性化関数）を意味しています．以下，簡単のために，各層間で行われた線形変換の結果を
<span class="math notranslate nohighlight">\({\bf u}_1, {\bf u}_2\)</span>とし，中間層の値，すなわち
<span class="math notranslate nohighlight">\({\bf u}_1\)</span> に活性化関数を適用した結果を <span class="math notranslate nohighlight">\({\bf h}_1\)</span>
と書きます．ただし，<span class="math notranslate nohighlight">\({\bf u}_2\)</span> に活性化関数を適用した結果は
<span class="math notranslate nohighlight">\({\bf y}\)</span>
と表記します．すると，これらの関係は以下のように整理することができます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\bf y} &amp;= a_2({\bf u}_2) \\
{\bf u}_2 &amp;= {\bf w}_2 {\bf h}_1 + {\bf b}_2 \\
{\bf h}_1 &amp;= a_1({\bf u}_1) \\
{\bf u}_1 &amp;= {\bf w}_1 {\bf x} + {\bf b}_1
\end{aligned}\end{split}\]</div>
<div class="section" id="パラメータ-{\bf-w}_2-の更新量">
<h4>3.3.1.1. パラメータ <span class="math notranslate nohighlight">\({\bf w}_2\)</span> の更新量<a class="headerlink" href="#パラメータ-{\bf-w}_2-の更新量" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>それではまず，出力層に近い方のパラメータ，<span class="math notranslate nohighlight">\({\bf w}_2\)</span> についての
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
の勾配を求めてみましょう．これは，合成関数の偏微分なので，連鎖律（chain
rule）を用いて以下のように展開できます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\bf w}_2}
&amp;= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf w}_2} \\
&amp;= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf u}_2} \frac{\partial {\bf u}_2}{\partial {\bf w}_2}
\end{aligned}\end{split}\]</div>
<p>この3つの偏微分はそれぞれ，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\bf y}}
&amp;= -\frac{2}{N} ({\bf t} - {\bf y}) \\
\frac{\partial {\bf y}}{\partial {\bf u}_2}
&amp;= \frac{\partial a_2}{\partial {\bf u}_2} \\
\frac{\partial {\bf u}_2}{\partial {\bf w}_2}
&amp;= {\bf h}_1
\end{aligned}\end{split}\]</div>
<p>と求まります．ここで，活性化関数の入力に関する出力の勾配</p>
<div class="math notranslate nohighlight">
\[\frac{\partial a_2}{\partial {\bf u}_2}\]</div>
<p>が登場しました．これは，例えば活性化関数にシグモイド関数を用いる場合は，</p>
<div class="math notranslate nohighlight">
\[a_2({\bf u}_2) = \frac{1}{1 + \exp(-{\bf u}_2)}\]</div>
<p>の微分ですから，すなわち</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial a_2({\bf u}_2)}{\partial {\bf u}_2}
&amp;= -\frac{-(\exp(-{\bf u}_2))}{(1 + \exp(-{\bf u}_2))^2} \\
&amp;= \frac{1}{1 + \exp(-{\bf u}_2)} \cdot \frac{\exp(-{\bf u}_2)}{1 + \exp(-{\bf u}_2)} \\
&amp;= \frac{1}{1 + \exp(-{\bf u}_2)} \cdot \frac{1 + \exp(-{\bf u}_2) - 1}{1 + \exp(-{\bf u}_2)} \\
&amp;= \frac{1}{1 + \exp(-{\bf u}_2)} (1 - \frac{1}{1 + \exp(-{\bf u}_2)}) \\
&amp;= a_2({\bf u}_2)(1 - a_2({\bf u}_2))
\end{aligned}\end{split}\]</div>
<p>となります．シグモイド関数の勾配は，このようにシグモイド関数の出力値を使って簡単に計算することができます．</p>
<p>これで <span class="math notranslate nohighlight">\({\bf w}_2\)</span>
の勾配を計算するのに必要な値は全て出揃いました．では実際にNumPyを使ってこれらを計算してみましょう．ここでは簡単のために，バイアスベクトルはすべて0で初期化されているとします．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import numpy as np

# 入力
x = np.array([2, 3, 1])

# 正解
t = np.array([20])
</pre></div>
</div>
</div>
<p>まず，NumPyモジュールを読み込んでから，入力の配列を定義します．ここでは，上図と同じになるように
<code class="docutils literal notranslate"><span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">1</span></code>
の3つの値を持つ3次元ベクトルを定義しています．また，正解として仮に
<code class="docutils literal notranslate"><span class="pre">20</span></code> を与えることにしました．次に，パラメータを定義します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 1-2層間のパラメータ
w1 = np.array([[3, 1, 2], [-2, -3, -1]])
b1 = np.array([0, 0])

# 2-3層間のパラメータ
w2 = np.array([[3, 2]])
b2 = np.array([0])
</pre></div>
</div>
</div>
<p>ここでは，以下の4つのパラメータを定義しました．</p>
<p><strong>1層目と2層目の間の線形変換のパラメータ</strong></p>
<p><span class="math notranslate nohighlight">\({\bf w}_1 \in \mathbb{R}^{2 \times 3}\)</span> :
3次元ベクトルを2次元ベクトルに変換する行列</p>
<p><span class="math notranslate nohighlight">\({\bf b}_1 \in \mathbb{R}^2\)</span> : 2次元バイアスベクトル</p>
<p><strong>2層目と3層目の間の線形変換のパラメータ</strong></p>
<p><span class="math notranslate nohighlight">\({\bf w}_2 \in \mathbb{R}^{1 \times 2}\)</span> :
2次元ベクトルを1次元ベクトルに変換する行列</p>
<p><span class="math notranslate nohighlight">\({\bf b}_2 \in \mathbb{R}^1\)</span> : 1次元バイアスベクトル</p>
<p>それでは，各層の計算を実際に実行してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 中間層の計算
u1 = w1.dot(x) + b1
h1 = 1. / (1 + np.exp(-u1))

# 出力の計算
u2 = w2.dot(h1) + b2
y = 1. / (1 + np.exp(-u2))

print(y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.95257194]
</pre></div></div>
</div>
<p>出力は <span class="math notranslate nohighlight">\(0.95257194\)</span>
と求まりました．つまり，<span class="math notranslate nohighlight">\(f([2, 3, 1]^T) = 0.95257194\)</span>
ということになります．次に，上で求めた</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial {\bf w}_2}
= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf u}_2} \frac{\partial {\bf u}_2}{\partial {\bf w}_2}\]</div>
<p>の右辺の3つの偏微分をそれぞれ計算してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># dL / dy
g_Ly = -2 / 1 * (t - y)

# dy / du_2
g_yu2 = y * (1 - y)

# du_2 / dw_2
g_u2w2 = h1
</pre></div>
</div>
</div>
<p>これらを掛け合わせれば，求めたかったパラメータ <span class="math notranslate nohighlight">\({\bf w}_2\)</span>
についての勾配を得ることができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># dL / dw_2: 求めたい勾配
g_Lw2 = g_Ly * g_yu2 * g_u2w2

print(g_Lw_2)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-1.72104507e+00 -1.43112111e-06]
</pre></div></div>
</div>
<p>勾配が求まりました．これが
<span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial {\bf w}_2\)</span>
の値です．これを学習率でスケールさせたものを使えば，パラメータ
<span class="math notranslate nohighlight">\({\bf w}_2\)</span>
を更新することができます．更新式は，具体的には以下のようになります．</p>
<div class="math notranslate nohighlight">
\[{\bf w}_2 \leftarrow {\bf w}_2 - \eta \frac{\partial \mathcal{L}}{\partial {\bf w}_2}\]</div>
<p>ここでは学習率を <span class="math notranslate nohighlight">\(\eta\)</span> で表記しました．</p>
</div>
<div class="section" id="学習率（learning-rate）について">
<h4>3.3.1.2. 学習率（learning rate）について<a class="headerlink" href="#学習率（learning-rate）について" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>学習率が大きすぎると，繰り返しパラメータ更新を行っていく中で目的関数の値が振動したり，発散したりしてしまいます．小さすぎると，収束に時間がかかってしまいます．そのため，この学習率を適切に決定することがニューラルネットワークの学習においては非常に重要となります．多くの場合，学習がきちんと進むもっとも大きな値を経験的に探すということが行われます．シンプルな画像認識のタスクなどでは大抵，0.1~0.01などが最初に用いられる場合が比較的多く見られます．</p>
</div>
<div class="section" id="パラメータ-{\bf-w}_1-の更新量">
<h4>3.3.1.3. パラメータ <span class="math notranslate nohighlight">\({\bf w}_1\)</span> の更新量<a class="headerlink" href="#パラメータ-{\bf-w}_1-の更新量" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>次に，<span class="math notranslate nohighlight">\({\bf w}_1\)</span>
の更新量も求めてみましょう．そのためには，<span class="math notranslate nohighlight">\({\bf w}_1\)</span> で目的関数
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
を偏微分した値が必要です．これは以下のように計算できます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\bf w}_1}
&amp;= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf w}_1} \\
&amp;=
\frac{\partial \mathcal{L}}{\partial {\bf y}}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
\frac{\partial {\bf u}_2}{\partial {\bf w}_1} \\
&amp;=
\frac{\partial \mathcal{L}}{\partial {\bf y}}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
\frac{\partial {\bf u}_2}{\partial {\bf h}_1}
\frac{\partial {\bf h}_1}{\partial {\bf w}_1} \\
&amp;=
\frac{\partial \mathcal{L}}{\partial {\bf y}}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
\frac{\partial {\bf u}_2}{\partial {\bf h}_1}
\frac{\partial {\bf h}_1}{\partial {\bf u}_1}
\frac{\partial {\bf u}_1}{\partial {\bf w}_1}
\end{aligned}\end{split}\]</div>
<p>この5つの偏微分のうち初めの1つはすでに求めました．残りの3つは，それぞれ，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
&amp;= {\bf y}(1 - {\bf y}) \\
\frac{\partial {\bf u}_2}{\partial {\bf h}_1}
&amp;= {\bf w}_2 \\
\frac{\partial {\bf h}_1}{\partial {\bf u}_1}
&amp;= {\bf h}_1(1 - {\bf h}_1) \\
\frac{\partial {\bf u}_1}{\partial {\bf w}_1}
&amp;= {\bf x}
\end{aligned}\end{split}\]</div>
<p>と計算できます．では，さっそく実際にNumPyを用いて計算を実行してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>g_yu2 = y * (1 - y)
g_u2h1 = w2
g_h1u1 = h1 * (1 - h1)
g_u1w1 = x

# 上から du1 / dw1 の直前までを一旦計算
g_Lu1 = g_Ly * g_yu_2 * g_u_2h_1 * g_h_1u_1

# g_u1w1は (3,) というshapeなので，g_u1w1[None]として(1, 3)に変形
g_u1w1 = g_u1w1[None]

# dL / dw_1: 求めたい勾配
g_Lw1 = g_Lu1.T.dot(g_u1w1)

print(g_Lw1)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-1.72463398e-04 -2.58695098e-04 -8.62316992e-05]
 [-5.72447970e-06 -8.58671954e-06 -2.86223985e-06]]
</pre></div></div>
</div>
<p>これが <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial {\bf w}_1\)</span>
の値です．これを用いて，<span class="math notranslate nohighlight">\({\bf w}_2\)</span>
と同様に以下のような更新式でパラメータ <span class="math notranslate nohighlight">\({\bf w}_1\)</span>
の更新をすることができます．</p>
<div class="math notranslate nohighlight">
\[{\bf w}_1 \leftarrow {\bf w}_1 - \eta \frac{\partial \mathcal{L}}{\partial {\bf w}_1}\]</div>
</div>
</div>
</div>
<div class="section" id="誤差逆伝播法（バックプロパゲーション）">
<h2>3.4. 誤差逆伝播法（バックプロパゲーション）<a class="headerlink" href="#誤差逆伝播法（バックプロパゲーション）" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここまでで，各パラメータについての目的関数の導関数を手計算により導出して実際に勾配の数値計算を行うということを体験しました．では，もっと層数の多いニューラルネットワークの場合は，どうなるでしょうか．同様に手計算によって導関数を求めることももちろん可能ですが，ニューラルネットワークが微分可能な関数を繰り返し適用するものであるという性質を用いると，コンピュータによって自動的に勾配を与える関数を導き出すことが可能です．合成関数の偏微分は，連鎖律によって複数の偏微分の積の形に変形できたことを思い出しましょう．</p>
<p>下図は，ここまでの説明で用いていた3層の全結合型ニューラルネットワークの出力を得るための計算と，その値を使って目的関数の値を計算する過程を青い矢印で，そして前節で手計算によって行った各パラメータによる目的関数の偏微分を計算する過程を赤い矢印で表現した動画となっています．</p>
<div class="figure" id="id29">
<img alt="誤差逆伝播法(Backpropagation)の計算過程" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/backpropagation.gif" />
<p class="caption"><span class="caption-text">誤差逆伝播法(Backpropagation)の計算過程</span></p>
</div>
<p>まず，目的関数の出力を <span class="math notranslate nohighlight">\(l = \mathcal{L}({\bf y}, {\bf t})\)</span>
とします．この図の丸いノードは変数を表し，四角いノードは関数を表しています．今，一つの巨大な合成関数として見ることができるニューラルネットワーク全体を
<span class="math notranslate nohighlight">\(f\)</span> と表し，その中で各層間の線形変換に用いられる関数を
<span class="math notranslate nohighlight">\(f_1\)</span>, <span class="math notranslate nohighlight">\(f_2\)</span>，非線形変換をそれぞれ <span class="math notranslate nohighlight">\(a_1\)</span>,
<span class="math notranslate nohighlight">\(a_2\)</span>
と表します．このとき，前節で行った更新量の算出はどのように捉えることができるでしょうか．</p>
<p>今，上図の青い矢印で表されるように，新しい入力 <span class="math notranslate nohighlight">\({\bf x}\)</span>
がニューラルネットワークに与えられ，それが順々に出力側に伝わっていき，最終的に目的関数の値
<span class="math notranslate nohighlight">\(l\)</span> まで計算が終わったとします．ここまでを <strong>順伝播（forward
propagation）</strong> といいます．</p>
<p>すると次は，目的関数の出力の値を小さくするような各パラメータの更新量を求めたいということになりますが，このために必要な目的関数の勾配は，各パラメータの丸いノードより先の部分（出力側）にある関数の勾配だけで計算できることが分かります．具体的には，それらを全て掛け合わせたものになっています．つまり，上図の赤い矢印で表されるように，出力側から入力側に向かって，<strong>順伝播とは逆向きに</strong>，各関数における入力についての勾配を求めて掛け合わせていけば，パラメータについての目的関数の勾配が計算できるわけです．</p>
<p>このように，微分の連鎖律の仕組みを用いて，ニューラルネットワークを構成する関数が持つパラメータについての目的関数の勾配を，<strong>順伝播で通った経路を逆向きにたどるようにして</strong>途中の関数の勾配の掛け算によって求めるアルゴリズムを
<strong>誤差逆伝播法（backpropagation）</strong> と呼びます．</p>
</div>
<div class="section" id="勾配消失">
<h2>3.5. 勾配消失<a class="headerlink" href="#勾配消失" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>活性化関数について初めに触れた際，シグモイド関数には勾配消失という現象が起きやすくなるという問題があり，現在はあまり使われていないと説明をしました．その理由についてもう少し詳しく見ていきましょう．</p>
<p>上で既に計算した，シグモイド関数の導関数を思い出してみます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
f\left( u\right) &amp;=\dfrac {1}{1+e^{-u}} \\
f'\left( u\right) &amp;= f\left( u\right) \left( 1-f\left( u\right) \right)
\end{aligned}\end{split}\]</div>
<p>さて，この導関数の値を入力変数に関してプロットしてみると，下記のようになります．</p>
<div class="figure" id="id30">
<img alt="シグモイド関数の導関数" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/09.png" />
<p class="caption"><span class="caption-text">シグモイド関数の導関数</span></p>
</div>
<p>この図の上2つは，導関数を構成する2つの部分 <span class="math notranslate nohighlight">\(f(u)\)</span> と
<span class="math notranslate nohighlight">\(1 - f(u)\)</span>
の値を別々にプロットしたもので，中央下の図が実際の導関数の値となります．上図中央下の導関数の形を見ると，入力が原点から遠くなるにつれ勾配の値がどんどん減少し，0に漸近していくことが見て取れます．</p>
<p>各パラメータの更新量を求めるには，前節で説明したように，<strong>そのパラメータよりも先のすべての関数の勾配を掛け合わせる</strong>必要がありました．このとき，活性化関数にシグモイド関数を用いていると，勾配は必ず<strong>最大でも0.25</strong>という値にしかなりません．すると，線形変換が計算グラフ中に現れるたびに，目的関数の勾配は，多くとも0.25倍されてしまいます．これは，層数が増えていけばいくほど，この最大でも0.25という値が繰り返し掛け合わされることになるため，入力に近い層に流れていく勾配がどんどん0に近づいていってしまいます．</p>
<p>具体例を見てみましょう．今回は3層のニューラルネットワークを用いて説明を行っていましたが，4層の場合を考えてみます，すると，一番入力に近い線形変換のパラメータの勾配は，多くとも目的関数の勾配を
<span class="math notranslate nohighlight">\(0.25 \times 0.25 = 0.0625\)</span>
倍したものということになります．層数が一つ増えるたびに，指数的に勾配が小さくなるということがよく分かります．</p>
<p>ディープラーニングでは，4層よりもさらに多くの層を積み重ねたニューラルネットワークが用いられます．そうすると，活性化関数としてシグモイド関数を使用した場合，<strong>目的関数の勾配が入力に近い関数が持つパラメータへほぼ全く伝わらなくなってしまいます</strong>，あまりにも小さな勾配しか伝わってこなくなると，パラメータの更新量がほとんど0になるため，どんなに目的関数が大きな値になっていても，入力層に近い関数が持つパラメータは変化しなくなります．つまり初期化時からほとんど値が変わらなくなるということになり，学習が行われていないという状態になるわけです．これを
<strong>勾配消失（vanishing gradient）</strong>
と呼び，長らく深い（十数層を超える）ニューラルネットワークの学習が困難であった一つの要因でした．</p>
</div>
<div class="section" id="「レイヤ」が指すもの">
<h2>3.6. 「レイヤ」が指すもの<a class="headerlink" href="#「レイヤ」が指すもの" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここまでの解説では，前々節の誤差逆伝播法の説明に登場した図のようなグラフにおける丸いノード（中間出力などの値）の数を指して○層（○-layer）のニューラルネットワーク，という言い方をしてきました．しかし，このグラフにおける四角いノード（関数）を指して層（layer）と呼ぶ場合もあります．そしてニューラルネットワークの実装に使われるフレームワークでは，多くの場合，層タイプ（layer
type）として様々な関数がまとめられています．そこで，以下ではこの関数に対して「層」もしくは「レイヤ」という言葉を用いることに注意してください．</p>
</div>
<div class="section" id="様々なレイヤタイプ">
<h2>3.7. 様々なレイヤタイプ<a class="headerlink" href="#様々なレイヤタイプ" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここまでで，ニューラルネットワークには大別して線形変換を行う関数と非線形変換を行う関数という二種類の関数が含まれることを説明し，線形変換の例として
<strong>全結合層（fully-connected layer）</strong> のみを用いてきました．</p>
<p>しかし，ニューラルネットワークの構成要素として用いることができるレイヤは，全結合層と活性化関数だけではありません．画像認識や画像生成，画像の変換や超解像（低解像の画像から解像度を高めた画像を作る技術）など，様々なタスクで入力に画像というデータ形式が用いられますが，こういったタスクではしばしば画像というデータ形式の特徴と相性の良い変換である畳み込み層（convolution
layer）というものが全結合層の代わりにニューラルネットワークの中で線形変換を担います．</p>
<p>また，ニューラルネットワークの構成要素となるレイヤには，微分可能な関数であればなんであろうと用いることができます．そこで，いわゆる
<strong>畳み込みニューラルネットワーク（convolutional neural networks; CNN）</strong>
と呼ばれるネットワークアーキテクチャの種別では，この畳込み層に加えてプーリング層（pooling
layer）というレイヤがしばしば用いられてきました．</p>
<p>本節では，このCNNでしばしば用いられる畳み込み層とプーリング層について，その計算の概要を説明します．</p>
<div class="section" id="畳み込み層（convolution-layer）">
<h3>3.7.1. 畳み込み層（convolution layer）<a class="headerlink" href="#畳み込み層（convolution-layer）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>全結合層では，出力の計算のためにパラメータ行列<span class="math notranslate nohighlight">\({\bf W}\)</span>中の一つ一つの値が入力データの持つ全ての要素（ベクトルであれば，全次元の値）と掛け合わされていました．言い換えると，「結合」を何らかの演算が行われる関係を指すものとすれば，パラメータ行列<span class="math notranslate nohighlight">\({\bf W}\)</span>の<span class="math notranslate nohighlight">\(i, j\)</span>要素である<span class="math notranslate nohighlight">\(W_{ij}\)</span>と，入力データ<span class="math notranslate nohighlight">\({\bf x}\)</span>の全要素の間が，全結合（fully-connected）していたということです．</p>
<p>一方，畳み込み層では，ここまでで説明した全結合層とは異なり，パラメータと入力データの間の結合が局所的になっており，各々のパラメータは入力データの全ての要素と結合しているとは限りません．具体的には，2次元畳み込み層の場合，パラメータはカーネル（もしくはフィルタ）と呼ばれる小さな画像パッチのようなものの集合として用意され，そのパッチごとに入力データの一部の値との間の演算が行われる形になっています．</p>
<p>下の図を用いて，<span class="math notranslate nohighlight">\(3 \times 3\)</span>の大きさの小さな画像パッチのような畳み込みカーネルを2つ持つ畳み込み層の実際の計算過程を調べてみましょう．</p>
<div class="figure" id="id31">
<img alt="畳み込み層の計算過程" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/convolution.gif" />
<p class="caption"><span class="caption-text">畳み込み層の計算過程</span></p>
</div>
<p>（図は<a class="reference external" href="http://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual
Recognition</a>より引用）</p>
<p>一番左の青いブロックが並ぶ列は，入力画像を表しています．入力は
幅・高さが<span class="math notranslate nohighlight">\(5 \times 5\)</span>
の大きさの3チャンネル画像です．上図ではまず，パディング（padding）と呼ばれる処理をして，入力画像の周囲に値が0となる領域を付け足しています．今回は幅1の領域をパディングしています．</p>
<p>中央2列は，それぞれ1列ごとに1つの畳み込みカーネルを表しています．1つの畳み込みカーネルは，入力の画像（もしくは特徴マップ（feature
map）と呼ばれる一つ前の層の出力）が持つのと同数のチャンネルを持ちます．ここでは入力画像は3チャンネルだったので，各カーネルは3チャンネル分の値を持っています．これが縦に並んだ3つの赤い<span class="math notranslate nohighlight">\(3 \times 3\)</span>の大きさのブロックで表されています．各カーネルの各チャンネルは，対応する入力のチャンネルに対して自らをスライドさせていくような形で計算を行います．このときのスライド幅をストライド（stride）と呼びます．図では，2個飛びに入力画像上を<span class="math notranslate nohighlight">\(3 \times 3\)</span>の枠が移動しているようすが見て取れます．すなわち，ストライドは2に設定されています．ここで行われる計算は，入力のある位置にカーネルを重ね合わせた際に，重なっている<span class="math notranslate nohighlight">\(3 \times 3 = 9\)</span>個の要素を互いに掛け合わせて，結果を全て足すという計算です．上の図では，例えば1つ目のカーネル（W0）の1つ目のチャンネルは，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1 &amp; 1 &amp; -1 \\
0 &amp; -1 &amp; 0 \\
1 &amp; -1 &amp; -1
\end{matrix}
\right]\end{split}\]</div>
<p>という値をもっており，それに対応する入力画像の1チャンネル目の左上から<span class="math notranslate nohighlight">\(3 \times 3\)</span>の小領域には</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0
\end{matrix}
\right]\end{split}\]</div>
<p>という値が並んでいます．そのため，それぞれ重なりあっている値同士を掛け合わせると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; -2 &amp; 0
\end{matrix}
\right]\end{split}\]</div>
<p>となることが分かります．この9つの値を全て足したものが，まずこのカーネルの1つ目のチャンネルの1つ目の計算結果となります．計算すると，結果は-2です．これを2チャンネル目，3チャンネル目の値でも同様に行うと，2チャンネル目は0，3チャンネル目は1という計算結果になります．そして，最後にこの全てのチャンネルの結果を足し合わせます．その結果は-1となります．これにカーネルごとに用意されるバイアスの値（上図では1つ目のカーネルのバイアスは1）を足したものが，このカーネルの入力の左上隅の領域に対する出力結果となります．結果は，0です．一番右の列の緑色のブロックはこの畳込み層の出力値を表しており，1つ目のブロックの左上のマスを見ると，確かに0となっていることが分かります．</p>
<p>同様に，他のマスの値もいくつか自分で計算してみてください．</p>
<p>このレイヤの微分を考えるようとすると一見複雑に感じられますが，個別のパラメータ値を使って行われる演算は
<strong>重みを掛けてバイアスを足す</strong>
だけであるため，原理的にはシンプルな線形変換のみで構成されていることからこの変換をパラメータおよび入力について微分することは可能であることが分かります．実際，カーネルサイズが
<span class="math notranslate nohighlight">\(1 \times 1\)</span>
のカーネルを用いた畳み込み演算は各入力次元に同一の重み行列を用いた線形変換を施すのと同等です．たとえば，入力の幅と高さが
<span class="math notranslate nohighlight">\(W \times H\)</span> で3チャンネルの場合，カーネルサイズが
<span class="math notranslate nohighlight">\(1 \times 1\)</span> の畳み込み層のカーネルは <span class="math notranslate nohighlight">\(1 \times 1 \times 3\)</span>
のテンソル，バイアスは1次元のスカラーとなり，<span class="math notranslate nohighlight">\(W \times H\)</span>
個の全ての入力次元にこのテンソルとスカラーによる線形変換を施します．</p>
<p>このように，畳み込み層はカーネルを様々に定義することで多様なデータ形式を扱うことができます．1次元のカーネルを使うと1次元の系列データなどにも適用でき，3次元のカーネルを用いれば動画やボクセルのようなデータにも適用できます．</p>
</div>
<div class="section" id="プーリング層（pooling-layer）">
<h3>3.7.2. プーリング層（pooling layer）<a class="headerlink" href="#プーリング層（pooling-layer）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>プーリングは，主に特徴マップに対して行われる操作で，特徴マップの空間方向の次元（spatial
dimension，入力画像における幅・高さに対応する次元）の大きさを削減し計算量を抑えたり，微小な平行移動について出力を不変とすることで画像認識タスクなどにおけるロバスト性の向上のためなどに用いられます．</p>
<p>プーリングの計算方法は畳み込みと似ています．ただしパラメータはなく，畳み込みカーネルを用いて行われた計算の部分が，対応する入力の部分領域に対する平均の計算や最大値計算などに置き換えられたものです．部分領域ごとの平均値を計算していくプーリングを平均値プーリング（average
pooling），最大値を計算するプーリングを最大値プーリング（max
pooling）と呼びます．</p>
<p>以下の図のうち左側の部分では，プーリング層を用いたダウンサンプリング（解像度を下げること）を
<span class="math notranslate nohighlight">\(224 \times 224\)</span> サイズの，<span class="math notranslate nohighlight">\(64\)</span>
チャンネルの特徴マップに適用し，縦横半分の大きさ
（<span class="math notranslate nohighlight">\(112 \times 112\)</span>サイズ）の特徴マップにしています．このとき，チャンネル数（depthとも呼ばれる）は維持されています．</p>
<p>この図のうち右側の部分は，左側の直方体の64チャンネルのうちの一つ，濃い色になっているチャンネルの
<span class="math notranslate nohighlight">\(4 \times 4\)</span>
の部分領域を抜き出してきて模式的に表したものに，<span class="math notranslate nohighlight">\(2 \times 2\)</span>
領域ごとに最大値を計算しその領域の代表値とする最大値プーリングを領域を2つ飛びで（ストライド2で）ずらしながら行った際の結果を表しています．</p>
<div class="figure" id="id32">
<img alt="プーリング層の計算過程" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/pooling.png" />
<p class="caption"><span class="caption-text">プーリング層の計算過程</span></p>
</div>
<p>（図は<a class="reference external" href="http://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual
Recognition</a>より引用）</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_Chainer.html" class="btn btn-neutral float-right" title="4. Deep Learningフレームワークの基礎" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Introduction_to_ML_libs.html" class="btn btn-neutral" title="2. 機械学習ライブラリの基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>