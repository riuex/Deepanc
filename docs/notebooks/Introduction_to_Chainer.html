

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4. Deep Learningフレームワークの基礎 &mdash; メディカルAI専門コース オンライン講義資料  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="5. 実践編: MRI画像のセグメンテーション" href="Image_Segmentation.html" />
    <link rel="prev" title="3. ニューラルネットワークの基礎" href="Introduction_to_Neural_Network.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 機械学習ライブラリの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. ニューラルネットワークの基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. Deep Learningフレームワークの基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#環境構築">4.1. 環境構築</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Chainerの基本的な使い方">4.2. Chainerの基本的な使い方</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#データセットの準備">4.2.1. データセットの準備</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Validation用データセットを作る">4.2.2. Validation用データセットを作る</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Iteratorの作成">4.2.3. Iteratorの作成</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#SerialIteratorについて">4.2.3.1. SerialIteratorについて</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ネットワークの定義">4.2.4. ネットワークの定義</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#LinkとFunction">4.2.4.1. LinkとFunction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Chain">4.2.4.2. Chain</a></li>
<li class="toctree-l4"><a class="reference internal" href="#同じ結果を保証する">4.2.4.3. 同じ結果を保証する</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Chainを継承したネットワークの定義">4.2.4.4. Chainを継承したネットワークの定義</a></li>
<li class="toctree-l4"><a class="reference internal" href="#GPUで実行するには">4.2.4.5. GPUで実行するには</a></li>
<li class="toctree-l4"><a class="reference internal" href="#入力側ユニット数の自動計算">4.2.4.6. 入力側ユニット数の自動計算</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#最適化手法の選択">4.2.5. 最適化手法の選択</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#学習率（learning-rate）">4.2.5.1. 学習率（learning rate）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#学習の開始">4.2.6. 学習の開始</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#目的関数">4.2.6.1. 目的関数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#学習ループのコード">4.2.6.2. 学習ループのコード</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ValidationやTestを行う際の注意点">4.2.6.3. ValidationやTestを行う際の注意点</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#学習済みモデルの保存">4.2.7. 学習済みモデルの保存</a></li>
<li class="toctree-l3"><a class="reference internal" href="#保存したモデルを読み込んで推論">4.2.8. 保存したモデルを読み込んで推論</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Trainerの使用方法">4.3. Trainerの使用方法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#データセット・Iterator・ネットワークの準備">4.3.1. データセット・Iterator・ネットワークの準備</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Updaterの準備">4.3.2. Updaterの準備</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#損失計算のためのChain">4.3.2.1. 損失計算のためのChain</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Trainerの準備">4.3.3. Trainerの準備</a></li>
<li class="toctree-l3"><a class="reference internal" href="#TrainerにExtensionを追加">4.3.4. TrainerにExtensionを追加</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#LogReport">4.3.4.1. <code class="docutils literal notranslate"><span class="pre">LogReport</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#snapshot">4.3.4.2. <code class="docutils literal notranslate"><span class="pre">snapshot</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#dump_graph">4.3.4.3. <code class="docutils literal notranslate"><span class="pre">dump_graph</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#Evaluator">4.3.4.4. <code class="docutils literal notranslate"><span class="pre">Evaluator</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#PrintReport">4.3.4.5. <code class="docutils literal notranslate"><span class="pre">PrintReport</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#PlotReport">4.3.4.6. <code class="docutils literal notranslate"><span class="pre">PlotReport</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ParameterStatistics">4.3.4.7. <code class="docutils literal notranslate"><span class="pre">ParameterStatistics</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#学習の開始-(Trainer利用)">4.3.5. 学習の開始 (Trainer利用)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#テストデータ評価">4.3.6. テストデータ評価</a></li>
<li class="toctree-l3"><a class="reference internal" href="#学習済みモデルで推論する">4.3.7. 学習済みモデルで推論する</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#新しいネットワークの利用">4.4. 新しいネットワークの利用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#新しいネットワークの定義">4.4.1. 新しいネットワークの定義</a></li>
<li class="toctree-l3"><a class="reference internal" href="#学習">4.4.2. 学習</a></li>
<li class="toctree-l3"><a class="reference internal" href="#学習済みネットワークを使った予測">4.4.3. 学習済みネットワークを使った予測</a></li>
<li class="toctree-l3"><a class="reference internal" href="#深いネットワークの定義">4.4.4. 深いネットワークの定義</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#構成要素を定義する">4.4.4.1. 構成要素を定義する</a></li>
<li class="toctree-l4"><a class="reference internal" href="#大きなネットワークの定義">4.4.4.2. 大きなネットワークの定義</a></li>
<li class="toctree-l4"><a class="reference internal" href="#高速化のTIPS">4.4.4.3. 高速化のTIPS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#データセットクラスの使用方法">4.5. データセットクラスの使用方法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#CIFAR10データセットクラス">4.5.1. CIFAR10データセットクラス</a></li>
<li class="toctree-l3"><a class="reference internal" href="#作成したデータセットクラスを用いた学習">4.5.2. 作成したデータセットクラスを用いた学習</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Data-Augmentationの簡単な使い方">4.6. Data Augmentationの簡単な使い方</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ChainerCVを活用した変換処理">4.6.1. ChainerCVを活用した変換処理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#参考文献">4.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 実践編: ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>4. Deep Learningフレームワークの基礎</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Introduction_to_Chainer.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/mitmul/medical-ai-course-materials/blob/master/notebooks/Introduction_to_Chainer.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="Deep-Learningフレームワークの基礎">
<h1>4. Deep Learningフレームワークの基礎<a class="headerlink" href="#Deep-Learningフレームワークの基礎" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>Chainerは，現在様々なDeep Learningフレームワーク（TensorFlow, PyTorch,
etc.）でも採用され主要なニューラルネットワークの記法となっている<strong>Define-by-Run</strong>というアイデアを汎用的なDeep
Learningフレームワークとしては初めて採用し，2015年からPreferred
Networks社によって開発が続けられています．Define-by-Runとは，ニューラルネットワーク中の計算を行うコードを記述することでニューラルネットワークの構造を定義する考え方です．学習を行う前にネットワーク構造を定義しておき，そのネットワークに学習に用いるデータを入力するためのコードを別途書く必要がある方法はDefine-and-Runと呼ばれます．Define-by-Runは実行時にネットワーク構造が決定されるため，動的な構造を記述しやすいという特徴があります．</p>
<p>ここでは，その<strong>柔軟性</strong>と<strong>直感的である</strong>ことを特徴とするこのChainerというフレームワークの基本的な使い方を解説します．</p>
<div class="section" id="環境構築">
<h2>4.1. 環境構築<a class="headerlink" href="#環境構築" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まずはColab上で以下のセルを実行し，最新版のChainerをインストールしましょう．ここでは同時に<code class="docutils literal notranslate"><span class="pre">graphviz</span></code>というソフトウェアもインストールしています．これは，後にニューラルネットワークのアーキテクチャをグラフ構造として可視化するために使用します．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!curl https://colab.chainer.org/install | sh -
!apt-get install -y graphviz
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1379  100  1379    0     0   8512      0 --:--:-- --:--:-- --:--:--  8512
+ apt -y -q install cuda-libraries-dev-9-2
Reading package lists...
Building dependency tree...
Reading state information...
cuda-libraries-dev-9-2 is already the newest version (9.2.148-1).
0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.
+ pip install -q cupy-cuda92  chainer
+ set +ex
Installation succeeded!
Reading package lists... Done
Building dependency tree
Reading state information... Done
graphviz is already the newest version (2.40.1-2).
0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.
</pre></div></div>
</div>
<p>それでは，以下のコマンドをターミナルで実行し，Chainerや，ChainerでGPUを活用するために必要となるCuPyというパッケージが正しくインストールされているかどうかを確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!python -c &#39;import chainer; chainer.print_runtime_info()&#39;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform: Linux-4.14.65+-x86_64-with-Ubuntu-18.04-bionic
Chainer: 5.0.0
NumPy: 1.14.6
CuPy:
  CuPy Version          : 5.0.0
  CUDA Root             : /usr/local/cuda
  CUDA Build Version    : 9020
  CUDA Driver Version   : 9020
  CUDA Runtime Version  : 9020
  cuDNN Build Version   : 7201
  cuDNN Version         : 7201
  NCCL Build Version    : 2213
iDeep: 2.0.0.post3
</pre></div></div>
</div>
<p>Chainer, NumPy, そしてCuPy, さらにCuPyの下にCUDAやcuDNN,
NCCLといった項目があり，それぞれバージョン番号が表示されていれば成功です．以下のチュートリアルでは，matplotlibをグラフの可視化に使いますので，これも同時にインストールしておきましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!pip install matplotlib
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)
Requirement already satisfied: numpy&gt;=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)
Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)
Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)
Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.7)
</pre></div></div>
</div>
</div>
<div class="section" id="Chainerの基本的な使い方">
<h2>4.2. Chainerの基本的な使い方<a class="headerlink" href="#Chainerの基本的な使い方" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>はじめに，シンプルなタスクに実際に取り組むことによって，Chainerの基本的な使い方を説明していきます．さっそく，有名な手書き数字のデータセットMNISTを使って，画像を10クラス（数字の0
- 9）のいずれかに分類するネットワークを書き，学習させてみましょう．</p>
<div class="section" id="データセットの準備">
<h3>4.2.1. データセットの準備<a class="headerlink" href="#データセットの準備" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>まずは学習対象となる<strong>データセット</strong>の準備をします．教師あり学習の場合，データセットは<strong>「入力データ」と「それと対になるラベルデータ」のペアを返すオブジェクト</strong>である必要があります．</p>
<p>Chainerには，MNISTやCIFAR10/100のような良く用いられるデータセットに対して，データのダウンロードからオブジェクト作成までを自動的に行ってくれる便利なメソッドがあります．ここではひとまずこれを用いましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.datasets import mnist

# データセットがダウンロード済みでなければ，ダウンロードも行う
train_val, test = mnist.get_mnist(withlabel=True, ndim=1)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Downloading from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz...
Downloading from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz...
Downloading from http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz...
Downloading from http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz...
</pre></div></div>
</div>
<p>データセットオブジェクトの準備ができました．このオブジェクトは，
<code class="docutils literal notranslate"><span class="pre">train_val[i]</span></code> のように指定すると，<strong>i番目の (data, label)
というタプルを返すリスト</strong>
と同様のものと考えてください．（実際ただのPythonリストもChainerのデータセットオブジェクトとして利用可能です）．それでは，0番目のデータとラベルを取り出して，表示してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># matplotlibを使ったグラフ描画結果がnotebook内に表示されるようにします．
%matplotlib inline
import matplotlib.pyplot as plt

# データの例示
x, t = train_val[0]  # 0番目の (data, label) を取り出す
plt.imshow(x.reshape(28, 28), cmap=&#39;gray&#39;)
plt.axis(&#39;off&#39;)
plt.show()
print(&#39;label:&#39;, t)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_11_0.png" src="../_images/notebooks_Introduction_to_Chainer_11_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
label: 5
</pre></div></div>
</div>
</div>
<div class="section" id="Validation用データセットを作る">
<h3>4.2.2. Validation用データセットを作る<a class="headerlink" href="#Validation用データセットを作る" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>次に，さきほど作成した<code class="docutils literal notranslate"><span class="pre">train_val</span></code>データセットを，Training用のデータセットとValidation用のデータセットに分割します．Validationデータセットとは，学習には用いずにモデルの汎化性能をチェックしたり，学習率などのハイパーパラメータを調整するために用いる検証用のデータセットスプリットのことです．分割処理も，Chainerが提供しているデータセット分割用の関数を用いて行うことができます．元々60000個のデータが入っている<code class="docutils literal notranslate"><span class="pre">train</span></code>データセットを，ランダムに選択された50000個のデータと残りの10000個のデータの2つに分割しましょう．これには，<code class="docutils literal notranslate"><span class="pre">split_dataset_random</span></code>という関数を使用します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.datasets import split_dataset_random

train, valid = split_dataset_random(train_val, 50000, seed=0)
</pre></div>
</div>
</div>
<p>関数の第1引数が分割したい対象のデータセットオブジェクト，第2引数が1つ目のデータセットの要素数，第3引数がランダムな抽出を行う際に用いられる乱数シード（これは省略可）となります．第3引数の<code class="docutils literal notranslate"><span class="pre">seed</span></code>として同じ値を指定すると，再実行した際にデータセットを同じように分割するようになります．それでは，それぞれのデータセットの中に入っているデータの数を確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&#39;Training dataset size:&#39;, len(train))
print(&#39;Validation dataset size:&#39;, len(valid))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training dataset size: 50000
Validation dataset size: 10000
</pre></div></div>
</div>
</div>
<div class="section" id="Iteratorの作成">
<h3>4.2.3. Iteratorの作成<a class="headerlink" href="#Iteratorの作成" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>次に，さきほど準備したデータセットオブジェクトから，幾つかのデータ（入力とラベルのペア）を束ねて学習モデルに次々に渡す，<strong>Iterator</strong>という機能を紹介します．なぜIteratorの機能が必要かというと，ニューラルネットワークのパラメータを更新する際に利用される，確率的勾配降下法（Stochastic
Gradient Descent,
SGD）をはじめとする最適化手法では，一つのデータだけを元に更新する処理を繰り返すのではなく，幾つかのデータを束ねた
<strong>ミニバッチ</strong>
を元に計算していくのが一般的となっているためです（ミニバッチ計算が一般的である理由としては，勾配のミニバッチ平均を計算することでパラメータ更新が安定することや，GPUなどを用いた並列化がしやすいこと等が挙げられます）．</p>
<p><code class="docutils literal notranslate"><span class="pre">Iterator</span></code>は，さきほど作成したデータセットオブジェクトを引数として指定し，<code class="docutils literal notranslate"><span class="pre">next()</span></code>メソッドを呼ぶことで新しいミニバッチを返してくれます．データセット内のデータすべてを1度ずつ学習に利用し終えた時点のことを
<strong>1エポック（epoch）</strong>
と呼びます．Iteratorの内部では，学習中に何エポックまで学習を行ったか，などの情報が逐次記録されており，データセット内のデータを何度も使って学習のループを回すようなコードを簡単に書くことができるようになります．</p>
<p>データセットオブジェクトからイテレータを作るには，以下のようにします．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer import iterators

batchsize = 128

train_iter = iterators.SerialIterator(train, batchsize)
valid_iter = iterators.SerialIterator(
    valid, batchsize, repeat=False, shuffle=False)
test_iter = iterators.SerialIterator(
    test, batchsize, repeat=False, shuffle=False)
</pre></div>
</div>
</div>
<p>今，学習データセット用のイテレータ（<code class="docutils literal notranslate"><span class="pre">train_iter</span></code>）と，検証データセット用のイテレータ（<code class="docutils literal notranslate"><span class="pre">valid_iter</span></code>），および学習したネットワークの評価に用いるテストデータセット用のイテレータ（<code class="docutils literal notranslate"><span class="pre">test_iter</span></code>）の計3つを作成しました．ここでは<code class="docutils literal notranslate"><span class="pre">batchsize</span> <span class="pre">=</span> <span class="pre">128</span></code>としているため，作成した3つのイテレータは<code class="docutils literal notranslate"><span class="pre">next()</span></code>メソッドが（<code class="docutils literal notranslate"><span class="pre">train_iter.next()</span></code>のように）呼ばれると，<strong>128枚の数字画像データ</strong>を一括りにして返します．実際に<code class="docutils literal notranslate"><span class="pre">next()</span></code>の返り値を調べてみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>minibatch = train_iter.next()
</pre></div>
</div>
</div>
<p>この<code class="docutils literal notranslate"><span class="pre">minibatch</span></code>という変数は，<code class="docutils literal notranslate"><span class="pre">(img,</span> <span class="pre">label)</span></code>というタプルが128個（ミニバッチサイズだけ）並んだリストになっています．実際に，このリストの長さが128であることを確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&#39;batchsize:&#39;, len(minibatch))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
batchsize: 128
</pre></div></div>
</div>
<p>次に，この<code class="docutils literal notranslate"><span class="pre">minibatch</span></code>というリストの一つ目の要素（画像とラベルを持つタプルになっているはずです）を<code class="docutils literal notranslate"><span class="pre">minibatch[0]</span></code>として取り出してみます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x, t = minibatch[0]

print(&#39;x:&#39;, x.shape)
print(&#39;t:&#39;, t.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x: (784,)
t: ()
</pre></div></div>
</div>
<p>そのときの返り値である2つの配列 <code class="docutils literal notranslate"><span class="pre">x</span></code> と <code class="docutils literal notranslate"><span class="pre">t</span></code>
のshapeを調べてみると，データはそれぞれ長さ784のベクトルとして格納されており，正解ラベルはスカラー値となっています．784は，<span class="math notranslate nohighlight">\(28 \times 28\)</span>で，28ピクセル四方の画像データの画素値を1列に並べたものになっています．</p>
<div class="section" id="SerialIteratorについて">
<h4>4.2.3.1. SerialIteratorについて<a class="headerlink" href="#SerialIteratorについて" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>Chainerにいくつか用意されているイテレータの一種である<code class="docutils literal notranslate"><span class="pre">SerialIterator</span></code>は，データセットの中のデータを順番に取り出してくる最もシンプルなイテレータです．<code class="docutils literal notranslate"><span class="pre">SerialIterator</span></code>
のコンストラクタ（クラスをインスタンス化するタイミングで呼ばれるメソッド）の引数にデータセットオブジェクトと，バッチサイズを取ります．このとき，渡したデータセットオブジェクトから，データを繰り返し読み出す必要がある場合は<code class="docutils literal notranslate"><span class="pre">repeat</span></code>引数を<code class="docutils literal notranslate"><span class="pre">True</span></code>とし，1周が終わったらそれ以上データを取り出したくない場合はこれを<code class="docutils literal notranslate"><span class="pre">False</span></code>とします．これは，主にvalidation用のデータセットに対して使うフラグです．デフォルトでは，<code class="docutils literal notranslate"><span class="pre">True</span></code>になっています．また，<code class="docutils literal notranslate"><span class="pre">shuffle</span></code>引数に<code class="docutils literal notranslate"><span class="pre">True</span></code>を渡すと，データセットから取り出されてくるデータの順番をエポックごとにランダムに変更します．<code class="docutils literal notranslate"><span class="pre">SerialIterator</span></code>の他にも，マルチプロセスで高速にデータを処理できるようにした<code class="docutils literal notranslate"><span class="pre">MultiprocessIterator</span></code>や<code class="docutils literal notranslate"><span class="pre">MultithreadIterator</span></code>など，複数のイテレータが用意されています．詳しくは以下を見てください．</p>
<ul class="simple">
<li><a class="reference external" href="https://docs.chainer.org/en/stable/reference/iterators.html">Chainerで使えるIterator一覧</a></li>
</ul>
</div>
</div>
<div class="section" id="ネットワークの定義">
<h3>4.2.4. ネットワークの定義<a class="headerlink" href="#ネットワークの定義" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは，学習させるネットワークを定義してみましょう．今回は，全結合層のみからなるニューラルネットワーク（多層パーセプトロン）を作ることにして，中間層のユニット数は100とします．今回用いるMNISTデータセットは0〜9までの数字のいずれかを意味する10種のラベルを持つことから，出力ユニット数は10とします．</p>
<p>ここで，ネットワークを定義するために必要な<code class="docutils literal notranslate"><span class="pre">Link</span></code>, <code class="docutils literal notranslate"><span class="pre">Function</span></code>,
<code class="docutils literal notranslate"><span class="pre">Chain</span></code>について簡単に説明します．</p>
<div class="section" id="LinkとFunction">
<h4>4.2.4.1. LinkとFunction<a class="headerlink" href="#LinkとFunction" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>Chainerでは，ニューラルネットワークの各層を，<code class="docutils literal notranslate"><span class="pre">Link</span></code>と<code class="docutils literal notranslate"><span class="pre">Function</span></code>に区別します．</p>
<ul class="simple">
<li>Linkは，<strong>パラメータを持つ</strong>関数です．</li>
<li>Functionは，<strong>パラメータを持たない</strong>関数です．</li>
</ul>
<p>これらを組み合わせてネットワークを記述します．パラメータを持つ層は，<code class="docutils literal notranslate"><span class="pre">chainer.links</span></code>モジュール以下に用意されています．例えば
<code class="docutils literal notranslate"><span class="pre">chainer.links.Linear</span></code>
は，前章で説明した全結合層に対応しており，内部に <code class="docutils literal notranslate"><span class="pre">W</span></code> と <code class="docutils literal notranslate"><span class="pre">b</span></code>
という学習できるパラメータが保持されています．パラメータを持たない層は，<code class="docutils literal notranslate"><span class="pre">chainer.functions</span></code>モジュール以下に用意されています．これらに簡単にアクセスするために，</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">chainer.links</span> <span class="k">as</span> <span class="nn">L</span>
<span class="kn">import</span> <span class="nn">chainer.functions</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
<p>と別名を与えて，<code class="docutils literal notranslate"><span class="pre">L.Convolution2D(...)</span></code>や<code class="docutils literal notranslate"><span class="pre">F.relu(...)</span></code>のように用いる慣習がありますが，特にこれが決まった書き方というわけではありません．</p>
</div>
<div class="section" id="Chain">
<h4>4.2.4.2. Chain<a class="headerlink" href="#Chain" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Chain</span></code>は，<strong>パラメータを持つ層（Link）をまとめておくためのクラス</strong>です．パラメータを持つということは，基本的にネットワークの学習の際にそれらを更新していく必要があるということです（更新されないパラメータを持たせることもできます）．Chainerでは，モデルのパラメータの更新は，<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>という機能が担います．その際，更新すべき全てのパラメータを簡単に発見できるように，<code class="docutils literal notranslate"><span class="pre">Chain</span></code>で一箇所にまとめておきます．</p>
</div>
<div class="section" id="同じ結果を保証する">
<h4>4.2.4.3. 同じ結果を保証する<a class="headerlink" href="#同じ結果を保証する" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>ネットワークを書き始める際に乱数シードを固定すると，本記事とほぼ同様の結果が再現できるようになります．（cuDNNが有効になっている環境下でより厳密に計算結果の再現性を保証したい場合は，<code class="docutils literal notranslate"><span class="pre">chainer.config.cudnn_deterministic</span></code>というConfiguringオプションについて知る必要があります．こちらのドキュメントを参照してください：<a class="reference external" href="https://docs.chainer.org/en/stable/reference/configuration.html?highlight=chainer.config.cudnn_deterministic">chainer.config.cudnn_deterministic</a>．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import random
import numpy
import chainer

def reset_seed(seed=0):
    random.seed(seed)
    numpy.random.seed(seed)
    if chainer.cuda.available:
        chainer.cuda.cupy.random.seed(seed)

reset_seed(0)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Chainを継承したネットワークの定義">
<h4>4.2.4.4. Chainを継承したネットワークの定義<a class="headerlink" href="#Chainを継承したネットワークの定義" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>Chainerでは，ネットワークは <code class="docutils literal notranslate"><span class="pre">Chain</span></code>
クラスを継承したクラスとして定義されることが一般的です． <code class="docutils literal notranslate"><span class="pre">Chain</span></code>
を継承することで，中間層のユニット数=100，出力ユニット数=10とした3層の多層パーセプトロンは以下のように書くことができます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import chainer
import chainer.links as L
import chainer.functions as F

class MLP(chainer.Chain):

    def __init__(self, n_mid_units=100, n_out=10):
        super(MLP, self).__init__()

        # パラメータを持つ層の登録
        with self.init_scope():
            self.l1 = L.Linear(None, n_mid_units)
            self.l2 = L.Linear(n_mid_units, n_mid_units)
            self.l3 = L.Linear(n_mid_units, n_out)

    def forward(self, x):
        # データを受け取った際のforward計算を書く
        h1 = F.relu(self.l1(x))
        h2 = F.relu(self.l2(h1))
        return self.l3(h2)

gpu_id = 0  # CPUを用いる場合は，この値を-1にしてください

net = MLP()

if gpu_id &gt;= 0:
    net.to_gpu(gpu_id)
</pre></div>
</div>
</div>
<p>継承した <code class="docutils literal notranslate"><span class="pre">MLP</span></code> クラスのコンストラクタ内で <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">self.init_scope()</span></code>
が呼ばれており，その中でネットワークに登場する<code class="docutils literal notranslate"><span class="pre">Link</span></code>
（具体的には，全結合層の <code class="docutils literal notranslate"><span class="pre">L.Linear</span></code>
）が定義されています．このような形で記述することで，<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>
はこれらが最適化対象となるパラメータを持つ層であると自動的に解釈してくれるようになります．</p>
<p>また， <code class="docutils literal notranslate"><span class="pre">forward</span></code>
というメソッドには，関数の名前の通り，ネットワークの順伝播を記述します．<code class="docutils literal notranslate"><span class="pre">forward</span></code>
の引数としてデータ <code class="docutils literal notranslate"><span class="pre">x</span></code>
を受け取り，出力として順伝播の計算結果を返すようにすることで， <code class="docutils literal notranslate"><span class="pre">MLP</span></code>
クラスをインスタンス化して作成されたオブジェクトを，関数のように使えるようになります．（例：<code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">net(data)</span></code>）</p>
<p>Chainerには数多くの <code class="docutils literal notranslate"><span class="pre">Function</span></code> や <code class="docutils literal notranslate"><span class="pre">Link</span></code>
が用意されています．ぜひ一度以下の一覧のページを見てみてください．</p>
<ul class="simple">
<li><a class="reference external" href="https://docs.chainer.org/en/stable/reference/functions.html">Chainerで使える関数(Function)一覧</a></li>
<li><a class="reference external" href="https://docs.chainer.org/en/stable/reference/links.html">Chainerで学習できる層(Link)一覧</a></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Link</span></code>には，ニューラルネットワークによく用いられる全結合層や畳み込み層，LSTMなどに加えて，ResNetや，VGGなどの有名なネットワーク構造も登録されています．また，<code class="docutils literal notranslate"><span class="pre">Function</span></code>には，ReLUなどの活性化関数や，画像の大きさをresizeする関数，サイン・コサインのような関数を始め，ネットワークの要素として使える関数が登録されています．Define-by-Runでは，データをネットワークに入力して順伝播計算を行ったあとに，データに適用された関数（パラメータあり・なし両方）の履歴をたどり直すことで，バックプロパゲーションによる勾配計算を行うパスを取得するため，パラメータを持たない関数であっても
<code class="docutils literal notranslate"><span class="pre">chainer.functions</span></code> に含まれているものを繋げて用いる必要があります．</p>
</div>
<div class="section" id="GPUで実行するには">
<h4>4.2.4.5. GPUで実行するには<a class="headerlink" href="#GPUで実行するには" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>深層学習で用いられるような多くのパラメータを持ったネットワークの学習には，GPUを用いることが一般的となっています．GPUを使うと，行列演算などの一部の処理をCPUに比べとても高速に行うことができます．Chainerで計算をGPUで行う方法は簡単です．<code class="docutils literal notranslate"><span class="pre">Chain</span></code>クラスは<code class="docutils literal notranslate"><span class="pre">to_gpu</span></code>メソッドを持ち，この引数にGPU
IDを指定すると，指定したGPU
IDのメモリ上にネットワークの全パラメータを転送します．こうしておくと，順伝播も学習の際のパラメータ更新なども全てGPU上で行われるようになります．GPU
IDとして-1を指定すると，CPUを使用します．</p>
</div>
<div class="section" id="入力側ユニット数の自動計算">
<h4>4.2.4.6. 入力側ユニット数の自動計算<a class="headerlink" href="#入力側ユニット数の自動計算" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>上のネットワーク定義で，最初のLinear層は第一引数に<code class="docutils literal notranslate"><span class="pre">None</span></code>が渡されています．このように引数を指定すると，データが最初にその層に入力されたタイミングで，自動的に必要な数の入力側のユニット数を判断し，
<code class="docutils literal notranslate"><span class="pre">n_input</span></code> <span class="math notranslate nohighlight">\(\times\)</span> <code class="docutils literal notranslate"><span class="pre">n_mid_units</span></code>
の大きさの行列を作成し，学習対象パラメータとして保持します．これは後々，畳み込み層を全結合層の前に配置する際などに便利な機能となるため，覚えておいてください．</p>
</div>
</div>
<div class="section" id="最適化手法の選択">
<h3>4.2.5. 最適化手法の選択<a class="headerlink" href="#最適化手法の選択" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは，上で定義したネットワークをMNISTデータセットを使って訓練してみましょう．学習時に用いる最適化の手法は数多く提案されていますが，Chainerは多くの手法を同一のインターフェースで利用できるよう，<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>という機能でそれらを提供しています．<code class="docutils literal notranslate"><span class="pre">chainer.optimizers</span></code>モジュール以下に定義されています．一覧はこちらにあります：</p>
<ul class="simple">
<li><a class="reference external" href="https://docs.chainer.org/en/stable/reference/optimizers.html">Chainerで使える最適化手法一覧</a></li>
</ul>
<p>ここでは最もシンプルな勾配降下法の手法である<code class="docutils literal notranslate"><span class="pre">optimizers.SGD</span></code>を用います．<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>のオブジェクトには，<code class="docutils literal notranslate"><span class="pre">setup</span></code>メソッドを使ってモデル（<code class="docutils literal notranslate"><span class="pre">Chain</span></code>オブジェクト）を渡します．こうすることで<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>に，何を最適化すればいいか把握させることができます．</p>
<p>他にもいろいろな最適化手法が手軽に試せるので，色々と試してみて結果の変化を見てみてください．例えば，下の<code class="docutils literal notranslate"><span class="pre">chainer.optimizers.SGD</span></code>のうち<code class="docutils literal notranslate"><span class="pre">SGD</span></code>の部分を<code class="docutils literal notranslate"><span class="pre">MomentumSGD</span></code>,
<code class="docutils literal notranslate"><span class="pre">RMSprop</span></code>,
<code class="docutils literal notranslate"><span class="pre">Adam</span></code>などに変えるだけで，最適化手法の違いがどのような学習曲線（ロスカーブとも言う．目的関数の値のプロットのこと）の違いを生むかなどを簡単に調べることができます．最適化の手法によっては，人が与える必要があった学習率を適切に自動決定するものもあります．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer import optimizers

optimizer = optimizers.SGD(lr=0.01).setup(net)
</pre></div>
</div>
</div>
<div class="section" id="学習率（learning-rate）">
<h4>4.2.5.1. 学習率（learning rate）<a class="headerlink" href="#学習率（learning-rate）" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>今回はSGDの<code class="docutils literal notranslate"><span class="pre">lr</span></code>という引数に <span class="math notranslate nohighlight">\(0.01\)</span>
を与えました．この値は学習率として知られ，モデルをうまく訓練して良いパフォーマンスを発揮させるために調整する必要がある重要な<strong>ハイパーパラメータ</strong>として知られています．ハイパーパラメータは学習されるパラメータとは異なり人が手で与える学習の設定に関するものやネットワークの構造に関するもののことを指します．</p>
</div>
</div>
<div class="section" id="学習の開始">
<h3>4.2.6. 学習の開始<a class="headerlink" href="#学習の開始" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>今回は0〜9の数字を区別する分類問題なので，<code class="docutils literal notranslate"><span class="pre">softmax_cross_entropy</span></code>という損失関数を使って最小化すべき損失を計算します．Softmax関数は，<span class="math notranslate nohighlight">\(d\)</span>次元のベクトル<span class="math notranslate nohighlight">\({\bf y} \in \mathbb{R}^d\)</span>が与えられたとき，その書く次元の値の合計が1になるように正規化することができます．すなわち，確率分布のような出力を任意の実数ベクトルから作ることができます．<span class="math notranslate nohighlight">\({\bf y}\)</span>の<span class="math notranslate nohighlight">\(i\)</span>番目の次元を<span class="math notranslate nohighlight">\(y_i\)</span>と書くと，Softmax関数は</p>
<div class="math notranslate nohighlight">
\[p_i = \frac{\exp(y_i)}{\sum_{j=1}^d \exp(y_j)}\]</div>
<p>と表せます．これによって正規化された出力ベクトルを入力が各クラスに所属する確率を表しているものと考え，正解の1-hotベクトルとの間で前章で説明した交差エントロピーを計算するのが
<code class="docutils literal notranslate"><span class="pre">softmax_cross_entropy</span></code> 関数です．</p>
<p>まずネットワークにデータを渡し，順伝播により予測値を計算します．そして，この予測値と入力データに対応する正解ラベルを損失関数に渡して損失（最小化したい値）を計算をします．損失は，<code class="docutils literal notranslate"><span class="pre">chainer.Variable</span></code>のオブジェクトとして得られます．この<code class="docutils literal notranslate"><span class="pre">Variable</span></code>は，<strong>過去の計算の履歴を覚えていて，辿れるようになっています</strong>．この仕組みが，Define-by-Run
<a class="reference external" href="http://learningsys.org/papers/LearningSys_2015_paper_33.pdf">[Tokui
2015]</a>とよばれる発明の中心的な役割を果たしています．</p>
<p>計算した損失に対する勾配を<strong>ネットワークに逆向きに計算していく</strong>処理は，Chainerではネットワークが出力した<code class="docutils literal notranslate"><span class="pre">Variable</span></code>から，<code class="docutils literal notranslate"><span class="pre">backward</span></code>メソッドを呼ぶだけで実現できます．これを呼ぶことで，<strong>誤差逆伝播用の計算グラフを構築</strong>し，途中のパラメータの勾配を連鎖率を使って計算してくれます．（詳しくは<a class="reference external" href="https://www.slideshare.net/mitmul/chainer-79942361">日本ソフトウェア科学会におけるチュートリアルの資料</a>をご覧ください．）</p>
<p>最後に，計算された各パラメータに対する勾配を用いて，<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>によってネットワークパラメータの更新（＝学習）が行われます．</p>
<p>まとめると，一連の更新処理の中で行われるのは，以下の4項目となります．</p>
<ol class="arabic simple">
<li>ネットワークにデータを渡して順伝播を計算し，出力<code class="docutils literal notranslate"><span class="pre">y</span></code>を得る</li>
<li>出力<code class="docutils literal notranslate"><span class="pre">y</span></code>と正解ラベル<code class="docutils literal notranslate"><span class="pre">t</span></code>を使って，最小化すべき損失を<code class="docutils literal notranslate"><span class="pre">softmax_cross_entropy</span></code>関数で計算する</li>
<li><code class="docutils literal notranslate"><span class="pre">softmax_cross_entropy</span></code>関数の出力（<code class="docutils literal notranslate"><span class="pre">Variable</span></code>）の<code class="docutils literal notranslate"><span class="pre">backward</span></code>メソッドを呼んで，ネットワークの全てのパラメータの勾配を誤差逆伝播法で計算する</li>
<li>Optimizerの<code class="docutils literal notranslate"><span class="pre">update</span></code>メソッドを呼び，3.で計算した勾配を使って全パラメータを更新する</li>
</ol>
<p>パラメータの更新は，上記ステップを繰り返すことで行われます．一度のパラメータ更新に用いられるデータは，ネットワークに入力された，ミニバッチとして束ねられたデータのみです．次々と新しいミニバッチを入力し，上記のステップを繰り返すことで，データセット全体を用いて学習を行います．この過程を<strong>学習ループ</strong>と呼んでいます．</p>
<div class="section" id="目的関数">
<h4>4.2.6.1. 目的関数<a class="headerlink" href="#目的関数" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>目的関数として，例えば分類問題ではなく回帰問題を解きたいような場合，<code class="docutils literal notranslate"><span class="pre">F.softmax_cross_entropy</span></code>の代わりに<code class="docutils literal notranslate"><span class="pre">F.mean_squared_error</span></code>などを用いることもできます．他にも，いろいろな問題設定に対応するために様々な損失関数がChainerには用意されています．こちらからその一覧を見ることができます：</p>
<ul class="simple">
<li><a class="reference external" href="http://docs.chainer.org/en/stable/reference/functions.html#loss-functions">Chainerで使える目的関数（損失関数）一覧</a></li>
</ul>
</div>
<div class="section" id="学習ループのコード">
<h4>4.2.6.2. 学習ループのコード<a class="headerlink" href="#学習ループのコード" title="このヘッドラインへのパーマリンク">¶</a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import numpy as np
from chainer.dataset import concat_examples
from chainer.cuda import to_cpu

max_epoch = 10

while train_iter.epoch &lt; max_epoch:

    # ---------- 学習の1イテレーション ----------
    train_batch = train_iter.next()
    x, t = concat_examples(train_batch, gpu_id)

    # 予測値の計算
    y = net(x)

    # 損失の計算
    loss = F.softmax_cross_entropy(y, t)

    # 勾配の計算
    net.cleargrads()
    loss.backward()

    # パラメータの更新
    optimizer.update()
    # --------------- ここまで ----------------

    # 1エポック終了ごとにValidationデータに対する予測精度を測って，
    # モデルの汎化性能が向上していることをチェックしよう
    if train_iter.is_new_epoch:  # 1 epochが終わったら

        # 損失の表示
        print(&#39;epoch:{:02d} train_loss:{:.4f} &#39;.format(
            train_iter.epoch, float(to_cpu(loss.data))), end=&#39;&#39;)

        valid_losses = []
        valid_accuracies = []
        while True:
            valid_batch = valid_iter.next()
            x_valid, t_valid = concat_examples(valid_batch, gpu_id)

            # Validationデータをforward
            with chainer.using_config(&#39;train&#39;, False), \
                    chainer.using_config(&#39;enable_backprop&#39;, False):
                y_valid = net(x_valid)

            # 損失を計算
            loss_valid = F.softmax_cross_entropy(y_valid, t_valid)
            valid_losses.append(to_cpu(loss_valid.array))

            # 精度を計算
            accuracy = F.accuracy(y_valid, t_valid)
            accuracy.to_cpu()
            valid_accuracies.append(accuracy.array)

            if valid_iter.is_new_epoch:
                valid_iter.reset()
                break

        print(&#39;val_loss:{:.4f} val_accuracy:{:.4f}&#39;.format(
            np.mean(valid_losses), np.mean(valid_accuracies)))

# テストデータでの評価
test_accuracies = []
while True:
    test_batch = test_iter.next()
    x_test, t_test = concat_examples(test_batch, gpu_id)

    # テストデータをforward
    with chainer.using_config(&#39;train&#39;, False), \
            chainer.using_config(&#39;enable_backprop&#39;, False):
        y_test = net(x_test)

    # 精度を計算
    accuracy = F.accuracy(y_valid, t_valid)
    accuracy.to_cpu()
    test_accuracies.append(accuracy.array)

    if test_iter.is_new_epoch:
        test_iter.reset()
        break

print(&#39;test_accuracy:{:.4f}&#39;.format(np.mean(test_accuracies)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch:01 train_loss:0.9191 val_loss:0.9743 val_accuracy:0.7979
epoch:02 train_loss:0.5390 val_loss:0.5334 val_accuracy:0.8647
epoch:03 train_loss:0.4004 val_loss:0.4231 val_accuracy:0.8847
epoch:04 train_loss:0.3318 val_loss:0.3741 val_accuracy:0.8942
epoch:05 train_loss:0.4590 val_loss:0.3454 val_accuracy:0.9001
epoch:06 train_loss:0.2474 val_loss:0.3273 val_accuracy:0.9072
epoch:07 train_loss:0.3298 val_loss:0.3108 val_accuracy:0.9124
epoch:08 train_loss:0.3800 val_loss:0.2989 val_accuracy:0.9148
epoch:09 train_loss:0.2985 val_loss:0.2884 val_accuracy:0.9178
epoch:10 train_loss:0.3218 val_loss:0.2802 val_accuracy:0.9203
test_accuracy:0.9375
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">val_accuracy</span></code>に着目してみると，最終的におよそ92%程度の精度で手書きの数字が分類できるようになりました．ここで言う精度とは，Validationデータセット中に
<span class="math notranslate nohighlight">\(N\)</span> 個のデータがあり分類結果が正しかったものが <span class="math notranslate nohighlight">\(M\)</span>
個あるとすると <span class="math notranslate nohighlight">\(M/N\)</span>
を指します．学習中は，各ループの終わりに始めに取り分けておいたValidationデータセットを使って精度をはかることで，モデルの汎化性能をチェックしています．汎化性能とは，主に未知のデータに対する性能の高さのことを意味します．学習終了後には，テスト用のデータセットを用いて，学習が完了したネットワークの評価を行います．テストデータでの評価結果は，およそ93.75%の正解率となりました．</p>
</div>
<div class="section" id="ValidationやTestを行う際の注意点">
<h4>4.2.6.3. ValidationやTestを行う際の注意点<a class="headerlink" href="#ValidationやTestを行う際の注意点" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>学習終了後の最終的な評価には，ハイパーパラメータ調整などにも用いられるValidationデータセットとはさらに別のTestデータセットを用います．TestデータセットはTrainingデータセットともValidationデータセットともデータの重複がないように用意しておきます．</p>
<p>さて，これまでは主に，「学習」のやり方について説明してきましたが，「評価」を行う際には注意すべき点があります．なぜなら，一部の関数や，計算過程において，学習時と評価時でその挙動が異なるためです．以下では，それらの挙動の違いを制御するための方法について説明します．</p>
<div class="section" id="chainer.using_config('train',-False)">
<h5>4.2.6.3.1. <code class="docutils literal notranslate"><span class="pre">chainer.using_config('train',</span> <span class="pre">False)</span></code><a class="headerlink" href="#chainer.using_config('train',-False)" title="このヘッドラインへのパーマリンク">¶</a></h5>
<p>先程の例では，学習時と推論時で動作が異なる関数は含まれていませんでしたが，Validationやテストのために推論を行うときは以下のように，<code class="docutils literal notranslate"><span class="pre">chainer.using_config('train',</span> <span class="pre">False)</span></code>をwith構文と共に使うことで，その中では<strong>対応する関数が推論モードとして動作</strong>することになります．これによって，学習時と推論時で挙動が異なる関数などが正しく推論のための動作をするようになります（例えば，<a class="reference external" href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)">Dropout</a>など）．詳しくはこちらの
<code class="docutils literal notranslate"><span class="pre">train</span></code> の項をお読みください：<a class="reference external" href="https://docs.chainer.org/en/stable/reference/configuration.html#configuration-keys">Configuration
Keys</a>．</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>with chainer.using_config(&#39;train&#39;, False):
    --- 何か推論処理 ---
</pre></div>
</div>
</div>
<div class="section" id="chainer.using_config('enable_backprop',-False)">
<h5>4.2.6.3.2. <code class="docutils literal notranslate"><span class="pre">chainer.using_config('enable_backprop',</span> <span class="pre">False)</span></code><a class="headerlink" href="#chainer.using_config('enable_backprop',-False)" title="このヘッドラインへのパーマリンク">¶</a></h5>
<p>評価のみ行うことを考えた場合，出力の計算後に損失関数の各パラメータについての勾配の情報は不要なため，<code class="docutils literal notranslate"><span class="pre">chainer.using_config('enable_backprop',</span> <span class="pre">False)</span></code>とすることで，<strong>無駄な計算グラフの構築が行われず，メモリ消費量を節約</strong>することができます．詳しくはこちらの
<code class="docutils literal notranslate"><span class="pre">enable_backprop</span></code> の項をお読みください：<a class="reference external" href="https://docs.chainer.org/en/stable/reference/configuration.html#configuration-keys">Configuration
Keys</a>．</p>
</div>
<div class="section" id="ChainerのConfig">
<h5>4.2.6.3.3. ChainerのConfig<a class="headerlink" href="#ChainerのConfig" title="このヘッドラインへのパーマリンク">¶</a></h5>
<p>Chainerにはこの他にも，いくつかのグローバルなConfigが用意されています．また，<code class="docutils literal notranslate"><span class="pre">chainer.config</span></code>以下にユーザが自由な設定値を置くこともできます．詳しくはこちらを一読してください：<a class="reference external" href="https://docs.chainer.org/en/stable/reference/configuration.html">Configuring
Chainer</a></p>
</div>
</div>
</div>
<div class="section" id="学習済みモデルの保存">
<h3>4.2.7. 学習済みモデルの保存<a class="headerlink" href="#学習済みモデルの保存" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>学習が終了後，その結果を保存します．Chainerには，2種類のフォーマットで学習済みネットワークをファイルに保存する機能が用意されています．一つはHDF5形式，もう一つはNumPyのNPZ形式で，ネットワークを保存します．今回は，追加ライブラリのインストールが必要なHDF5ではなく，NumPy標準機能で提供されているシリアライズ機能（<code class="docutils literal notranslate"><span class="pre">numpy.savez()</span></code>）を利用したNPZ形式でのモデルの保存を行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer import serializers

serializers.save_npz(&#39;my_mnist.model&#39;, net)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 保存されていることを確認
%ls -la my_mnist.model
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-rw-r--r-- 1 root root 333971 Dec 16 09:49 my_mnist.model
</pre></div></div>
</div>
</div>
<div class="section" id="保存したモデルを読み込んで推論">
<h3>4.2.8. 保存したモデルを読み込んで推論<a class="headerlink" href="#保存したモデルを読み込んで推論" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>学習が終了して保存したモデルを読み込み，推論を行う方法について説明します．はじめに，学習に利用したネットワークを再度インスタンス化して，そこにさきほど保存したNPZファイルを読み込ませます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># まず同じネットワークのオブジェクトを作る
infer_net = MLP()

# そのオブジェクトに保存済みパラメータをロードする
serializers.load_npz(&#39;my_mnist.model&#39;, infer_net)
</pre></div>
</div>
</div>
<p>以上で準備が整いました．それでは，試しにテストデータの中から一つ目の画像を取ってきて，それに対する分類を行ってみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>gpu_id = 0  # CPUで計算をしたい場合は，-1を指定してください

if gpu_id &gt;= 0:
    infer_net.to_gpu(gpu_id)

# 1つ目のテストデータを取り出します
x, t = test[0]  #  tは使わない

# どんな画像か表示してみます
plt.imshow(x.reshape(28, 28), cmap=&#39;gray&#39;)
plt.show()

# ミニバッチの形にする（複数の画像をまとめて推論に使いたい場合は，サイズnのミニバッチにしてまとめればよい）
print(&#39;元の形：&#39;, x.shape, end=&#39; -&gt; &#39;)

x = x[None, ...]

print(&#39;ミニバッチの形にしたあと：&#39;, x.shape)

# ネットワークと同じデバイス上にデータを送る
x = infer_net.xp.asarray(x)

# モデルのforward関数に渡す
with chainer.using_config(&#39;train&#39;, False), chainer.using_config(&#39;enable_backprop&#39;, False):
    y = infer_net(x)

# Variable形式で出てくるので中身を取り出す
y = y.array

# 結果をCPUに送る
y = to_cpu(y)

# 予測確率の最大値のインデックスを見る
pred_label = y.argmax(axis=1)

print(&#39;ネットワークの予測:&#39;, pred_label[0])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_54_0.png" src="../_images/notebooks_Introduction_to_Chainer_54_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
元の形： (784,) -&gt; ミニバッチの形にしたあと： (1, 784)
ネットワークの予測: 7
</pre></div></div>
</div>
<p>ネットワークの予測は7でした．画像を見る限り，正しく予測できていることが確認できます．</p>
</div>
</div>
<div class="section" id="Trainerの使用方法">
<h2>4.3. Trainerの使用方法<a class="headerlink" href="#Trainerの使用方法" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>Chainerは，これまで書いてきたような学習ループを隠蔽する<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>という機能を提供しています．これを使うと，学習ループを自ら書く必要がなくなり，また便利な拡張機能(<code class="docutils literal notranslate"><span class="pre">Extention</span></code>)を使うことで，学習過程での学習曲線の可視化や，ログの保存なども簡単に行うことができます．</p>
<div class="section" id="データセット・Iterator・ネットワークの準備">
<h3>4.3.1. データセット・Iterator・ネットワークの準備<a class="headerlink" href="#データセット・Iterator・ネットワークの準備" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>データセット，Iterator，ネットワークは，Trainerを使用する場合にも同様に準備します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>reset_seed(0)

train_val, test = mnist.get_mnist()
train, valid = split_dataset_random(train_val, 50000, seed=0)

batchsize = 128

train_iter = iterators.SerialIterator(train, batchsize)
valid_iter = iterators.SerialIterator(valid, batchsize, False, False)
test_iter = iterators.SerialIterator(test, batchsize, False, False)

gpu_id = 0  # CPUを用いたい場合は，-1を指定してください

net = MLP()

if gpu_id &gt;= 0:
    net.to_gpu(gpu_id)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Updaterの準備">
<h3>4.3.2. Updaterの準備<a class="headerlink" href="#Updaterの準備" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>学習ループを自分で書く場合の学習ステップについて再度確認すると，「データセットからミニバッチを作成」「ネットワークに入力して予測を出力」「正解と比較し誤差を計算」「バックワード（誤差逆伝播）を実行」「<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>によってパラメータを更新」という一連のステップを，以下のように書いていました．</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ---------- 学習の1イテレーション ----------</span>
<span class="n">train_batch</span> <span class="o">=</span> <span class="n">train_iter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">concat_examples</span><span class="p">(</span><span class="n">train_batch</span><span class="p">,</span> <span class="n">gpu_id</span><span class="p">)</span>

<span class="c1"># 予測値の計算</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 損失の計算</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="c1"># 勾配の計算</span>
<span class="n">net</span><span class="o">.</span><span class="n">cleargrads</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># パラメータの更新</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
<p>Chainerの機能として提供されている<code class="docutils literal notranslate"><span class="pre">Updater</span></code>を用いることで，これらの一連の処理を簡単に書けるようになります．<code class="docutils literal notranslate"><span class="pre">Updater</span></code>には<code class="docutils literal notranslate"><span class="pre">Iterator</span></code>と<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>を渡します．
<code class="docutils literal notranslate"><span class="pre">Iterator</span></code>はデータセットオブジェクトを持っているため，そこからミニバッチを作成します．<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>は最適化対象のネットワークを持っているため，それを使って順伝播と誤差計算・パラメータのアップデートをすることができます．従って，この2つを渡すことで，<code class="docutils literal notranslate"><span class="pre">Updater</span></code>内で全ての処理が完結します．さっそく，<code class="docutils literal notranslate"><span class="pre">Updater</span></code>オブジェクトを作成してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer import training

gpu_id = 0  # CPUを使いたい場合は-1を指定してください

# ネットワークをClassifierで包んで，損失の計算などをモデルに含める
net = L.Classifier(net)

# 最適化手法の選択
optimizer = optimizers.SGD(lr=0.01).setup(net)

# UpdaterにIteratorとOptimizerを渡す
updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)
</pre></div>
</div>
</div>
<div class="section" id="損失計算のためのChain">
<h4>4.3.2.1. 損失計算のためのChain<a class="headerlink" href="#損失計算のためのChain" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>ここでは，ネットワークを<code class="docutils literal notranslate"><span class="pre">L.Classifier</span></code>で包んでいます．<code class="docutils literal notranslate"><span class="pre">L.Classifier</span></code>は，渡されたネットワーク自体を<code class="docutils literal notranslate"><span class="pre">predictor</span></code>というattributeに持ち，<strong>損失計算を行う機能を追加してくれます</strong>．こうすることで，<code class="docutils literal notranslate"><span class="pre">net()</span></code>はデータ<code class="docutils literal notranslate"><span class="pre">x</span></code>だけでなくラベル<code class="docutils literal notranslate"><span class="pre">t</span></code>も取るようになり，受け取ったデータを<code class="docutils literal notranslate"><span class="pre">predictor</span></code>に通して予測値を計算し，正解ラベル<code class="docutils literal notranslate"><span class="pre">t</span></code>と比較して<strong>損失のVariableを返します．</strong>損失関数として何を用いるかはデフォルトでは<code class="docutils literal notranslate"><span class="pre">F.softmax_cross_entropy</span></code>となっていますが，<code class="docutils literal notranslate"><span class="pre">L.Classifier</span></code>の引数<code class="docutils literal notranslate"><span class="pre">lossfunc</span></code>に損失計算を行う関数を渡してやれば変更することができ，（Classifierという名前ながら）回帰問題などの損失計算機能の追加にも使うことができます．（<code class="docutils literal notranslate"><span class="pre">L.Classifier(net,</span> <span class="pre">lossfun=L.mean_squared_error,</span> <span class="pre">compute_accuracy=False)</span></code>のようにする）</p>
<p><code class="docutils literal notranslate"><span class="pre">StandardUpdater</span></code>は前述のような<code class="docutils literal notranslate"><span class="pre">Updater</span></code>の担当する処理を遂行するための最もシンプルなクラスです．この他にも複数のGPUを用いるための<code class="docutils literal notranslate"><span class="pre">ParallelUpdater</span></code>などが用意されています．</p>
</div>
</div>
<div class="section" id="Trainerの準備">
<h3>4.3.3. Trainerの準備<a class="headerlink" href="#Trainerの準備" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>実際に学習ループ部分を隠蔽しているのは<code class="docutils literal notranslate"><span class="pre">Updater</span></code>ですが，<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>はさらに<code class="docutils literal notranslate"><span class="pre">Updater</span></code>を受け取って学習全体の管理を行う機能を提供しています．例えば，<strong>データセットを何周したら学習を終了するか(stop_trigger)</strong>
や，<strong>途中の損失の値をどのファイルに保存したいか</strong>，<strong>学習曲線を可視化した画像ファイルを保存するかどうか</strong>など，学習全体の設定として必須・もしくはあると便利な色々な機能を提供しています．</p>
<p>必須なものとしては学習終了のタイミングを指定する<code class="docutils literal notranslate"><span class="pre">stop_trigger</span></code>がありますが，これは<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>オブジェクトを作成するときのコンストラクタで指定します．指定の方法は単純で，<code class="docutils literal notranslate"><span class="pre">(長さ,</span> <span class="pre">単位)</span></code>という形のタプルを与えればよいだけです．「長さ」には数字を，「単位」には<code class="docutils literal notranslate"><span class="pre">'iteration'</span></code>もしくは<code class="docutils literal notranslate"><span class="pre">'epoch'</span></code>のいずれかの文字列を指定します．こうすると，たとえば100
epoch（データセット100周）で学習を終了してください，とか，1000
iteration（1000回更新）で学習を終了してください，といったことが指定できます．<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>を作るときに，<code class="docutils literal notranslate"><span class="pre">stop_trigger</span></code>を指定しないと，学習は自動的には止まりません．</p>
<p>では，実際に<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>オブジェクトを作ってみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>max_epoch = 10

# TrainerにUpdaterを渡す
trainer = training.Trainer(
    updater, (max_epoch, &#39;epoch&#39;), out=&#39;results/mnist_result&#39;)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">out</span></code>引数では，この次に説明する<code class="docutils literal notranslate"><span class="pre">Extension</span></code>を使って，ログファイルや損失の変化の過程を描画したグラフの画像ファイルなどを保存するディレクトリを指定しています．</p>
<p>Trainerと，その内側にあるいろいろなオブジェクトの関係は，図にまとめると以下のようになっています．このイメージを持っておくと自分で部分的に改造したりする際に便利だと思います．</p>
<div class="figure" id="id29">
<img alt="Trainerに関連するオブジェクト間の関係図" src="https://qiita-image-store.s3.amazonaws.com/0/17934/a751df31-b999-f692-d839-488c26b1c48a.png" />
<p class="caption"><span class="caption-text">Trainerに関連するオブジェクト間の関係図</span></p>
</div>
</div>
<div class="section" id="TrainerにExtensionを追加">
<h3>4.3.4. TrainerにExtensionを追加<a class="headerlink" href="#TrainerにExtensionを追加" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code>を使う利点として，</p>
<ul class="simple">
<li>ログを自動的にファイルに保存（<code class="docutils literal notranslate"><span class="pre">LogReport</span></code>)</li>
<li>ターミナルに定期的に損失などの情報を表示（<code class="docutils literal notranslate"><span class="pre">PrintReport</span></code>）</li>
<li>損失を定期的にグラフで可視化して画像として保存（<code class="docutils literal notranslate"><span class="pre">PlotReport</span></code>)</li>
<li>定期的にモデルやOptimizerの状態を自動シリアライズ（<code class="docutils literal notranslate"><span class="pre">snapshot</span></code>）</li>
<li>学習の進捗を示すプログレスバーを表示（<code class="docutils literal notranslate"><span class="pre">ProgressBar</span></code>）</li>
<li>ネットワークの構造をGraphvizのdot形式で保存（<code class="docutils literal notranslate"><span class="pre">dump_graph</span></code>）</li>
<li>ネットワークのパラメータの平均や分散などの統計情報を出力（<code class="docutils literal notranslate"><span class="pre">ParameterStatistics</span></code>）</li>
</ul>
<p>などの様々な便利な機能を簡単に利用することができる点があります．これらの機能を利用するには，<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>オブジェクトに対して<code class="docutils literal notranslate"><span class="pre">extend</span></code>メソッドを使って追加したい<code class="docutils literal notranslate"><span class="pre">Extension</span></code>のオブジェクトを渡すだけです．では実際に幾つかの<code class="docutils literal notranslate"><span class="pre">Extension</span></code>を追加してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.training import extensions

trainer.extend(extensions.LogReport())
trainer.extend(extensions.snapshot(filename=&#39;snapshot_epoch-{.updater.epoch}&#39;))
trainer.extend(extensions.Evaluator(valid_iter, net, device=gpu_id), name=&#39;val&#39;)
trainer.extend(extensions.PrintReport([&#39;epoch&#39;, &#39;main/loss&#39;, &#39;main/accuracy&#39;, &#39;val/main/loss&#39;, &#39;val/main/accuracy&#39;, &#39;l1/W/data/std&#39;, &#39;elapsed_time&#39;]))
trainer.extend(extensions.ParameterStatistics(net.predictor.l1, {&#39;std&#39;: np.std}))
trainer.extend(extensions.PlotReport([&#39;l1/W/data/std&#39;], x_key=&#39;epoch&#39;, file_name=&#39;std.png&#39;))
trainer.extend(extensions.PlotReport([&#39;main/loss&#39;, &#39;val/main/loss&#39;], x_key=&#39;epoch&#39;, file_name=&#39;loss.png&#39;))
trainer.extend(extensions.PlotReport([&#39;main/accuracy&#39;, &#39;val/main/accuracy&#39;], x_key=&#39;epoch&#39;, file_name=&#39;accuracy.png&#39;))
trainer.extend(extensions.dump_graph(&#39;main/loss&#39;))
</pre></div>
</div>
</div>
<div class="section" id="LogReport">
<h4>4.3.4.1. <code class="docutils literal notranslate"><span class="pre">LogReport</span></code><a class="headerlink" href="#LogReport" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">epoch</span></code>や<code class="docutils literal notranslate"><span class="pre">iteration</span></code>ごとの<code class="docutils literal notranslate"><span class="pre">loss</span></code>,
<code class="docutils literal notranslate"><span class="pre">accuracy</span></code>などを自動的に集計し，<code class="docutils literal notranslate"><span class="pre">log</span></code>というファイル名で保存します．</p>
</div>
<div class="section" id="snapshot">
<h4>4.3.4.2. <code class="docutils literal notranslate"><span class="pre">snapshot</span></code><a class="headerlink" href="#snapshot" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code>オブジェクトを指定されたタイミング（デフォルトでは1エポックごと）で保存します．<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>オブジェクトは上述のように<code class="docutils literal notranslate"><span class="pre">Updater</span></code>を持っており，この中に<code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>とモデルが保持されているため，この<code class="docutils literal notranslate"><span class="pre">Extension</span></code>でスナップショットをとっておけば，その時点から学習を再開させたり，学習済みモデルを使った推論などが可能になります．</p>
</div>
<div class="section" id="dump_graph">
<h4>4.3.4.3. <code class="docutils literal notranslate"><span class="pre">dump_graph</span></code><a class="headerlink" href="#dump_graph" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>指定された<code class="docutils literal notranslate"><span class="pre">Variable</span></code>オブジェクトから辿れる計算グラフをGraphvizのdot形式で保存します．</p>
</div>
<div class="section" id="Evaluator">
<h4>4.3.4.4. <code class="docutils literal notranslate"><span class="pre">Evaluator</span></code><a class="headerlink" href="#Evaluator" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>評価用のデータセットの<code class="docutils literal notranslate"><span class="pre">Iterator</span></code>と，学習に使うモデルのオブジェクトを渡しておくことで，学習中のモデルを指定されたタイミングで評価用データセットを用いて評価します．内部では，<code class="docutils literal notranslate"><span class="pre">chainer.config.using_config('train',</span> <span class="pre">False)</span></code>が自動的に行われます．<code class="docutils literal notranslate"><span class="pre">backprop_enable</span></code>を<code class="docutils literal notranslate"><span class="pre">False</span></code>にすることは行われないため，メモリ使用効率はデフォルトでは最適ではありませんが，基本的には<code class="docutils literal notranslate"><span class="pre">Evaluator</span></code>を使えば評価を行うという点において問題はありません．</p>
</div>
<div class="section" id="PrintReport">
<h4>4.3.4.5. <code class="docutils literal notranslate"><span class="pre">PrintReport</span></code><a class="headerlink" href="#PrintReport" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">LogReport</span></code>と同様に集計された値を標準出力に出力します．この際，どの値を出力するかをリストの形で与えます．</p>
</div>
<div class="section" id="PlotReport">
<h4>4.3.4.6. <code class="docutils literal notranslate"><span class="pre">PlotReport</span></code><a class="headerlink" href="#PlotReport" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>引数のリストで指定された値の変遷を<code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>ライブラリを使ってグラフに描画し，出力ディレクトリに<code class="docutils literal notranslate"><span class="pre">file_name</span></code>引数で指定されたファイル名で画像として保存します．</p>
</div>
<div class="section" id="ParameterStatistics">
<h4>4.3.4.7. <code class="docutils literal notranslate"><span class="pre">ParameterStatistics</span></code><a class="headerlink" href="#ParameterStatistics" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>指定したレイヤ（Link）が持つパラメータの平均・分散・最小値・最大値などなどの統計情報を計算して，ログに保存します．パラメータが発散していないかなどをチェックするのに便利です．</p>
<hr class="docutils" />
<p>これらの<code class="docutils literal notranslate"><span class="pre">Extension</span></code>は，ここで紹介した以外にも，例えば<code class="docutils literal notranslate"><span class="pre">trigger</span></code>によって個別に作動するタイミングを指定できるなどのいくつかのオプションを持っており，より柔軟に組み合わせることができます．詳しくは公式のドキュメントを見てください．</p>
<ul class="simple">
<li><a class="reference external" href="http://docs.chainer.org/en/stable/reference/extensions.html">ChainerのTrainer
extension一覧</a></li>
</ul>
</div>
</div>
<div class="section" id="学習の開始-(Trainer利用)">
<h3>4.3.5. 学習の開始 (Trainer利用)<a class="headerlink" href="#学習の開始-(Trainer利用)" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>学習を開始するために，<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>オブジェクトのメソッド<code class="docutils literal notranslate"><span class="pre">run</span></code>を実行してください．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [24]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  l1/W/data/std  elapsed_time
1           1.66917     0.599884       0.93892        0.806764           0.0359232      8.77465
2           0.673347    0.843211       0.519291       0.86699            0.0366054      16.4562
3           0.459921    0.878686       0.414858       0.887658           0.037035       24.183
4           0.389535    0.893262       0.370489       0.896855           0.037301       31.9615
5           0.353169    0.901255       0.342332       0.904569           0.03749        39.6508
6           0.330151    0.90609        0.32213        0.909711           0.037639       47.3595
7           0.312338    0.910846       0.306804       0.91337            0.0377671      55.1196
8           0.298136    0.914663       0.295119       0.915843           0.0378811      62.8246
9           0.285837    0.917559       0.284161       0.918513           0.0379865      70.5711
10          0.275229    0.921116       0.27476        0.921776           0.0380852      78.4846
</pre></div></div>
</div>
<p>学習ループを自分で書いた場合よりも遥かに簡単に，同様の結果を得ることができました．さらに，<code class="docutils literal notranslate"><span class="pre">Extension</span></code>の機能を利用することで，様々なスコアや，学習曲線の可視化も自動で出力されます．</p>
<p>では，保存されている損失のグラフを確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [25]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from IPython.display import Image
Image(filename=&#39;results/mnist_result/loss.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[25]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_78_0.png" src="../_images/notebooks_Introduction_to_Chainer_78_0.png" />
</div>
</div>
<p>精度のグラフも見てみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [26]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/mnist_result/accuracy.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[26]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_80_0.png" src="../_images/notebooks_Introduction_to_Chainer_80_0.png" />
</div>
</div>
<p>もう少し学習を続ければ，さらに精度の向上が期待できそうです．</p>
<p>最後に，<code class="docutils literal notranslate"><span class="pre">dump_graph</span></code>という<code class="docutils literal notranslate"><span class="pre">Extension</span></code>によって出力された計算グラフのファイルを，<code class="docutils literal notranslate"><span class="pre">Graphviz</span></code>で画像化してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!dot -Tpng results/mnist_result/cg.dot -o results/mnist_result/cg.png
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [28]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/mnist_result/cg.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[28]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_83_0.png" src="../_images/notebooks_Introduction_to_Chainer_83_0.png" />
</div>
</div>
<p>データやパラメータが関数に次々と渡され，損失が出力されるまでの一連の計算過程が確認できます．</p>
</div>
<div class="section" id="テストデータ評価">
<h3>4.3.6. テストデータ評価<a class="headerlink" href="#テストデータ評価" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>Validationデータに対する評価を学習中に行うために使用されるEvaluatorは，Trainerと関係なく独立して使うこともできます．以下のようにして<code class="docutils literal notranslate"><span class="pre">Iterator</span></code>とネットワークのオブジェクト（<code class="docutils literal notranslate"><span class="pre">net</span></code>），使用するデバイスIDを渡してEvaluatorオブジェクトを作成し，これを関数として実行するだけです．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [29]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>test_evaluator = extensions.Evaluator(test_iter, net, device=gpu_id)
results = test_evaluator()
print(&#39;Test accuracy:&#39;, results[&#39;main/accuracy&#39;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Test accuracy: 0.92494065
</pre></div></div>
</div>
</div>
<div class="section" id="学習済みモデルで推論する">
<h3>4.3.7. 学習済みモデルで推論する<a class="headerlink" href="#学習済みモデルで推論する" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは，Trainer
Extensionのsnapshotが保存した学習済みパラメータを読み込んで，以前と同様に1番目のテストデータで推論を行ってみましょう．</p>
<p>ここで一点注意が必要ですが，snapshotが保存するnpzファイルはTrainer全体のスナップショットとなっており，学習の再開に必要となるextensionの内部のパラメータなども一緒に保存されています．しかし，今回はネットワークのパラメータだけを読み込めば良いので，
<code class="docutils literal notranslate"><span class="pre">serializers.load_npz()</span></code>
の<code class="docutils literal notranslate"><span class="pre">path</span></code>引数にネットワーク部分までのパスを指定します．こうすることで，ネットワークのオブジェクトにパラメータだけを読み込ませることができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [30]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>reset_seed(0)

infer_net = MLP()
serializers.load_npz(
    &#39;results/mnist_result/snapshot_epoch-10&#39;,
    infer_net, path=&#39;updater/model:main/predictor/&#39;)

if gpu_id &gt;= 0:
    infer_net.to_gpu(gpu_id)

x, t = test[0]
plt.imshow(x.reshape(28, 28), cmap=&#39;gray&#39;)
plt.show()

x = infer_net.xp.asarray(x[None, ...])
with chainer.using_config(&#39;train&#39;, False), chainer.using_config(&#39;enable_backprop&#39;, False):
    y = infer_net(x)
y = to_cpu(y.array)

print(&#39;予測ラベル:&#39;, y.argmax(axis=1)[0])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_88_0.png" src="../_images/notebooks_Introduction_to_Chainer_88_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
予測ラベル: 7
</pre></div></div>
</div>
<p>無事正解できていることが確認できました．</p>
</div>
</div>
<div class="section" id="新しいネットワークの利用">
<h2>4.4. 新しいネットワークの利用<a class="headerlink" href="#新しいネットワークの利用" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここでは，MNISTデータセットではなくCIFAR10という32x32サイズの小さなカラー画像に10クラスのいずれかのラベルがついたデータセットを用いて，いろいろなモデルを自分で書いて試行錯誤する流れを体験してみます．</p>
<table border="1" class="docutils">
<colgroup>
<col width="13%" />
<col width="16%" />
<col width="9%" />
<col width="8%" />
<col width="9%" />
<col width="8%" />
<col width="9%" />
<col width="10%" />
<col width="9%" />
<col width="10%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">airplane</th>
<th class="head">automobile</th>
<th class="head">bird</th>
<th class="head">cat</th>
<th class="head">deer</th>
<th class="head">dog</th>
<th class="head">frog</th>
<th class="head">horse</th>
<th class="head">ship</th>
<th class="head">truck</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><img alt="Airplane" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png" /></td>
<td><img alt="Automobile" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png" /></td>
<td><img alt="Bird" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png" /></td>
<td><img alt="Cat" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png" /></td>
<td><img alt="Deer" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png" /></td>
<td><img alt="Dog" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png" /></td>
<td><img alt="Frog" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png" /></td>
<td><img alt="Horse" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png" /></td>
<td><img alt="Ship" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png" /></td>
<td><img alt="Truck" src="https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png" /></td>
</tr>
</tbody>
</table>
<div class="section" id="新しいネットワークの定義">
<h3>4.4.1. 新しいネットワークの定義<a class="headerlink" href="#新しいネットワークの定義" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここでは，さきほど試した全結合層だけからなるネットワークではなく，前章で紹介した，畳込み層を持つネットワークを定義してみます．3つの畳み込み層を持ち，2つの全結合層がそのあとに続いています．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class MyNet(chainer.Chain):

    def __init__(self, n_out):
        super(MyNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 32, 3, 3, 1)
            self.conv2 = L.Convolution2D(32, 64, 3, 3, 1)
            self.conv3 = L.Convolution2D(64, 128, 3, 3, 1)
            self.fc4 = L.Linear(None, 1000)
            self.fc5 = L.Linear(1000, n_out)

    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(h))
        h = F.relu(self.conv3(h))
        h = F.relu(self.fc4(h))
        h = self.fc5(h)
        return h
</pre></div>
</div>
</div>
</div>
<div class="section" id="学習">
<h3>4.4.2. 学習<a class="headerlink" href="#学習" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここで，あとから別のネットワークも簡単に同じ設定で訓練できるよう，<code class="docutils literal notranslate"><span class="pre">train</span></code>関数を作っておきます．これは，</p>
<ul class="simple">
<li>ネットワークのオブジェクト</li>
<li>バッチサイズ</li>
<li>使用するGPU ID</li>
<li>学習を終了するエポック数</li>
<li>データセットオブジェクト</li>
<li>学習率の初期値</li>
<li>学習率減衰のタイミング</li>
</ul>
<p>などを渡すと，内部で<code class="docutils literal notranslate"><span class="pre">Trainer</span></code>を用いて渡されたデータセットを使ってネットワークを訓練し，学習が終了した状態のネットワークを返してくれる関数です．<code class="docutils literal notranslate"><span class="pre">Trainer.run()</span></code>が終了した後に，テストデータセットを使って評価まで行ってくれます．先程のMNISTでの例と違い，最適化手法にはMomentumSGDを用い，<code class="docutils literal notranslate"><span class="pre">ExponentialShift</span></code>という<code class="docutils literal notranslate"><span class="pre">Extention</span></code>を使って，指定したタイミングごとに学習率を減衰させるようにしてみます．</p>
<p>また，ここでは<code class="docutils literal notranslate"><span class="pre">cifar.get_cifar10()</span></code>が返す学習用データセットのうち9割のデータを<code class="docutils literal notranslate"><span class="pre">train</span></code>，残りの1割を<code class="docutils literal notranslate"><span class="pre">valid</span></code>として使うようにしています．</p>
<p>この<code class="docutils literal notranslate"><span class="pre">train</span></code>関数を用いて，上で定義した<code class="docutils literal notranslate"><span class="pre">MyModel</span></code>モデルを訓練してみます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.datasets import cifar


def train(network_object, batchsize=128, gpu_id=0, max_epoch=20, train_dataset=None, valid_dataset=None, test_dataset=None, postfix=&#39;&#39;, base_lr=0.01, lr_decay=None, snapshot=None):

    # 1. Dataset
    if train_dataset is None and valid_dataset is None and test_dataset is None:
        train_val, test = cifar.get_cifar10()
        train_size = int(len(train_val) * 0.9)
        train, valid = split_dataset_random(train_val, train_size, seed=0)
    else:
        train, valid, test = train_dataset, valid_dataset, test_dataset

    # 2. Iterator
    train_iter = iterators.MultiprocessIterator(train, batchsize)
    valid_iter = iterators.MultiprocessIterator(valid, batchsize, False, False)

    # 3. Model
    net = L.Classifier(network_object)

    # 4. Optimizer
    optimizer = optimizers.MomentumSGD(lr=base_lr).setup(net)
    optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005))

    # 5. Updater
    updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)

    # 6. Trainer
    trainer = training.Trainer(updater, (max_epoch, &#39;epoch&#39;), out=&#39;results/{}_cifar10_{}result&#39;.format(network_object.__class__.__name__, postfix))

    # 7. Trainer extensions
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.observe_lr())
    trainer.extend(extensions.snapshot(filename=&#39;snapshot_epoch_{.updater.epoch}&#39;), trigger=(10, &#39;epoch&#39;))
    trainer.extend(extensions.Evaluator(valid_iter, net, device=gpu_id), name=&#39;val&#39;)
    trainer.extend(extensions.PrintReport([&#39;epoch&#39;, &#39;main/loss&#39;, &#39;main/accuracy&#39;, &#39;val/main/loss&#39;, &#39;val/main/accuracy&#39;, &#39;elapsed_time&#39;, &#39;lr&#39;]))
    trainer.extend(extensions.PlotReport([&#39;main/loss&#39;, &#39;val/main/loss&#39;], x_key=&#39;epoch&#39;, file_name=&#39;loss.png&#39;))
    trainer.extend(extensions.PlotReport([&#39;main/accuracy&#39;, &#39;val/main/accuracy&#39;], x_key=&#39;epoch&#39;, file_name=&#39;accuracy.png&#39;))
    if lr_decay is not None:
        trainer.extend(extensions.ExponentialShift(&#39;lr&#39;, 0.1), trigger=lr_decay)
    if snapshot is not None:
        chainer.serializers.load_npz(snapshot, trainer)
    trainer.run()
    del trainer

    # 8. Evaluation
    test_iter = iterators.MultiprocessIterator(test, batchsize, False, False)
    test_evaluator = extensions.Evaluator(test_iter, net, device=gpu_id)
    results = test_evaluator()
    print(&#39;Test accuracy:&#39;, results[&#39;main/accuracy&#39;])

    return net
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [33]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>net = train(MyNet(10), gpu_id=0)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Downloading from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz...
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr
1           1.92592     0.30471        1.72211        0.400586           10.1106       0.01
2           1.60862     0.422408       1.52882        0.462305           19.5112       0.01
3           1.47156     0.469173       1.47382        0.481445           28.7014       0.01
4           1.39201     0.500333       1.39246        0.501562           38.0033       0.01
5           1.32716     0.527066       1.37835        0.510352           47.2195       0.01
6           1.26649     0.546875       1.35262        0.523047           56.443        0.01
7           1.21219     0.567508       1.25813        0.55918            65.6184       0.01
8           1.16413     0.584073       1.23688        0.566992           74.845        0.01
9           1.12082     0.602295       1.24074        0.562695           84.2727       0.01
10          1.07053     0.619235       1.22592        0.561914           93.4938       0.01
11          1.02863     0.636009       1.17342        0.588867           103.122       0.01
12          0.981062    0.651732       1.19127        0.577734           112.311       0.01
13          0.935378    0.668879       1.15679        0.591797           121.544       0.01
14          0.901103    0.681913       1.22091        0.575781           130.724       0.01
15          0.856386    0.697687       1.2014         0.58418            139.901       0.01
16          0.813217    0.711806       1.19123        0.591406           149.075       0.01
17          0.764969    0.729892       1.19881        0.594727           158.33        0.01
18          0.718949    0.745605       1.19928        0.595898           167.642       0.01
19          0.669597    0.764712       1.2195         0.60625            176.827       0.01
20          0.624032    0.780895       1.22516        0.598828           186.019       0.01
Test accuracy: 0.6075949
</pre></div></div>
</div>
<p>学習が20エポックまで終わりました．損失と精度のプロットを見てみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [34]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/MyNet_cifar10_result/loss.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[34]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_97_0.png" src="../_images/notebooks_Introduction_to_Chainer_97_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [35]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/MyNet_cifar10_result/accuracy.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[35]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_98_0.png" src="../_images/notebooks_Introduction_to_Chainer_98_0.png" />
</div>
</div>
<p>学習データでの精度（<code class="docutils literal notranslate"><span class="pre">main/accuracy</span></code>)は77%程度まで到達していますが，テストデータでの損失（<code class="docutils literal notranslate"><span class="pre">val/main/loss</span></code>）は途中から下げ止まり，精度（<code class="docutils literal notranslate"><span class="pre">val/main/accuracy</span></code>）も60%前後で頭打ちになってしまっています．表示されたログの最後の行を確認すると，テストデータでの精度も同様に60%程度となっています．学習データでは精度が良いが，
テストデータでは精度が良くない場合，<strong>モデルが学習データにオーバーフィッティングしている</strong>と考えられます．</p>
</div>
<div class="section" id="学習済みネットワークを使った予測">
<h3>4.4.3. 学習済みネットワークを使った予測<a class="headerlink" href="#学習済みネットワークを使った予測" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>テスト精度は60%程度でしたが，試しにこの学習済みネットワークを使っていくつかのテスト画像を分類させてみましょう．あとで使いまわせるように<code class="docutils literal notranslate"><span class="pre">predict</span></code>関数を作っておきます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [36]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cls_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;,
             &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def predict(net, image_id):
    _, test = cifar.get_cifar10()
    x, t = test[image_id]
    net.to_cpu()
    with chainer.using_config(&#39;train&#39;, False), chainer.using_config(&#39;enable_backprop&#39;, False):
        y = net.predictor(x[None, ...]).data.argmax(axis=1)[0]

    plt.imshow(x.transpose(1, 2, 0))
    plt.show()
    print(&#39;predicted_label:&#39;, cls_names[y])
    print(&#39;answer:&#39;, cls_names[t])

for i in range(10, 15):
    predict(net, i)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_101_0.png" src="../_images/notebooks_Introduction_to_Chainer_101_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predicted_label: airplane
answer: airplane
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_101_2.png" src="../_images/notebooks_Introduction_to_Chainer_101_2.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predicted_label: truck
answer: truck
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_101_4.png" src="../_images/notebooks_Introduction_to_Chainer_101_4.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predicted_label: dog
answer: dog
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_101_6.png" src="../_images/notebooks_Introduction_to_Chainer_101_6.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predicted_label: horse
answer: horse
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_101_8.png" src="../_images/notebooks_Introduction_to_Chainer_101_8.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predicted_label: truck
answer: truck
</pre></div></div>
</div>
<p>うまく分類できているものもあれば，そうでないものもありました．ネットワークの学習に使用したデータセット上ではほぼ百発百中で正解できても，未知のデータ，すなわちテストデータセットの画像に対して高精度な予測ができなければ意味がありません．テストデータでの精度は，モデルの<strong>汎化性能</strong>に関係していると言われています．</p>
<p>どうすれば高い汎化性能を持つネットワークを設計し，学習することができるでしょうか？これは非常に難しい問いですが，機械学習を使った応用を考えるとき，最も重要な問いの一つです．</p>
</div>
<div class="section" id="深いネットワークの定義">
<h3>4.4.4. 深いネットワークの定義<a class="headerlink" href="#深いネットワークの定義" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>では，さきほどのネットワークよりも多層のネットワークを定義してみましょう．ここでは，1層の畳み込みネットワークを<code class="docutils literal notranslate"><span class="pre">ConvBlock</span></code>，1層の全結合ネットワークを<code class="docutils literal notranslate"><span class="pre">LinearBlock</span></code>として定義し，これを数多く積み重ねることで大きなネットワークを定義してみます．</p>
<div class="section" id="構成要素を定義する">
<h4>4.4.4.1. 構成要素を定義する<a class="headerlink" href="#構成要素を定義する" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>まず，ネットワークの構成要素となる<code class="docutils literal notranslate"><span class="pre">ConvBlock</span></code>と<code class="docutils literal notranslate"><span class="pre">LinearBlock</span></code>を定義してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class ConvBlock(chainer.Chain):

    def __init__(self, n_ch, pool_drop=False):
        w = chainer.initializers.HeNormal()
        super(ConvBlock, self).__init__()
        with self.init_scope():
            self.conv = L.Convolution2D(None, n_ch, 3, 1, 1, nobias=True, initialW=w)
            self.bn = L.BatchNormalization(n_ch)
        self.pool_drop = pool_drop

    def forward(self, x):
        h = F.relu(self.bn(self.conv(x)))
        if self.pool_drop:
            h = F.max_pooling_2d(h, 2, 2)
            h = F.dropout(h, ratio=0.25)
        return h

class LinearBlock(chainer.Chain):

    def __init__(self, drop=False):
        w = chainer.initializers.HeNormal()
        super(LinearBlock, self).__init__()
        with self.init_scope():
            self.fc = L.Linear(None, 1024, initialW=w)
        self.drop = drop

    def forward(self, x):
        h = F.relu(self.fc(x))
        if self.drop:
            h = F.dropout(h)
        return h
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ConvBlock</span></code>は<code class="docutils literal notranslate"><span class="pre">Chain</span></code>を継承した小さなネットワークとして定義されており，一つの畳み込み層とBatch
Normalization層で構成されます．Batch
Normalization層は，ネットワークの学習プロセスを安定させるために広く利用されている手法の一つで，例えば今回のように，畳み込み層の直後に挿入する形で利用されます．<code class="docutils literal notranslate"><span class="pre">forward</span></code>メソッドでは，これらにデータを渡しつつ，活性化関数ReLUを適用して，さらに<code class="docutils literal notranslate"><span class="pre">pool_drop</span></code>引数が<code class="docutils literal notranslate"><span class="pre">True</span></code>であれば，Max
PoolingとDropoutを適用するような順伝播の計算が行われます．Dropoutは，ネットワークの過学習を避けて汎化性能を上げる目的で利用される手法の一つで，層の中のノードの内，一定割合（dropout
ratioと呼ばれる）をランダムに無効にしながら学習を行います（無効にする割合は<code class="docutils literal notranslate"><span class="pre">ratio</span></code>という引数で指定でき，何も指定しなければ50%が無効化されます）．推論時は，dropout
ratioを<span class="math notranslate nohighlight">\(p\)</span>とすると，Dropout層への入力をただ<span class="math notranslate nohighlight">\(p\)</span>倍して出力するだけの層として働きます．これによって，擬似的に複数のネットワークの学習結果をアンサンブル（参考：<a class="reference external" href="https://en.wikipedia.org/wiki/Ensemble_averaging_(machine_learning)">Ensemble
averaging</a>)するような効果があると言われ，汎化性能が向上する場合があります．最適化の際にモデルのパラメータに何らかの制約を与えて汎化性能を向上させるための工夫は正則化（regularization）と呼ばれ，このDropoutやパラメータの絶対値が大きくなりすぎないようにするWeight
decayなどの方法が知られています．</p>
<p>Chainerでは，Pythonを使って書いたforward計算のコード自体がネットワークの構造を表します．すなわち，実行時にデータがどの層を通過していったか，によってネットワークそのものが定義されます．この性質によって，上記のような分岐などを含むネットワークも簡単に記述でき，柔軟かつシンプルで可読性の高いネットワーク定義が可能になります．これが<strong>Define-by-Run</strong>の大きな特徴となっています．</p>
</div>
<div class="section" id="大きなネットワークの定義">
<h4>4.4.4.2. 大きなネットワークの定義<a class="headerlink" href="#大きなネットワークの定義" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>次に，これらの小さなネットワークを構成要素として積み重ねて，大きなネットワークを定義してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class DeepCNN(chainer.ChainList):

    def __init__(self, n_output):
        super(DeepCNN, self).__init__(
            ConvBlock(64),
            ConvBlock(64, True),
            ConvBlock(128),
            ConvBlock(128, True),
            ConvBlock(256),
            ConvBlock(256),
            ConvBlock(256),
            ConvBlock(256, True),
            LinearBlock(),
            LinearBlock(),
            L.Linear(None, n_output)
        )

    def forward(self, x):
        for f in self:
            x = f(x)
        return x
</pre></div>
</div>
</div>
<p>ここで，<code class="docutils literal notranslate"><span class="pre">ChainList</span></code>というクラスが利用されています．このクラスは<code class="docutils literal notranslate"><span class="pre">Chain</span></code>を継承したクラスで，いくつもの<code class="docutils literal notranslate"><span class="pre">Link</span></code>や<code class="docutils literal notranslate"><span class="pre">Chain</span></code>を順次呼び出していくようなネットワークを定義するときに便利です．<code class="docutils literal notranslate"><span class="pre">ChainList</span></code>を継承して定義されるモデルは，親クラスのコンストラクタを呼び出す際に，キーワード引数ではなく通常の引数として<code class="docutils literal notranslate"><span class="pre">Link</span></code>もしくは<code class="docutils literal notranslate"><span class="pre">Chain</span></code>オブジェクトを渡すことができ，<code class="docutils literal notranslate"><span class="pre">self.children()</span></code>メソッドによって<strong>登録した順番に</strong>取り出すことができます．この特徴を使うと，forward計算が上記のように簡単に記述可能となります．</p>
<p>それでは，学習を回してみます．今回はパラメータ数も多いので，学習を停止するエポック数を100に設定します．また，学習率を0.1から始めて，30エポックごとに10分の1にするように設定します．</p>
</div>
<div class="section" id="高速化のTIPS">
<h4>4.4.4.3. 高速化のTIPS<a class="headerlink" href="#高速化のTIPS" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>今回は多くの畳込み層を使う大きなネットワークを使うので，Chainerが用意してくれているcuDNNのautotune機能を有効にしてみます．やり方は簡単で，以下の二行を事前に実行しておくだけです．これを有効にすると，cuDNNが自動的に高速な畳み込みのアルゴリズムを選択するなどの実行時の調整を行ってくれるようになります．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>chainer.cuda.set_max_workspace_size(1024 * 1024 * 1024)
chainer.config.autotune = True
</pre></div>
</div>
</div>
<p>それでは，学習を開始してみましょう．本来は，以下の2行を実行することで乱数シードを固定し，100エポック分上で定義した
<code class="docutils literal notranslate"><span class="pre">DeepCNN</span></code>
というクラスが表すモデルの学習ができるのですが，これは40分以上の時間を要するので，今回は事前に90エポックまで学習を進めておいた重みを読み込んで，90エポック終了時点から学習を再開し，最後の10エポックだけ実際にここで学習を回すことにします．</p>
<p><strong>ゼロから学習する場合:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reset_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">DeepCNN</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">max_epoch</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">lr_decay</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [40]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!wget https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/DeepCNN_cifar10_snapshot_epoch_90.npz

reset_seed(0)

model = train(DeepCNN(10), max_epoch=100, base_lr=0.1, lr_decay=(30, &#39;epoch&#39;), snapshot=&#39;DeepCNN_cifar10_snapshot_epoch_90.npz&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2018-12-16 09:55:12--  https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/DeepCNN_cifar10_snapshot_epoch_90.npz
Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113
Connecting to github.com (github.com)|192.30.253.112|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/4fcc1200-eeb7-11e8-8ca0-9095e5bca078?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T095512Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=185a882eef32bceaa93e4afbf4004e87be3980dafa652907385c0bfd9a189360&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3DDeepCNN_cifar10_snapshot_epoch_90.npz&amp;response-content-type=application%2Foctet-stream [following]
--2018-12-16 09:55:12--  https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/4fcc1200-eeb7-11e8-8ca0-9095e5bca078?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T095512Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=185a882eef32bceaa93e4afbf4004e87be3980dafa652907385c0bfd9a189360&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3DDeepCNN_cifar10_snapshot_epoch_90.npz&amp;response-content-type=application%2Foctet-stream
Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.234.51
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.234.51|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 56889445 (54M) [application/octet-stream]
Saving to: ‘DeepCNN_cifar10_snapshot_epoch_90.npz’

DeepCNN_cifar10_sna 100%[===================&gt;]  54.25M  74.3MB/s    in 0.7s

2018-12-16 09:55:13 (74.3 MB/s) - ‘DeepCNN_cifar10_snapshot_epoch_90.npz’ saved [56889445/56889445]

epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr
1           2.62849     0.144931       2.22525        0.15625            27.9748       0.1
2           2.11316     0.210804       1.97533        0.266406           54.5483       0.1
3           1.87483     0.289396       1.8026         0.320508           81.0827       0.1
4           1.74066     0.340443       1.74728        0.358203           107.7         0.1
5           1.58789     0.409411       1.63916        0.40332            134.235       0.1
6           1.38757     0.492831       1.22399        0.561719           161.093       0.1
7           1.2036      0.566128       1.32939        0.553906           187.55        0.1
8           1.07527     0.617077       1.17079        0.589648           214.184       0.1
9           0.964984    0.660711       0.950063       0.67207            240.817       0.1
10          0.895905    0.688056       0.972697       0.657031           267.372       0.1
11          0.828796    0.715043       0.944733       0.686914           297.779       0.1
12          0.784123    0.731793       0.960205       0.687305           324.312       0.1
13          0.742033    0.748291       0.858308       0.719727           351.032       0.1
14          0.692516    0.76627        0.811853       0.725195           377.591       0.1
15          0.65644     0.776256       0.692374       0.767578           404.23        0.1
16          0.650682    0.780738       0.796731       0.733008           430.797       0.1
17          0.610249    0.793435       0.632162       0.791406           457.467       0.1
18          0.591896    0.803667       0.791757       0.739453           484.04        0.1
19          0.578718    0.804465       1.04659        0.671484           510.602       0.1
20          0.554954    0.814431       0.8127         0.733594           537.247       0.1
21          0.549188    0.814236       0.654926       0.787109           567.567       0.1
22          0.535866    0.820446       0.640323       0.788672           594.214       0.1
23          0.527765    0.823028       0.958373       0.70957            620.761       0.1
24          0.512286    0.830056       0.793664       0.748633           647.363       0.1
25          0.497195    0.833141       0.717548       0.759961           673.951       0.1
26          0.495153    0.835759       1.7557         0.511328           700.494       0.1
27          0.486062    0.837069       0.732518       0.775391           727.13        0.1
28          0.47963     0.839387       0.669157       0.786328           753.692       0.1
29          0.475502    0.838312       0.915904       0.718555           780.308       0.1
30          0.460236    0.844841       0.877713       0.72793            806.806       0.1
31          0.298483    0.897239       0.381921       0.879687           837.16        0.01
32          0.211182    0.92784        0.364046       0.882617           863.898       0.01
33          0.180429    0.937478       0.374651       0.883984           890.554       0.01
34          0.164156    0.943692       0.361041       0.888867           917.199       0.01
35          0.144584    0.950387       0.375391       0.889258           943.774       0.01
36          0.132288    0.954235       0.377427       0.890625           970.4         0.01
37          0.12103     0.957376       0.390434       0.892578           996.96        0.01
38          0.111974    0.961204       0.400307       0.886133           1023.62       0.01
39          0.102573    0.964476       0.399275       0.892773           1050.2        0.01
40          0.0972647   0.965931       0.432854       0.887109           1076.83       0.01
41          0.0928545   0.966597       0.418165       0.887305           1107.19       0.01
42          0.08498     0.96964        0.432462       0.884961           1133.75       0.01
43          0.0845448   0.970792       0.4365         0.880273           1160.37       0.01
44          0.0770674   0.973914       0.441935       0.883789           1187.02       0.01
45          0.0732439   0.974565       0.469901       0.881836           1213.62       0.01
46          0.070491    0.975962       0.453856       0.8875             1240.16       0.01
47          0.068846    0.97583        0.461264       0.881055           1266.75       0.01
48          0.0694101   0.976941       0.435111       0.885742           1293.25       0.01
49          0.0653772   0.977117       0.461284       0.882617           1319.87       0.01
50          0.0633419   0.97836        0.464232       0.889648           1346.47       0.01
51          0.0584663   0.979834       0.464193       0.885547           1376.6        0.01
52          0.0607617   0.979714       0.466352       0.881445           1403.21       0.01
53          0.0615791   0.978632       0.451807       0.889453           1429.69       0.01
54          0.0588031   0.979914       0.489054       0.881836           1456.28       0.01
55          0.0582368   0.979367       0.4719         0.882617           1482.83       0.01
56          0.0558719   0.981379       0.495846       0.878125           1509.45       0.01
57          0.0579962   0.979936       0.472415       0.875781           1536.06       0.01
58          0.0592009   0.9793         0.454762       0.88418            1562.6        0.01
59          0.0568546   0.980735       0.487556       0.876172           1589.17       0.01
60          0.0579785   0.980079       0.472908       0.883398           1615.93       0.01
61          0.0318918   0.98968        0.416953       0.895703           1646.15       0.001
62          0.0220127   0.993612       0.416859       0.899609           1672.69       0.001
63          0.0186169   0.99434        0.417849       0.898242           1699.29       0.001
64          0.0159041   0.995526       0.41804        0.900781           1725.78       0.001
65          0.0147089   0.995916       0.429896       0.899609           1752.39       0.001
66          0.0129457   0.996404       0.433748       0.898828           1779.01       0.001
67          0.0131643   0.996283       0.433923       0.898828           1805.54       0.001
68          0.0112659   0.996893       0.437222       0.901758           1832.17       0.001
69          0.0106502   0.997151       0.443475       0.901758           1858.73       0.001
70          0.0107926   0.997203       0.445066       0.900391           1885.34       0.001
71          0.0105973   0.997062       0.44159        0.898633           1915.8        0.001
72          0.00934292  0.99767        0.450084       0.897266           1942.45       0.001
73          0.0104884   0.997092       0.451691       0.899023           1969.06       0.001
74          0.00849317  0.997707       0.450391       0.9                1995.61       0.001
75          0.00846932  0.997891       0.451362       0.902148           2022.21       0.001
76          0.00826699  0.997841       0.448779       0.900781           2048.69       0.001
77          0.00875069  0.997492       0.45095        0.900391           2075.31       0.001
78          0.00823296  0.998019       0.449194       0.898438           2102.1        0.001
79          0.00701245  0.998113       0.454196       0.899609           2128.74       0.001
80          0.00846517  0.997596       0.455877       0.901172           2155.29       0.001
81          0.00677115  0.99818        0.459518       0.899805           2185.54       0.001
82          0.00717393  0.998047       0.465337       0.899805           2212.13       0.001
83          0.00709802  0.997908       0.464472       0.898828           2238.69       0.001
84          0.00699702  0.998091       0.470595       0.899219           2265.27       0.001
85          0.00746627  0.997975       0.470063       0.901172           2291.78       0.001
86          0.00666763  0.998069       0.466201       0.899414           2318.38       0.001
87          0.00616266  0.998531       0.462948       0.9                2344.86       0.001
88          0.00719447  0.997847       0.463587       0.899609           2371.48       0.001
89          0.00638493  0.998335       0.465655       0.901367           2398.08       0.001
90          0.0061445   0.998286       0.464918       0.900586           2424.59       0.001
91          0.00582548  0.99838        0.46484        0.900977           2454.96       0.0001
92          0.0060277   0.998331       0.461774       0.901367           2482.74       0.0001
93          0.00597372  0.998491       0.464172       0.900195           2510.55       0.0001
94          0.00619661  0.99813        0.464461       0.900391           2538.26       0.0001
95          0.00545576  0.998557       0.466655       0.900977           2566.24       0.0001
96          0.00613308  0.998308       0.465332       0.900977           2593.93       0.0001
97          0.00541786  0.998624       0.465639       0.900586           2621.72       0.0001
98          0.00512303  0.998801       0.467117       0.900781           2649.5        0.0001
99          0.00562249  0.998576       0.464966       0.901367           2677.23       0.0001
100         0.00551715  0.998624       0.462724       0.901367           2705.02       0.0001
Test accuracy: 0.8966574
</pre></div></div>
</div>
<p>学習が終了しました．学習曲線と精度のグラフを見てみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [41]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/DeepCNN_cifar10_result/loss.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[41]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_116_0.png" src="../_images/notebooks_Introduction_to_Chainer_116_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [42]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/DeepCNN_cifar10_result/accuracy.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[42]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_117_0.png" src="../_images/notebooks_Introduction_to_Chainer_117_0.png" />
</div>
</div>
<p>先程の浅い（層数の少ない）CNNを用いた際には60%前後だったValidationデータでの精度が，90%程度まで上がりました．また，テストデータを用いた精度も，およそ90%程度となっています．しかし最新の研究成果では97%以上まで達成されています．さらに精度を上げるには，今回行ったようなネットワークの構造自体の改良ももちろんのこと，学習データを擬似的に増やす操作（Data
augmentation）や，複数のモデルの出力を一つの出力に統合する操作（Ensemble）などなど，いろいろな工夫が考えられます．</p>
</div>
</div>
</div>
<div class="section" id="データセットクラスの使用方法">
<h2>4.5. データセットクラスの使用方法<a class="headerlink" href="#データセットクラスの使用方法" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここでは，Chainerにすでに用意されているCIFAR10のデータを取得する機能を使って，データセットクラスを自分で書いてみます．Chainerでは，データセットを表すクラスは以下の機能を持っている必要があります．</p>
<ul class="simple">
<li>データセット内のデータ数を返す<code class="docutils literal notranslate"><span class="pre">__len__</span></code>メソッド</li>
<li>引数として渡される<code class="docutils literal notranslate"><span class="pre">i</span></code>に対応したデータもしくはデータとラベルの組を返す<code class="docutils literal notranslate"><span class="pre">get_example</span></code>メソッド</li>
</ul>
<p>その他のデータセットに必要な機能は，<code class="docutils literal notranslate"><span class="pre">chainer.dataset.DatasetMixin</span></code>クラスを継承することで用意できます．ここでは，<code class="docutils literal notranslate"><span class="pre">DatasetMixin</span></code>クラスを継承し，学習時に学習データに変換を施してモデルが受け取るデータのバリエーションを増やすData
augmentation機能のついたデータセットクラスを作成してみましょう．</p>
<div class="section" id="CIFAR10データセットクラス">
<h3>4.5.1. CIFAR10データセットクラス<a class="headerlink" href="#CIFAR10データセットクラス" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class CIFAR10Augmented(chainer.dataset.DatasetMixin):

    def __init__(self, split=&#39;train&#39;, train_ratio=0.9):
        train_val, test_data = cifar.get_cifar10()
        train_size = int(len(train_val) * train_ratio)
        train_data, valid_data = split_dataset_random(train_val, train_size, seed=0)
        if split == &#39;train&#39;:
            self.data = train_data
        elif split == &#39;valid&#39;:
            self.data = valid_data
        elif split == &#39;test&#39;:
            self.data = test_data
        else:
            raise ValueError(&quot;&#39;split&#39; argument should be either &#39;train&#39;, &#39;valid&#39;, or &#39;test&#39;. But {} was given.&quot;.format(split))

        self.split = split
        self.random_crop = 4

    def __len__(self):
        return len(self.data)

    def get_example(self, i):
        x, t = self.data[i]
        if self.split == &#39;train&#39;:
            x = x.transpose(1, 2, 0)
            h, w, _ = x.shape
            x_offset = np.random.randint(self.random_crop)
            y_offset = np.random.randint(self.random_crop)
            x = x[y_offset:y_offset + h - self.random_crop,
                  x_offset:x_offset + w - self.random_crop]
            if np.random.rand() &gt; 0.5:
                x = np.fliplr(x)
            x = x.transpose(2, 0, 1)

        return x, t
</pre></div>
</div>
</div>
<p>このクラスは，CIFAR10のデータのそれぞれに対し，</p>
<ul class="simple">
<li>32x32の大きさの中からランダムに28x28の領域をクロップ</li>
<li>1/2の確率で左右を反転させる</li>
</ul>
<p>という加工を行っています．このような操作を加えて擬似的に学習データのバリエーションを増やすことで，オーバーフィッティングの抑制などに寄与することが知られています．これらの操作以外にも，画像の色味を変化させるような変換やランダムな回転，アフィン変換など，さまざまな加工によって学習データ数を擬似的に増やす方法が提案されています．</p>
</div>
<div class="section" id="作成したデータセットクラスを用いた学習">
<h3>4.5.2. 作成したデータセットクラスを用いた学習<a class="headerlink" href="#作成したデータセットクラスを用いた学習" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それではさっそくこの<code class="docutils literal notranslate"><span class="pre">CIFAR10</span></code>クラスを使って学習を行ってみましょう．先程と同じネットワークを用い，Data
augmentationの効果がどの程度あるのかを調べてみましょう．<code class="docutils literal notranslate"><span class="pre">train</span></code>関数も含め，データセットクラス以外は先程とすべて同様です．</p>
<p>ここでも，40分ほどの時間がかかりますので，上と同様に90エポックまで学習したあとのsnapshotをダウンロードして読み込ませ，最後の10エポックだけ実際に学習させてみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [44]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!wget https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/DeepCNN_cifar10_augmented_snapshot_epoch_90.npz

reset_seed(0)

model = train(DeepCNN(10), max_epoch=100, train_dataset=CIFAR10Augmented(), valid_dataset=CIFAR10Augmented(&#39;valid&#39;), test_dataset=CIFAR10Augmented(&#39;test&#39;), postfix=&#39;augmented_&#39;, base_lr=0.1, lr_decay=(30, &#39;epoch&#39;), snapshot=&#39;DeepCNN_cifar10_augmented_snapshot_epoch_90.npz&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2018-12-16 10:00:04--  https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/DeepCNN_cifar10_augmented_snapshot_epoch_90.npz
Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112
Connecting to github.com (github.com)|192.30.253.113|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/5064a880-eeb7-11e8-95bf-80b5d9533256?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T100004Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=0a47f4f64408753c2449d1b68aef4eeeb33a7626e403a19d003d74a85ae0d45b&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3DDeepCNN_cifar10_augmented_snapshot_epoch_90.npz&amp;response-content-type=application%2Foctet-stream [following]
--2018-12-16 10:00:04--  https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/5064a880-eeb7-11e8-95bf-80b5d9533256?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T100004Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=0a47f4f64408753c2449d1b68aef4eeeb33a7626e403a19d003d74a85ae0d45b&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3DDeepCNN_cifar10_augmented_snapshot_epoch_90.npz&amp;response-content-type=application%2Foctet-stream
Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.1.208
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.1.208|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 56730280 (54M) [application/octet-stream]
Saving to: ‘DeepCNN_cifar10_augmented_snapshot_epoch_90.npz’

DeepCNN_cifar10_aug 100%[===================&gt;]  54.10M  71.6MB/s    in 0.8s

2018-12-16 10:00:05 (71.6 MB/s) - ‘DeepCNN_cifar10_augmented_snapshot_epoch_90.npz’ saved [56730280/56730280]

epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr
1           2.5875      0.156405       2.11656        0.203125           24.1767       0.1
2           1.99359     0.233842       1.84577        0.304492           47.9466       0.1
3           1.76968     0.325365       1.98983        0.26543            71.6864       0.1
4           1.61662     0.389537       2.07369        0.26875            95.5072       0.1
5           1.41259     0.478989       1.52089        0.446484           119.236       0.1
6           1.23382     0.555487       1.48775        0.480664           143.06        0.1
7           1.09323     0.613404       1.11949        0.590234           166.813       0.1
8           0.99303     0.650857       1.33017        0.566992           190.624       0.1
9           0.926848    0.678755       1.0075         0.665234           214.446       0.1
10          0.863165    0.702769       0.858035       0.717383           238.218       0.1
11          0.807948    0.727162       0.9556         0.679297           265.757       0.1
12          0.769224    0.739116       0.857765       0.711328           289.507       0.1
13          0.739977    0.752952       0.91583        0.711133           313.339       0.1
14          0.72064     0.759393       1.20587        0.61875            337.097       0.1
15          0.690136    0.77093        0.837919       0.726562           360.905       0.1
16          0.673935    0.774706       1.03539        0.678711           384.67        0.1
17          0.662879    0.778742       0.730712       0.758789           408.47        0.1
18          0.639202    0.78742        0.758566       0.765625           432.298       0.1
19          0.625988    0.792713       1.24791        0.664062           456.061       0.1
20          0.616269    0.795277       0.963706       0.70625            479.882       0.1
21          0.611734    0.795962       0.887129       0.723437           507.312       0.1
22          0.600444    0.800582       0.889526       0.710352           531.173       0.1
23          0.605317    0.800414       0.715702       0.756445           554.926       0.1
24          0.584194    0.805731       0.984225       0.694336           578.687       0.1
25          0.584041    0.805464       0.956576       0.685156           602.49        0.1
26          0.57384     0.80954        0.977559       0.712695           627.031       0.1
27          0.560405    0.814298       0.894127       0.718945           650.845       0.1
28          0.559933    0.816195       0.729981       0.752734           674.584       0.1
29          0.555933    0.814387       0.841304       0.727344           698.4         0.1
30          0.558057    0.814971       0.753542       0.757227           722.119       0.1
31          0.397613    0.866455       0.337977       0.888867           749.619       0.01
32          0.320024    0.890202       0.322082       0.894336           773.368       0.01
33          0.293655    0.900479       0.323365       0.888867           797.169       0.01
34          0.279553    0.904874       0.308263       0.897656           820.951       0.01
35          0.26519     0.909945       0.301763       0.897852           844.682       0.01
36          0.259166    0.909846       0.286909       0.904688           868.482       0.01
37          0.246168    0.915064       0.289997       0.904688           892.324       0.01
38          0.24097     0.91697        0.280986       0.903906           916.139       0.01
39          0.233951    0.918892       0.291962       0.904102           939.884       0.01
40          0.220939    0.923628       0.299502       0.902539           963.666       0.01
41          0.216055    0.924272       0.2946         0.905664           991.125       0.01
42          0.215143    0.926972       0.308637       0.897266           1014.88       0.01
43          0.213903    0.926625       0.291742       0.907812           1038.64       0.01
44          0.203283    0.929198       0.296043       0.905469           1062.33       0.01
45          0.19772     0.931041       0.327708       0.89375            1086.11       0.01
46          0.194907    0.932893       0.312555       0.901563           1109.82       0.01
47          0.190354    0.934326       0.336271       0.895703           1133.58       0.01
48          0.190431    0.932915       0.326305       0.902539           1157.3        0.01
49          0.18926     0.934326       0.312767       0.901172           1181.05       0.01
50          0.184469    0.936035       0.296937       0.907812           1204.93       0.01
51          0.181691    0.936521       0.324149       0.901172           1232.22       0.01
52          0.17546     0.939675       0.347524       0.893945           1256          0.01
53          0.175786    0.937723       0.335627       0.893164           1279.74       0.01
54          0.173612    0.940274       0.317897       0.902344           1303.52       0.01
55          0.171849    0.939548       0.306998       0.90625            1327.23       0.01
56          0.168304    0.94165        0.31145        0.902148           1351.04       0.01
57          0.170139    0.941495       0.301311       0.910156           1374.82       0.01
58          0.165011    0.942708       0.359516       0.892773           1398.52       0.01
59          0.163968    0.94256        0.365818       0.886133           1422.3        0.01
60          0.16541     0.942575       0.357          0.890234           1446.01       0.01
61          0.129435    0.955988       0.277052       0.915234           1473.42       0.001
62          0.101981    0.965434       0.284798       0.916406           1497.13       0.001
63          0.0953637   0.967285       0.279956       0.919727           1520.93       0.001
64          0.0911066   0.968171       0.28204        0.918359           1544.63       0.001
65          0.0853851   0.97037        0.28504        0.918945           1568.41       0.001
66          0.0800331   0.972101       0.287688       0.917969           1592.21       0.001
67          0.0761374   0.973202       0.29148        0.920117           1616.19       0.001
68          0.0756613   0.973699       0.299635       0.918945           1639.98       0.001
69          0.075577    0.97407        0.293845       0.918359           1663.68       0.001
70          0.0730666   0.974676       0.29563        0.920508           1687.47       0.001
71          0.070825    0.975516       0.295581       0.920313           1714.73       0.001
72          0.0710753   0.975697       0.29838        0.919336           1738.52       0.001
73          0.0705982   0.975142       0.298369       0.920508           1762.3        0.001
74          0.0667571   0.976562       0.299809       0.920508           1786.01       0.001
75          0.0642319   0.978427       0.300881       0.920898           1809.77       0.001
76          0.0640179   0.977742       0.304647       0.918359           1833.51       0.001
77          0.0629752   0.977761       0.299763       0.919336           1857.31       0.001
78          0.0586612   0.979523       0.306034       0.922461           1881.03       0.001
79          0.059752    0.979869       0.311227       0.921289           1904.82       0.001
80          0.0571715   0.980213       0.304607       0.920703           1928.51       0.001
81          0.0573339   0.980136       0.315108       0.92168            1956.06       0.001
82          0.0560348   0.979847       0.321934       0.916992           1979.87       0.001
83          0.0553193   0.980613       0.315378       0.914648           2003.58       0.001
84          0.0531816   0.98129        0.318977       0.919531           2027.34       0.001
85          0.0560367   0.98097        0.310993       0.919141           2051.02       0.001
86          0.0535048   0.981534       0.317829       0.920117           2075.04       0.001
87          0.0522188   0.981571       0.31144        0.920313           2098.73       0.001
88          0.0526632   0.982156       0.318594       0.920703           2122.46       0.001
89          0.0528096   0.981445       0.309017       0.92207            2146.21       0.001
90          0.0499371   0.982928       0.313269       0.920508           2169.92       0.001
91          0.0459802   0.985063       0.313539       0.919727           2196.56       0.0001
92          0.0467323   0.983752       0.309761       0.92168            2221.4        0.0001
93          0.0451435   0.984442       0.308279       0.921875           2246.32       0.0001
94          0.043241    0.985688       0.310432       0.921875           2271.16       0.0001
95          0.0456454   0.98422        0.309659       0.920508           2296.06       0.0001
96          0.0465676   0.983551       0.312543       0.921484           2320.9        0.0001
97          0.0446353   0.984286       0.311261       0.920703           2345.85       0.0001
98          0.0422733   0.985574       0.30984        0.921094           2370.88       0.0001
99          0.0418951   0.985532       0.309265       0.920898           2395.7        0.0001
100         0.0441129   0.985085       0.313141       0.919531           2420.6        0.0001
Test accuracy: 0.91752374
</pre></div></div>
</div>
<p>先程のData
augmentationなしの場合は90%程度だったテスト精度が，学習データにaugmentationを施すことでおよそ1.8%程度向上していることが分かりました．</p>
<p>損失と精度のグラフを見てみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [45]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/DeepCNN_cifar10_augmented_result/loss.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[45]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_126_0.png" src="../_images/notebooks_Introduction_to_Chainer_126_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [46]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Image(filename=&#39;results/DeepCNN_cifar10_augmented_result/accuracy.png&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[46]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_Chainer_127_0.png" src="../_images/notebooks_Introduction_to_Chainer_127_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Data-Augmentationの簡単な使い方">
<h2>4.6. Data Augmentationの簡単な使い方<a class="headerlink" href="#Data-Augmentationの簡単な使い方" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>前述のようにデータセット内の各画像についていろいろな変換を行って擬似的にデータを増やすような操作をData
Augmentationといいます．上では，オリジナルのデータセットクラスを作る方法を示すために変換の操作も<code class="docutils literal notranslate"><span class="pre">get_example()</span></code>内に書くという実装を行いましたが，実はもっと簡単にいろいろな変換をデータに対して行う方法があります．</p>
<p>それは，<code class="docutils literal notranslate"><span class="pre">TransformDataset</span></code>クラスを使う方法です．<code class="docutils literal notranslate"><span class="pre">TransformDataset</span></code>は，元になるデータセットオブジェクトと，そこからサンプルしてきた各データ点に対して行いたい変換を関数の形で与えると，変換済みのデータを返してくれるようなデータセットオブジェクトに加工してくれる便利なクラスです．簡単な使い方は以下のようになります．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.datasets import TransformDataset

train_val, test_dataset = cifar.get_cifar10()
train_size = int(len(train_val) * 0.9)
train_dataset, valid_dataset = split_dataset_random(train_val, train_size, seed=0)


# 行いたい変換を関数の形で書く
def transform(inputs):
    x, t = inputs
    x = x.transpose(1, 2, 0)
    h, w, _ = x.shape
    x_offset = np.random.randint(4)
    y_offset = np.random.randint(4)
    x = x[y_offset:y_offset + h - 4,
          x_offset:x_offset + w - 4]
    if np.random.rand() &gt; 0.5:
        x = np.fliplr(x)
    x = x.transpose(2, 0, 1)

    return x, t


# 各データをtransform関数で処理して返すデータセットオブジェクト
train_dataset = TransformDataset(train_dataset, transform)
</pre></div>
</div>
</div>
<p>このようにして得られた新しい<code class="docutils literal notranslate"><span class="pre">train_dataset</span></code>は，自作のデータセットクラスと同じような変換処理を行った上でデータを返してくれるデータセットオブジェクトとなります．</p>
<div class="section" id="ChainerCVを活用した変換処理">
<h3>4.6.1. ChainerCVを活用した変換処理<a class="headerlink" href="#ChainerCVを活用した変換処理" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>さて，先ほどご紹介したコードでは，画像に対するランダムクロップ，及びランダムな左右反転の処理を自ら実装していました．もし，より多様な変換を行いたい場合，上記の<code class="docutils literal notranslate"><span class="pre">transform</span></code>関数に処理を追加していくことになりますが，一般的に用いられる変換処理をその度に自ら実装するのは手間です．そこで本項では最後に，<a class="reference external" href="http://chainercv.readthedocs.io/en/stable">ChainerCV</a><a class="reference external" href="https://arxiv.org/abs/1708.08169">[Niitani
2017]</a>をご紹介します．ChainerCVは，Computer
Visionに特化した機能が豊富に追加された，Chainerの補助パッケージとしての役割を担うオープンソース・ソフトウェアです．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [48]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!pip install chainercv
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Collecting chainercv
  Downloading https://files.pythonhosted.org/packages/bb/1c/cee12630628d0acd4d6c5d7a3ba196a3aa079a863baa3dc74eb70d8494c5/chainercv-0.11.0.tar.gz (223kB)
    100% |████████████████████████████████| 225kB 9.2MB/s
Requirement already satisfied: chainer&gt;=5.0 in /usr/local/lib/python3.6/dist-packages (from chainercv) (5.0.0)
Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from chainercv) (4.0.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer&gt;=5.0-&gt;chainercv) (3.0.10)
Requirement already satisfied: numpy&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer&gt;=5.0-&gt;chainercv) (1.14.6)
Requirement already satisfied: protobuf&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer&gt;=5.0-&gt;chainercv) (3.6.1)
Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer&gt;=5.0-&gt;chainercv) (1.11.0)
Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow-&gt;chainercv) (0.46)
Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.0.0-&gt;chainer&gt;=5.0-&gt;chainercv) (40.6.2)
Building wheels for collected packages: chainercv
  Running setup.py bdist_wheel for chainercv ... - \ | / - \ | / done
  Stored in directory: /root/.cache/pip/wheels/95/24/5a/1611db416857b5e092962bc22a70722315881e03970c7fa966
Successfully built chainercv
Installing collected packages: chainercv
Successfully installed chainercv-0.11.0
</pre></div></div>
</div>
<p><a class="reference external" href="http://chainercv.readthedocs.io/en/stable">ChainerCV</a>には，画像に対する様々な変換があらかじめ用意されています．</p>
<ul class="simple">
<li><a class="reference external" href="http://chainercv.readthedocs.io/en/stable/reference/transforms.html#image">ChainerCVで使える画像変換一覧</a></li>
</ul>
<p>例えば，上でNumPyを使って書いていたランダムクロップやランダム左右反転は，<code class="docutils literal notranslate"><span class="pre">chainercv.transforms</span></code>モジュールを使うと，それぞれ以下のように1行で書くことができます：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">random_crop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>  <span class="c1"># ランダムクロップ</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">chainercv</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">random_flip</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># ランダム左右反転</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">chainercv.transforms</span></code>モジュールを使って，<code class="docutils literal notranslate"><span class="pre">transform</span></code>関数をアップデートしてみましょう．ちなみに，<code class="docutils literal notranslate"><span class="pre">get_cifar10()</span></code>で得られるデータセットでは，デフォルトで画像の画素値の範囲が<code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>にスケールされています．しかし，<code class="docutils literal notranslate"><span class="pre">get_cifar10()</span></code>に<code class="docutils literal notranslate"><span class="pre">scale=255.</span></code>を渡しておくと，値の範囲をもともとの<code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>のままにできます．今回<code class="docutils literal notranslate"><span class="pre">transform</span></code>の中で行う処理は，以下の5つです：</p>
<ol class="arabic simple">
<li>PCA lighting:
先行研究(AlexNet)の学習で使われていた方法で，色を変化させる変換処理を行います．</li>
<li>Standardization:
訓練用データセット全体からチャンネルごとの画素値の平均・標準偏差を求めて標準化をします</li>
<li>Random flip: ランダムに画像の左右を反転します</li>
<li>Random expand:
<code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">1.5]</span></code>からランダムに決めた大きさの黒いキャンバスを作り，その中のランダムな位置へ画像を配置します</li>
<li>Random crop: <code class="docutils literal notranslate"><span class="pre">(28,</span> <span class="pre">28)</span></code>の大きさの領域をランダムにクロップします</li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from functools import partial
from chainercv import transforms

train_val, test_dataset = cifar.get_cifar10(scale=255.)
train_size = int(len(train_val) * 0.9)
train_dataset, valid_dataset = split_dataset_random(train_val, train_size, seed=0)

mean = np.mean([x for x, _ in train_dataset], axis=(0, 2, 3))
std = np.std([x for x, _ in train_dataset], axis=(0, 2, 3))


def transform(inputs, train=True):
    img, label = inputs
    img = img.copy()

    # Color augmentation
    if train:
        img = transforms.pca_lighting(img, 76.5)

    # Standardization
    img -= mean[:, None, None]
    img /= std[:, None, None]

    # Random flip &amp; crop
    if train:
        img = transforms.random_flip(img, x_random=True)
        img = transforms.random_expand(img, max_ratio=1.5)
        img = transforms.random_crop(img, (28, 28))

    return img, label

train_dataset = TransformDataset(train_dataset, partial(transform, train=True))
valid_dataset = TransformDataset(valid_dataset, partial(transform, train=False))
test_dataset = TransformDataset(test_dataset, partial(transform, train=False))
</pre></div>
</div>
</div>
<p>では，standardizationとChainerCVによるPCA
Lightingを追加した<code class="docutils literal notranslate"><span class="pre">TransformDataset</span></code>を使って学習をしてみましょう．</p>
<p>これまでと同様，90エポックまで学習させておいたsnapshotを用いて，最後の10エポックだけ学習を行います．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [50]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!wget https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/DeepCNN_cifar10_augmented2_snapshot_epoch_90.npz

reset_seed(0)

model = train(DeepCNN(10), max_epoch=100, train_dataset=train_dataset, valid_dataset=valid_dataset, test_dataset=test_dataset, postfix=&#39;augmented2_&#39;, base_lr=0.1, lr_decay=(30, &#39;epoch&#39;), snapshot=&#39;DeepCNN_cifar10_augmented2_snapshot_epoch_90.npz&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2018-12-16 10:04:44--  https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/DeepCNN_cifar10_augmented2_snapshot_epoch_90.npz
Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112
Connecting to github.com (github.com)|192.30.253.113|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/5064a880-eeb7-11e8-8e8b-fddbe76ecd56?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T100444Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=59aab1941e913de18a54bff2934ce9916f34e37381b505f8831705cba735c9e8&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3DDeepCNN_cifar10_augmented2_snapshot_epoch_90.npz&amp;response-content-type=application%2Foctet-stream [following]
--2018-12-16 10:04:44--  https://github-production-release-asset-2e65be.s3.amazonaws.com/153412006/5064a880-eeb7-11e8-8e8b-fddbe76ecd56?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20181216%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20181216T100444Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=59aab1941e913de18a54bff2934ce9916f34e37381b505f8831705cba735c9e8&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;response-content-disposition=attachment%3B%20filename%3DDeepCNN_cifar10_augmented2_snapshot_epoch_90.npz&amp;response-content-type=application%2Foctet-stream
Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.101.179
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.101.179|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 56734002 (54M) [application/octet-stream]
Saving to: ‘DeepCNN_cifar10_augmented2_snapshot_epoch_90.npz’

DeepCNN_cifar10_aug 100%[===================&gt;]  54.11M  71.2MB/s    in 0.8s

2018-12-16 10:04:45 (71.2 MB/s) - ‘DeepCNN_cifar10_augmented2_snapshot_epoch_90.npz’ saved [56734002/56734002]

epoch       main/loss   main/accuracy  val/main/loss  val/main/accuracy  elapsed_time  lr
1           2.64248     0.137296       2.15132        0.175              23.5716       0.1
2           2.09025     0.202814       1.90949        0.248633           47.2547       0.1
3           1.92989     0.251647       1.80744        0.313867           70.8962       0.1
4           1.82033     0.296742       1.75358        0.319727           94.6262       0.1
5           1.70069     0.350717       1.59837        0.397266           118.252       0.1
6           1.56064     0.418435       1.6835         0.418359           141.967       0.1
7           1.45132     0.473691       1.30451        0.528711           165.613       0.1
8           1.33843     0.523815       2.81471        0.372656           189.382       0.1
9           1.27017     0.556019       1.61386        0.499023           213.846       0.1
10          1.20204     0.579972       1.46257        0.519141           237.532       0.1
11          1.14896     0.601252       1.1028         0.623633           265.125       0.1
12          1.10957     0.614428       1.17823        0.594141           288.775       0.1
13          1.07055     0.635387       1.00013        0.670898           312.476       0.1
14          1.04187     0.647058       1.07628        0.642383           336.226       0.1
15          1.00359     0.661066       1.00439        0.655859           360.038       0.1
16          0.971513    0.675503       0.8598         0.723828           383.802       0.1
17          0.941225    0.686301       1.09454        0.661328           407.665       0.1
18          0.917967    0.694869       0.998599       0.681055           431.468       0.1
19          0.905397    0.699586       0.811614       0.738281           455.224       0.1
20          0.874732    0.706188       0.714926       0.762695           479.061       0.1
21          0.86753     0.712607       0.850176       0.738477           506.629       0.1
22          0.858491    0.714844       0.95919        0.692187           530.459       0.1
23          0.845307    0.717637       1.12914        0.668164           554.185       0.1
24          0.827677    0.72785        0.827821       0.746484           577.974       0.1
25          0.821103    0.728715       0.7663         0.741602           601.76        0.1
26          0.816583    0.728321       0.763894       0.757812           625.472       0.1
27          0.805109    0.735596       0.74157        0.754492           649.375       0.1
28          0.809963    0.734152       0.742951       0.7625             673.099       0.1
29          0.792127    0.737149       0.692472       0.773633           696.895       0.1
30          0.783124    0.741965       1.02244        0.702734           720.621       0.1
31          0.604456    0.795898       0.394226       0.868359           748.179       0.01
32          0.523254    0.822227       0.379255       0.873242           771.92        0.01
33          0.502693    0.830078       0.360393       0.877734           795.747       0.01
34          0.484705    0.835627       0.351212       0.884766           819.578       0.01
35          0.465001    0.842192       0.348862       0.882031           843.291       0.01
36          0.45372     0.846613       0.339081       0.885742           867.053       0.01
37          0.450468    0.846043       0.335244       0.887695           890.763       0.01
38          0.439256    0.848411       0.331884       0.889453           914.524       0.01
39          0.430965    0.852475       0.324836       0.895312           938.351       0.01
40          0.424651    0.855447       0.370822       0.881641           962.135       0.01
41          0.418223    0.857          0.328078       0.89082            989.723       0.01
42          0.409156    0.860332       0.334291       0.888672           1013.41       0.01
43          0.410969    0.860574       0.338266       0.888281           1037.14       0.01
44          0.397083    0.862469       0.337938       0.889258           1060.81       0.01
45          0.390805    0.8661         0.31488        0.900586           1084.56       0.01
46          0.39124     0.866008       0.324671       0.893945           1108.24       0.01
47          0.390007    0.865101       0.320954       0.895117           1131.96       0.01
48          0.383439    0.868367       0.355809       0.890625           1155.62       0.01
49          0.380758    0.87065        0.314927       0.899805           1179.36       0.01
50          0.379198    0.870716       0.302073       0.903125           1203.09       0.01
51          0.371325    0.873019       0.317537       0.899805           1230.61       0.01
52          0.368561    0.874578       0.338393       0.891016           1254.34       0.01
53          0.366614    0.872975       0.318233       0.89707            1278.2        0.01
54          0.366082    0.874667       0.328654       0.892187           1301.94       0.01
55          0.365454    0.873331       0.311355       0.902148           1325.61       0.01
56          0.357191    0.876509       0.330824       0.895898           1349.36       0.01
57          0.361274    0.874667       0.320739       0.897461           1373.1        0.01
58          0.354885    0.878339       0.308104       0.901172           1396.78       0.01
59          0.359033    0.876931       0.316534       0.900586           1420.55       0.01
60          0.356422    0.876291       0.366406       0.888281           1444.25       0.01
61          0.314513    0.891513       0.261128       0.914062           1471.67       0.001
62          0.275813    0.905159       0.257487       0.916406           1495.37       0.001
63          0.268649    0.907804       0.253204       0.918164           1519.12       0.001
64          0.264412    0.908298       0.256485       0.918555           1542.84       0.001
65          0.261826    0.910511       0.253851       0.917773           1566.66       0.001
66          0.255704    0.911998       0.257955       0.916602           1590.48       0.001
67          0.25836     0.909566       0.256463       0.919141           1614.22       0.001
68          0.251792    0.912753       0.254393       0.920117           1638.04       0.001
69          0.251154    0.914508       0.25251        0.920508           1661.81       0.001
70          0.24735     0.91464        0.255754       0.91875            1685.83       0.001
71          0.241314    0.917268       0.253207       0.921094           1713.35       0.001
72          0.246059    0.914617       0.257668       0.920508           1737.18       0.001
73          0.238097    0.918213       0.257092       0.919727           1761.02       0.001
74          0.235832    0.918981       0.251519       0.919922           1784.77       0.001
75          0.236254    0.918857       0.253711       0.919531           1808.58       0.001
76          0.235273    0.917646       0.249922       0.920117           1832.36       0.001
77          0.233553    0.918635       0.251188       0.921094           1856.13       0.001
78          0.229216    0.920829       0.256883       0.921484           1879.92       0.001
79          0.231176    0.919877       0.254759       0.92207            1903.75       0.001
80          0.227571    0.921007       0.251693       0.920508           1927.51       0.001
81          0.229313    0.92041        0.257421       0.920508           1955.13       0.001
82          0.225018    0.922896       0.255896       0.919922           1978.98       0.001
83          0.223251    0.923478       0.25848        0.918555           2002.75       0.001
84          0.221238    0.924294       0.259179       0.920703           2026.56       0.001
85          0.222084    0.923411       0.251648       0.920313           2050.3        0.001
86          0.221973    0.924339       0.252463       0.920898           2074.11       0.001
87          0.217851    0.925147       0.252802       0.920898           2097.85       0.001
88          0.215969    0.925138       0.255438       0.921094           2121.65       0.001
89          0.217307    0.92294        0.253423       0.920313           2145.48       0.001
90          0.217241    0.925058       0.254931       0.919922           2169.54       0.001
91          0.208632    0.926381       0.247816       0.923047           2195.58       0.0001
92          0.20828     0.927039       0.248997       0.922656           2220.52       0.0001
93          0.201544    0.929266       0.247823       0.922852           2245.4        0.0001
94          0.204958    0.929732       0.249951       0.922461           2270.17       0.0001
95          0.204098    0.931064       0.248762       0.923242           2295.04       0.0001
96          0.203404    0.929732       0.24906        0.923242           2319.85       0.0001
97          0.202788    0.929066       0.25099        0.92168            2344.72       0.0001
98          0.197761    0.931485       0.250801       0.922656           2369.6        0.0001
99          0.199837    0.930333       0.248233       0.922656           2394.46       0.0001
100         0.200537    0.930331       0.249902       0.921875           2419.37       0.0001
Test accuracy: 0.9247429
</pre></div></div>
</div>
<p>わずかに精度が向上しました．他にも，ResNet [He
2016]と呼ばれる有名なネットワーク構造を採用して学習を行うなど，簡単に試せる改善方法がいくつかあります．ぜひ色々と試してみてください．</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="参考文献">
<h2>4.7. 参考文献<a class="headerlink" href="#参考文献" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>[Tokui 2015] Tokui, S., Oono, K., Hido, S. and Clayton, J., Chainer: a
Next-Generation Open Source Framework for Deep Learning, Proceedings of
Workshop on Machine Learning Systems(LearningSys) in The Twenty-ninth
Annual Conference on Neural Information Processing Systems (NIPS),
(2015)</p>
<p>[Niitani 2017] Yusuke Niitani, Toru Ogawa, Shunta Saito, Masaki Saito,
“ChainerCV: a Library for Deep Learning in Computer Vision”, ACM
Multimedia (ACMMM), Open Source Software Competition, 2017</p>
<p>[Hidaka 2017] Masatoshi Hidaka, Yuichiro Kikura, Yoshitaka Ushiku,
Tatsuya Harada. WebDNN: Fastest DNN Execution Framework on Web Browser.
ACM International Conference on Multimedia (ACMMM), Open Source Software
Competition, pp.1213-1216, 2017.</p>
<p>[He 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, “Deep
Residual Learning for Image Recognition”, CVPR 2016</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Image_Segmentation.html" class="btn btn-neutral float-right" title="5. 実践編: MRI画像のセグメンテーション" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Introduction_to_Neural_Network.html" class="btn btn-neutral" title="3. ニューラルネットワークの基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>