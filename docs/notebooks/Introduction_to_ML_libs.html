

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>2. 機械学習ライブラリの基礎 &mdash; メディカルAI専門コース オンライン講義資料  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="3. ニューラルネットワークの基礎" href="Introduction_to_Neural_Network.html" />
    <link rel="prev" title="1. 機械学習に必要な数学の基礎" href="Basic_Math_for_ML.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2. 機械学習ライブラリの基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#単回帰分析">2.1. 単回帰分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#問題設定（単回帰分析）">2.1.1. 問題設定（単回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step1.-モデルを決める（単回帰分析）">2.1.2. Step1. モデルを決める（単回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step2.-目的関数を決める（単回帰分析）">2.1.3. Step2. 目的関数を決める（単回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step3.-最適なパラメータを求める（単回帰分析）">2.1.4. Step3. 最適なパラメータを求める（単回帰分析）</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#重回帰分析">2.2. 重回帰分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#問題設定（重回帰分析）">2.2.1. 問題設定（重回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step1.-モデルを決める（重回帰分析）">2.2.2. Step1. モデルを決める（重回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step2.-目的関数を決める（重回帰分析）">2.2.3. Step2. 目的関数を決める（重回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step3.-パラメータを最適化する（重回帰分析）">2.2.4. Step3. パラメータを最適化する（重回帰分析）</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#NumPyによる実装">2.3. NumPyによる実装</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Scikit-learnによる機械学習アルゴリズムの実行">2.4. Scikit-learnによる機械学習アルゴリズムの実行</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Scikit-learn-基礎編">2.4.1. Scikit-learn 基礎編</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scikit-learn-応用編">2.4.2. Scikit-learn 応用編</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#サンプルデータセットの使用">2.4.2.1. サンプルデータセットの使用</a></li>
<li class="toctree-l4"><a class="reference internal" href="#データセットの分割">2.4.2.2. データセットの分割</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. ニューラルネットワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 実践編: ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>2. 機械学習ライブラリの基礎</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Introduction_to_ML_libs.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Introduction_to_ML_libs.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="機械学習ライブラリの基礎">
<h1>2. 機械学習ライブラリの基礎<a class="headerlink" href="#機械学習ライブラリの基礎" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>本章では，代表的な機械学習アルゴリズムの紹介とその使い方のポイントを数学的な背景と合わせて紹介していきます．
機械学習の考え方を身に着ける練習として，<strong>単回帰分析</strong>と<strong>重回帰分析</strong>のアルゴリズムを数式と一緒に考えていきましょう．これらを学ぶことで微分と線形代数，統計の使い方が見えてくると思います．重回帰分析は次章で紹介するニューラルネットワークでもその考え方のベースになるところが多いため，しっかりと数式を理解しておきましょう．</p>
<div class="section" id="単回帰分析">
<h2>2.1. 単回帰分析<a class="headerlink" href="#単回帰分析" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まずはじめに，機械学習アルゴリズムの中でも最も基礎的な手法のひとつである，単回帰分析について説明します．機械学習アルゴリズムは，<strong>教師あり学習</strong>と<strong>教師なし学習</strong>に大別され，単回帰分析は教師あり学習の一種です．教師あり学習の典型的な問題として，<span class="math notranslate nohighlight">\(10\)</span>や<span class="math notranslate nohighlight">\(0.1\)</span>のように数値（厳密には連続値）を予測する<strong>回帰</strong>と，赤ワイン
or
白ワインのようにカテゴリ値を予測する<strong>分類</strong>があります．単回帰分析はその名の通り，回帰を取り扱う手法で，ひとつの入力変数からひとつの出力変数を予測する機械学習アルゴリズムです．</p>
<div class="section" id="問題設定（単回帰分析）">
<h3>2.1.1. 問題設定（単回帰分析）<a class="headerlink" href="#問題設定（単回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>機械学習では，データをもとに学習を行いますが，データに含まれる情報の中から何を利用し，何を予測させるかは人間が決める必要があります．</p>
<p>ここでは例として，家賃を予測する問題を考えることにします．従って，家賃が
<strong>出力変数</strong> <span class="math notranslate nohighlight">\(y\)</span> となります．</p>
<p>次に， <strong>入力変数</strong>
として何を採用するかを考えます．家賃の予測では，たとえば部屋の広さ，駅からの距離，犯罪発生率などを入力変数として検討することができます．ここでは部屋の広さを入力変数
<span class="math notranslate nohighlight">\(x\)</span>
として採用することにしましょう．実際には，複数の入力変数候補があった際に，それらすべてを扱うことができるようなモデル化が一般的ですが，それは次の重回帰分析以降で紹介することにします．</p>
<p>機械学習のアルゴリズムでは，どの手法も大きく分けて次の3ステップで成り立っています．</p>
<ul class="simple">
<li>Step1: モデルを決める</li>
<li>Step2: 目的関数を決める</li>
<li>Step3: 最適なパラメータを求める</li>
</ul>
<p>上記の3ステップについて，順に説明していきます．</p>
</div>
<div class="section" id="Step1.-モデルを決める（単回帰分析）">
<h3>2.1.2. Step1. モデルを決める（単回帰分析）<a class="headerlink" href="#Step1.-モデルを決める（単回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>まずStep1では<strong>モデル</strong>を決めます．モデルとは，出力変数 <span class="math notranslate nohighlight">\(y\)</span>
と入力変数 <span class="math notranslate nohighlight">\(x\)</span>
の関係性を<strong>定式化</strong>したものです．どのように定式化すれば，家賃をうまく予測することができるのでしょうか．このモデル設計は現在は人手で行うのが一般的であり，機械が自動的に決めてくれるわけではありません（最近ではAutoMLなど，モデルも自動決定する研究も進展してきています）．</p>
<p>例えば，与えられたデータセットにおいて，家賃と部屋の広さの関係性が次のようになっていたとします．</p>
<div class="figure" id="id13">
<img alt="家賃と部屋の広さの関係" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/01.png" />
<p class="caption"><span class="caption-text">家賃と部屋の広さの関係</span></p>
</div>
<p>この場合，部屋が広くなるほど，家賃が高くなるという関係がみられ，直線を予測に用いるのが妥当にみえます．</p>
<div class="figure" id="id14">
<img alt="直線式によるモデル化" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/02.png" />
<p class="caption"><span class="caption-text">直線式によるモデル化</span></p>
</div>
<p>そこで今回は直線をモデルとして採用して，Step1のモデルを以下のように定式化します．</p>
<div class="math notranslate nohighlight">
\[y = wx + b\]</div>
<p>ここで<span class="math notranslate nohighlight">\(w\)</span>は傾き，<span class="math notranslate nohighlight">\(b\)</span>は切片とよばれるパラメータです（機械学習では，傾きを<strong>重み
(weight)</strong> <span class="math notranslate nohighlight">\(w\)</span>, 切片を<strong>バイアス (bias)</strong> <span class="math notranslate nohighlight">\(b\)</span>
という記号で表現するのが一般的です）．</p>
<p>単回帰分析では，このようにモデルを直線 <span class="math notranslate nohighlight">\(y = wx + b\)</span>
と決めて，重み<span class="math notranslate nohighlight">\(w\)</span>とバイアス<span class="math notranslate nohighlight">\(b\)</span>をデータにうまくフィットするように調整していきます．</p>
<p>多くの機械学習ではこのようなパラメータで特徴付けられたモデルを使い，与えられた<strong>データセット</strong>に適合するように最適なパラメータを求めることが目標となります．ここでデータセットは，<strong>入力変数</strong>である部屋の広さ
<span class="math notranslate nohighlight">\(x\)</span> と，<strong>教師データ</strong>となる家賃 <span class="math notranslate nohighlight">\(t\)</span>
の組からなるデータの集合です（本解説では，機械学習による予測値を
<span class="math notranslate nohighlight">\(y\)</span> ，教師データとして与えるものを <span class="math notranslate nohighlight">\(t\)</span>
と使い分けています）．</p>
<p>データセットは <span class="math notranslate nohighlight">\(\mathcal{D} = \{x_n, t_n\}_{n=1}^{N}\)</span>
として表されることもあります．ここで，添え字 <span class="math notranslate nohighlight">\(n\)</span>
(<span class="math notranslate nohighlight">\(n=1,2,\ldots,N\)</span>) は <span class="math notranslate nohighlight">\(n\)</span>
番目の物件という意味であり，<span class="math notranslate nohighlight">\(N\)</span> は全体の物件数のことです．この
<span class="math notranslate nohighlight">\(N\)</span> は<strong>サンプル数</strong>とよばれています．</p>
<p>ここで，この後の計算を楽に進めるために，<strong>データの中心化</strong>というテクニックを紹介します．下図に示すように，部屋の広さと家賃は両方とも正の値であるため，左のグラフのような形になります．中心化では，<strong>平均を</strong>
<span class="math notranslate nohighlight">\(\boldsymbol{0}\)</span>とした中央に配置するような変換の処理を施します．この中心化は多くのアルゴリズムで前処理として行うことが一般的です．厳密には前章で紹介した中心化込みのスケーリングがよく用いられます．</p>
<div class="figure" id="id15">
<img alt="中心化処理" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/03.png" />
<p class="caption"><span class="caption-text">中心化処理</span></p>
</div>
<p>この処理を行う理由の一つとして，下図のように，データの中心化によってバイアス
<span class="math notranslate nohighlight">\(b\)</span> が<span class="math notranslate nohighlight">\(0\)</span>となり，<span class="math notranslate nohighlight">\(y_{c} = wx_{c}\)</span>
のように，モデルをバイアス成分なしで表現することができるということが挙げられます，これによって，調整すべきパラメータを減らすことができます．</p>
<div class="figure" id="id16">
<img alt="中心化後の直線式" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/04.png" />
<p class="caption"><span class="caption-text">中心化後の直線式</span></p>
</div>
<p>データの中心化は入出力の平均をデータの全体から引くことで実現されます．つまり，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
x_{c} &amp;= x - \bar{x} \\
t_{c} &amp;= t - \bar{t}
\end{aligned}\end{split}\]</div>
<p>となります．例えば，具体的な数値で見ると，下図の通りです．
<img alt="中心化前後の数値比較" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/05.png" /></p>
<p>中心化後を示す添え字の <span class="math notranslate nohighlight">\(c\)</span>
に関しては表現が冗長となるため，今後はこの添え字を省略し，データの中心化を事前に行っていることを前提とします．この時，モデルは</p>
<div class="math notranslate nohighlight">
\[y = wx\]</div>
<p>となり，単回帰分析の目標は，データセット
<span class="math notranslate nohighlight">\(\mathcal{D} = \{x_n, t_n\}_{n=1}^{N}\)</span> に基づいて，パラメータ
<span class="math notranslate nohighlight">\(w\)</span> を<strong>適切</strong>に調整することになります．</p>
</div>
<div class="section" id="Step2.-目的関数を決める（単回帰分析）">
<h3>2.1.3. Step2. 目的関数を決める（単回帰分析）<a class="headerlink" href="#Step2.-目的関数を決める（単回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>1章で説明したように，教師あり学習では多くの場合，目的関数を設計し，その目的関数を最小化（または最大化）することでモデルの学習を行います．</p>
<p>今回は教師データと予測値が一致することが目標であり，それを表す目的関数として教師データと予測値の二乗誤差を使います．二乗誤差が<span class="math notranslate nohighlight">\(0\)</span>であるとき，またその時のみ<span class="math notranslate nohighlight">\(t = y\)</span>
となり，完璧な予測を達成しているといえます．<span class="math notranslate nohighlight">\(n\)</span>
番目の物件に対する教師データ<span class="math notranslate nohighlight">\(t_{n}\)</span>
と予測値<span class="math notranslate nohighlight">\(y_{n}\)</span>の二乗誤差は</p>
<div class="math notranslate nohighlight">
\[(t_{n} - y_{n})^{2}\]</div>
<p>となります．これを全物件で考慮する必要があるため，最終的な目的関数はその総和をとり，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\left( t_{1}-y_{1}\right)^{2}+\left( t_{2}-y_{2}\right)^{2}+\ldots + (t_{N}-y_{N})^{2} \\
&amp;=\sum^{N}_{n=1}\left( t_{n}-y_{n}\right)^{2}\\
\end{aligned}\end{split}\]</div>
<p>となります．また，Step1で決めたモデル</p>
<div class="math notranslate nohighlight">
\[y_{n} = wx_{n}\]</div>
<p>を代入すると，目的関数は</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}=\sum^{N}_{n=1}\left( t_{n}-wx_{n}\right)^{2}\]</div>
<p>とパラメータを含んだ形式で表現することができます．このような関数を損失関数とよぶことを思い出してください．</p>
</div>
<div class="section" id="Step3.-最適なパラメータを求める（単回帰分析）">
<h3>2.1.4. Step3. 最適なパラメータを求める（単回帰分析）<a class="headerlink" href="#Step3.-最適なパラメータを求める（単回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>最後は目的関数を最小化するようなパラメータを求めます．ここで，ある関数を最小化する点を求める方法として，微分が使えることを既に学んでいます．今回のような差の二乗の場合，微分して「傾き0」となる点が損失が<span class="math notranslate nohighlight">\(0\)</span>となる点です．目的関数の微分を求めると，次のようになります．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac{\partial }{\partial w} \mathcal{L}  &amp;= \dfrac{\partial}{\partial w} { \sum^{N}_{n=1} ( t_{n}-wx_{n})^{2} }\\
\end{aligned}\end{split}\]</div>
<p>ここで，微分は<strong>線形性</strong>の性質を持っており，特に和の微分は，微分の和であることを利用して次を得ます．</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial w} \mathcal{L}=\sum^{N}_{n=1}\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2}\]</div>
<p>ここで微分と総和 <span class="math notranslate nohighlight">\(\sum\)</span>
の記号が入れ替わっています．次に，和の各項をみると，</p>
<div class="math notranslate nohighlight">
\[\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2}\]</div>
<p>の部分は<span class="math notranslate nohighlight">\(t_n - wx_n\)</span>とその二乗の<strong>合成関数</strong>になっていることがわかります．<span class="math notranslate nohighlight">\(u_{n} = t_{n} - wx_{n}\)</span>,
<span class="math notranslate nohighlight">\(f(u_{n}) = u_{n}^{2}\)</span> とおくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2} &amp;=  \dfrac {\partial }{\partial w} f(u_{n}) \\
&amp;= \dfrac {\partial u_{n}}{\partial w} \dfrac{\partial f(u_{n})}{\partial u_{n}} \\
&amp;=-x_{n} \times 2 u_{n}  \\
&amp;= -2x_{n}( t_{n}-wx_{n} )
\end{aligned}\end{split}\]</div>
<p>が得られます．これより，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac{\partial }{\partial w} \mathcal{L}
&amp;=\sum^{N}_{n=1}\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2}
\\&amp;=-\sum^{N}_{n=1}2x_{n}\left( t_{n}-wx_{n}\right)
\end{aligned}\end{split}\]</div>
<p>となります．この微分の値が0となるように<span class="math notranslate nohighlight">\(w\)</span>を求めていくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial w} \mathcal{L} &amp;=0\\
-2\sum^{N}_{n=1}x_{n}\left( t_{n}-wx_{n}\right) &amp;=0\\
-2 \sum^{N}_{n=1}x_{n}t_{n} + 2\sum^{N}_{n=1}wx^{2}_{n}&amp;=0\\
-2\sum^{N}_{n=1}x_{n}t_{n}+2w\sum^{N}_{n=1}x^{2}_{n}&amp;=0\\
w\sum^{N}_{n=1}x^{2}_{n}&amp;=\sum^{N}_{n=1}x_{n}t_{n}\\
\end{aligned}\end{split}\]</div>
<p>より，</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
w&amp;=\dfrac {\displaystyle  \sum^{N}_{n=1}x_{n}t_{n}}{\displaystyle  \sum^{N}_{n=1}x^{2}_{n}}
\end{aligned}\]</div>
<p>と求まりました．この求まったパラメータ <span class="math notranslate nohighlight">\(w\)</span>
を確認すると，与えられたデータセット
<span class="math notranslate nohighlight">\(\mathcal{D} = \{x_n, t_n\}_{n=1}^{N}\)</span>
のみから決定できていることがわかります．</p>
<p>次に，例題にあげていた数値例でパラメータ <span class="math notranslate nohighlight">\(w\)</span>
を求めてみましょう．まずは，データの中心化を行うために，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\bar{x} &amp;= \dfrac{1}{3} (1 + 2 + 3) = 2 \\
\bar{t} &amp;= \dfrac{1}{3}(2 + 3.9 + 6.1) = 4
\end{aligned}\end{split}\]</div>
<p>とそれぞれの平均を求め，各変数に対して前処理として，中心化の処理を施すと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
x_{1} &amp;= 1 - 2 = -1 \\
x_{2} &amp;= 2 -2 = 0 \\
x_{3} &amp;= 3- 2 = 1\\
t_{1} &amp;= 2 - 4 = -2\\
t_{2} &amp;= 3.9 - 4 = -0.1\\
t_{3} &amp;= 6.1 - 4 = 2.1
\end{aligned}\end{split}\]</div>
<p>となります．そして，中心化後の値を用いて，最適なパラメータ<span class="math notranslate nohighlight">\(w\)</span>を導出すると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
w &amp;= \dfrac{\displaystyle \sum_{n=1}^{N}x_{n}t_{n}}{\displaystyle  \sum_{n=1}^{N}x_{n}^{2}} \\
&amp;= \dfrac{x_{1}t_{1} + x_{2}t_{2} + x_{3}t_{3}}{x_{1}^{2} + x_{2}^{2} + x_{3}^{2}} \\
&amp;= \dfrac{-1 \times (-2) + 0 \times 0.1 + 1 \times 2.1}{(-1)^{2} + 0^2 + 1^2} \\
&amp;= 2.05
\end{aligned}\end{split}\]</div>
<p>と求まりました．これで単回帰分析の学習が完了しました．この求まったパラメータを使用したモデルが<strong>学習済みモデル</strong>となります．</p>
<p>続いて，このモデルを使って新しいサンプルに対する予測をしてみましょう．学習したモデルを使って新たな入力データについて予測値を計算する処理を
<strong>推論</strong> とよびます．例えば，新しいサンプル <span class="math notranslate nohighlight">\(x_{q}=1.5\)</span>
に対する予測値は次のように求まります，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y_{c} &amp;= wx_{c} \\
y_{q} - \bar{t} &amp;= w(x_{q}-\bar{x}) \\
\Rightarrow y_{q} &amp;= w(x_{q}-\bar{x}) + \bar{t} \\
&amp;= 2.05 \times (1.5 - 2) + 4 \\
&amp;= 2.975
\end{aligned}\end{split}\]</div>
<p>モデルは中心化データを用いて学習を行ったので，実際の予測値は中心化したデータを元に戻すことを忘れないようにしましょう．</p>
<p>以上が，単回帰分析の一連の手順となります．</p>
</div>
</div>
<div class="section" id="重回帰分析">
<h2>2.2. 重回帰分析<a class="headerlink" href="#重回帰分析" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>次に，多変数の入力変数を扱う重回帰分析を扱います．この重回帰分析を学ぶことで線形代数に関する知識が深まります．</p>
<p>重回帰分析は単回帰分析と同様に教師あり学習の一種であり，回帰を取り扱う手法です．問題設定は，ほとんど単回帰分析と同じですが，重回帰分析では入力変数の数が複数となります．つまり，複数の入力変数から出力変数を予測できるような機械学習アルゴリズムです．</p>
<div class="section" id="問題設定（重回帰分析）">
<h3>2.2.1. 問題設定（重回帰分析）<a class="headerlink" href="#問題設定（重回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここでは単回帰分析の場合と同様に家賃を予測する問題を考え，家賃を出力変数
<span class="math notranslate nohighlight">\(y\)</span>
とします．入力変数としては，単回帰分析では考慮しきれていなかった駅からの距離や犯罪発生率なども考慮していきます．例えば，部屋の広さ
<span class="math notranslate nohighlight">\(x_{1}\)</span>, 駅からの距離 <span class="math notranslate nohighlight">\(x_{2}\)</span>, …, 犯罪発生率 <span class="math notranslate nohighlight">\(x_{M}\)</span>
のように <span class="math notranslate nohighlight">\(M\)</span>
個の入力変数があるとします（<span class="math notranslate nohighlight">\(M=1\)</span>の場合，単回帰分析の問題に帰着されます）．</p>
<p>単回帰分析と同様，以下の3ステップで学習していきます．</p>
<ul class="simple">
<li>モデルを決める</li>
<li>目的関数を決める</li>
<li>最適なパラメータを求める</li>
</ul>
</div>
<div class="section" id="Step1.-モデルを決める（重回帰分析）">
<h3>2.2.2. Step1. モデルを決める（重回帰分析）<a class="headerlink" href="#Step1.-モデルを決める（重回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>単回帰分析のモデルは，</p>
<div class="math notranslate nohighlight">
\[y = wx + b\]</div>
<p>であり，<span class="math notranslate nohighlight">\(w\)</span> を重み（weight），<span class="math notranslate nohighlight">\(b\)</span> をバイアス (bias)
とよびました．重回帰分析では，この式を複数の入力変数へと拡張し，</p>
<div class="math notranslate nohighlight">
\[y=w_{1}x_{1}+w_{2}x_{2}+\ldots +w_{M}x_{M}+b\]</div>
<p>のような<strong>線形結合</strong>の形で表します．この場合，各入力変数は線形に出力変数に影響を与えることを仮定しており，かなり単純なモデル化といえます．実際には，入力変数間に非線形な依存関係が存在する場合には，そのことを考慮してモデル化を行う必要があります．それについては今後説明していきます．</p>
<p>重回帰分析のモデルは総和の記号を使って整理すると，</p>
<div class="math notranslate nohighlight">
\[y = \sum_{m=1}^{M} w_{m} x_{m} + b\]</div>
<p>のように書くことができます．さらにここで，<span class="math notranslate nohighlight">\(x_0 = 1\)</span>，<span class="math notranslate nohighlight">\(w_0 = b\)</span>とおくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y&amp;=w_{1}x_{1}+w_{2}x_{2}+\ldots +w_{M}x_{M}+b\\
&amp;=w_{1}x_{1}+w_{2}x_{2}+\ldots +w_{M}x_{M}+w_{0} x_{0}\\
&amp;=w_{0}x_{0}+w_{1}x_{1}+\ldots +w_{M}x_{M}\\
\end{aligned}\end{split}\]</div>
<p>のようにバイアス <span class="math notranslate nohighlight">\(b\)</span>
を総和に包含することができます．そして，この式を整理していくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y&amp;=w_{0}x_{0}+w_{1}x_{1}+\ldots +w_{M}x_{M}\\
&amp;=\begin{bmatrix}
w_{0} &amp; w_{1} &amp; \ldots  &amp; w_{M}
\end{bmatrix}\begin{bmatrix}
x_{0} \\
x_{1} \\
\vdots  \\
x_{M}
\end{bmatrix}\\
&amp;=\boldsymbol{w}^{T}\boldsymbol{x}
\end{aligned}\end{split}\]</div>
<p>のように，ベクトルの内積で表現することができます．また，今後取り扱う際には，<span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>
が前に来ているほうが計算上便利であるため，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y&amp;=w_{0}x_{0}+w_{1}x_{1}+\ldots +w_{M}x_{M}\\
&amp;=\begin{bmatrix}
x_{0} &amp; x_{1} &amp; \ldots  &amp; x_{M}
\end{bmatrix}\begin{bmatrix}
w_{0} \\
w_{1} \\
\vdots  \\
w_{M}
\end{bmatrix}\\
&amp;=\boldsymbol{x}^{T}\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>として表現します．これが重回帰分析のモデルです．今回はパラメータとして
<span class="math notranslate nohighlight">\(M+1\)</span> 個の重み <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> を求めていきます．</p>
</div>
<div class="section" id="Step2.-目的関数を決める（重回帰分析）">
<h3>2.2.3. Step2. 目的関数を決める（重回帰分析）<a class="headerlink" href="#Step2.-目的関数を決める（重回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>単回帰分析では，教師データ<span class="math notranslate nohighlight">\(t\)</span>と予測値<span class="math notranslate nohighlight">\(y\)</span>の二乗誤差が小さいほど良い予測であるとし，その総和を目的関数として定めました．重回帰分析でも，予測値<span class="math notranslate nohighlight">\(y\)</span>を求めるということは同じであるため，次のような同じ目的関数を使います．</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathcal{L}&amp;=\left( t_{1}-y_{1}\right)^{2}+\left( t_{2}-y_{2}\right)^{2}+\ldots + \left( t_{N}-y_{N}\right)^{2}
\end{aligned}\]</div>
<p>このように，<strong>二乗誤差の総和</strong>を単回帰分析同様，目的関数として採用します．単回帰分析では，これを</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}=\sum^{N}_{n=1} ( t_{n} - y_{n})^{2}\]</div>
<p>のように，総和の記号を使ってまとめていましたが，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\left( t_{1}-y_{1}\right)^{2}+\left( t_{2}-y_{2}\right)^{2}+\ldots + \left( t_{N}-y_{N}\right)^{2}\\
&amp;=\begin{bmatrix} t_{1} - y_{1} &amp; t_{2}-y_{2} &amp; \ldots &amp; t_{N}-y_{N} \end{bmatrix} \begin{bmatrix}
t_{1}-y_{1} \\
t_{2}-y_{2} \\
\vdots \\
t_{N}-y_{N}
\end{bmatrix}\\
&amp;=\left( \boldsymbol{t}-\boldsymbol{y}\right)^{T}\left( \boldsymbol{t}-\boldsymbol{y}\right)
\end{aligned}\end{split}\]</div>
<p>のようにベクトルを使って表現することもできます．また，<span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
に関して，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{y}=\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{N}
\end{bmatrix}=\begin{bmatrix}
\boldsymbol{x}_{1}^{T}\boldsymbol{w} \\
\boldsymbol{x}_{2}^{T}\boldsymbol{w} \\
\vdots  \\
\boldsymbol{x}_{N}^{T}\boldsymbol{w}
\end{bmatrix}
=\begin{bmatrix}
\boldsymbol{x}_{1}^{T} \\
\boldsymbol{x}_{2}^{T} \\
\vdots  \\
\boldsymbol{x}_{N}^{T}
\end{bmatrix}
\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>のように書くことができます．これを整理すると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{y}&amp;=
\begin{bmatrix}
x_{10} &amp; x_{11} &amp; x_{12} &amp; \ldots  &amp; x_{1M} \\
x_{20} &amp; x_{21} &amp; x_{22} &amp; \ldots  &amp; x_{2M} \\
\vdots  &amp; \vdots  &amp; \vdots  &amp; \ddots  \\
x_{N0} &amp; x_{N1} &amp; x_{N{2}} &amp; \ldots  &amp; x_{NM}
\end{bmatrix}\begin{bmatrix}
w_{0} \\
w_{1} \\
w_{2} \\
\vdots  \\
w_{M}
\end{bmatrix}\\
&amp;=\boldsymbol{X}\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>と表記できます．ここで，行（横）方向がサンプルを表しており，例えば各物件に対応します．列（縦）方向が入力変数を表しており，例えば，部屋の広さや駅からの距離などに対応します．もう少し具体的な数値で考えると，部屋の広さ
<span class="math notranslate nohighlight">\(= 50m^{2}\)</span> ，駅からの距離 <span class="math notranslate nohighlight">\(= 600 m\)</span> ，犯罪発生率
<span class="math notranslate nohighlight">\(= 2\)</span>% のような <span class="math notranslate nohighlight">\(n\)</span>
番目の物件の場合，入力変数の数<span class="math notranslate nohighlight">\(M=3\)</span>であり，</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x}_{n}^{T} = \begin{bmatrix}
1 &amp; 50 &amp; 600 &amp; 0.02
\end{bmatrix}\]</div>
<p>のようにデータが行方向に格納されているイメージです．先頭の <span class="math notranslate nohighlight">\(1\)</span>
はバイアスを包含する際に使用している <span class="math notranslate nohighlight">\(x_{0}\)</span>
であることに注意してください．</p>
</div>
<div class="section" id="Step3.-パラメータを最適化する（重回帰分析）">
<h3>2.2.4. Step3. パラメータを最適化する（重回帰分析）<a class="headerlink" href="#Step3.-パラメータを最適化する（重回帰分析）" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは，Step2の目的関数を最小化する，モデルのパラメータ<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>を求めていきましょう．</p>
<p><strong>※ここでは，式変形を駆使しながら最適パラメータの解析的な解を求めていきますが，導出過程が少々複雑であり，導出結果は次節(§2.3)で示されているので，興味のある方以外はスキップして次節に進んでいただいて構いません．</strong></p>
<p>まずは目的関数に関して，パラメータ <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
で表現できるように式変形を行うと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\left( \boldsymbol{t}-\boldsymbol{y}\right)^{T}\left( \boldsymbol{t}-\boldsymbol{y}\right) \\
&amp;=\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right)^{T}\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right) \\
&amp;= \left\{ \boldsymbol{t}^{T}-(\boldsymbol{X}\boldsymbol{w})^{T}\right\}\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right) \\
&amp;=\left( \boldsymbol{t}^{T}-\boldsymbol{w}^{T}\boldsymbol{X}^{T}\right)\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right)
\end{aligned}\end{split}\]</div>
<p>となります．ここで，転置の公式
<span class="math notranslate nohighlight">\((\boldsymbol{A}\boldsymbol{B})^{T} = \boldsymbol{B}^{T}\boldsymbol{A}^{T}\)</span>
を使っていることに注意しましょう．さらに分配法則を使って展開を進めていくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\boldsymbol{t}^{T}\boldsymbol{t}-\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}-\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{t} + \boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}\\
\end{aligned}\end{split}\]</div>
<p>となります．この目的関数に対しパラメータの<span class="math notranslate nohighlight">\(w\)</span>について偏微分をとりたいわけですが，その前にこの式はもう少し整理することができます．はじめに，</p>
<div class="math notranslate nohighlight">
\[(1)^T = 1\]</div>
<p>というように，スカラーは転置しても変化しません．上式の中で出てくる
<span class="math notranslate nohighlight">\(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}\)</span>
はスカラーなので，</p>
<div class="math notranslate nohighlight">
\[(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w})^{T} = \boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}\]</div>
<p>が成り立ちます．さらに，転置の公式
<span class="math notranslate nohighlight">\((\boldsymbol{A}\boldsymbol{B}\boldsymbol{C})^T = \boldsymbol{C}^T\boldsymbol{B}^T\boldsymbol{A}^T\)</span>
より，</p>
<div class="math notranslate nohighlight">
\[(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w})^T = \boldsymbol{w}^{T} \boldsymbol{X}^{T} \boldsymbol{t}\]</div>
<p>も成り立ちます．これより，</p>
<div class="math notranslate nohighlight">
\[(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w})^{T} = \boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w} = \boldsymbol{w}^{T} \boldsymbol{X}^{T} \boldsymbol{t}\]</div>
<p>を導くことができます．目的関数を <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
とおくと，上の式を利用して，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}=\boldsymbol{t}^{T}\boldsymbol{t}-2\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w} + \boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}\\
\end{aligned}\end{split}\]</div>
<p>とまとめることができます．ここで， <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
に関する偏微分を行っていくため， <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
以外の定数項をまとめていくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\boldsymbol{t}^{T}\boldsymbol{t}-2\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}+\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}\\
&amp;=\boldsymbol{t}^{T}\boldsymbol{t}-2\left( \boldsymbol{X}^{T}\boldsymbol{t}\right)^{T} \boldsymbol{w}+\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w} \\
&amp;= \gamma + \boldsymbol{\beta}^{T}\boldsymbol{w} + \boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>のように，線形代数で学んだ <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
に関する二次形式（二次関数）で表現することができました．ここで，<span class="math notranslate nohighlight">\(\boldsymbol{A}= \boldsymbol{X}^{T}\boldsymbol{X}, \ \boldsymbol{\beta} =-2 \boldsymbol{X}^{T}\boldsymbol{t}, \ \gamma = \boldsymbol{t}^{T}\boldsymbol{t}\)</span>
であり，<span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>
を転置の形式にした理由は，線形代数で学んだベクトルで微分の公式の形式に合わせるための工夫です．</p>
<p>それでは，目的関数を最小化することができるパラメータ
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
の求め方を考えましょう．先述の通り，目的関数はパラメータ
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>に関して二次関数です．例えば，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{w} = \begin{bmatrix}
w_{1} \\ w_{2}
\end{bmatrix},
\boldsymbol{A}=\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix},\boldsymbol{\beta}=\begin{bmatrix}
1 \\
2
\end{bmatrix}, \gamma = 1
\end{aligned}\end{split}\]</div>
<p>のように具体的な数値例で考えてみると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L} &amp; =
\boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w} + \boldsymbol{\beta}^{T}\boldsymbol{w} + \gamma \\
&amp;=
\begin{bmatrix}
w_{1} &amp; w_{2}
\end{bmatrix}\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
w_{1} \\
w_{2}
\end{bmatrix}
+\begin{bmatrix}
1 &amp; 2
\end{bmatrix} \begin{bmatrix}
w_{1} \\
w_{2}
\end{bmatrix} + 1 \\
&amp;=
\begin{bmatrix}
w_{1} &amp; w_{2}
\end{bmatrix}
\begin{bmatrix}
w_{1} + 2w_{2} \\
3w_{1} + 4w_{2}
\end{bmatrix} + w_{1} + 2w_{2} + 1 \\
&amp;=w_{1}\left( w_{1} + 2w_{2}\right) + w_{2}\left( 3w_{1} + 4w_{2}\right) + w_{1} + 2w_{2} + 1 \\
&amp;=w^{2}_{1} + 5w_{1}w_{2} + 4w^{2}_{2} + w_{1} + 2w_{2}+1 \\
\end{aligned}\end{split}\]</div>
<p>となり，<span class="math notranslate nohighlight">\(w_{1}, w_{2}\)</span>に関してそれぞれまとめると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;= w^{2}_{1} + \left( 5w_{2} + 1\right) w_{1} +
\left( 4w^{2}_{2}+2w_{2}+1\right) \\
&amp;=4w^{2}_{2} + \left(5w_{1} + 2\right) w_{2} + \left( w^{2}_{1} + w_{1} + 1\right) \end{aligned}\end{split}\]</div>
<p>のようにそれぞれの二次関数であることがわかります．</p>
<p>そして，二次関数であれば，下図のような形となります．</p>
<div class="figure" id="id17">
<img alt="パラメータと目的関数の関係（2次元）" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/06.png" />
<p class="caption"><span class="caption-text">パラメータと目的関数の関係（2次元）</span></p>
</div>
<p>これを3次元でイメージすると，下図のようになります．</p>
<div class="figure" id="id18">
<img alt="パラメータと目的関数の関係（3次元）" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/08.png" />
<p class="caption"><span class="caption-text">パラメータと目的関数の関係（3次元）</span></p>
</div>
<p>そして，目的関数である二乗誤差の総和が最小となる点では各変数で微分した時の傾きが0となります．</p>
<div class="figure" id="id19">
<img alt="目的関数が最小となる点" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/07.png" />
<p class="caption"><span class="caption-text">目的関数が最小となる点</span></p>
</div>
<p>この例では，<span class="math notranslate nohighlight">\(w_{1}\)</span> と <span class="math notranslate nohighlight">\(w_{2}\)</span>
の２つのパラメータの場合で考えましたが，これは <span class="math notranslate nohighlight">\(w_{0}\)</span>,
<span class="math notranslate nohighlight">\(w_{1}\)</span>, <span class="math notranslate nohighlight">\(w_{2}\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span>, <span class="math notranslate nohighlight">\(w_{M}\)</span>
の場合でも同様に考えることができ，目的関数が最小となる点は</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
\dfrac {\partial }{\partial w_{0}}\mathcal{L}=0\\
\dfrac {\partial }{\partial w_{1}}\mathcal{L}=0\\
\ \ \ \ \ \vdots \\
\dfrac {\partial }{\partial w_{M}}\mathcal{L}=0\\
\end{cases}\end{split}\]</div>
<p>となり，これをまとめると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
\dfrac {\partial}{\partial w_{0}} \mathcal{L} \\
\dfrac {\partial}{\partial w_{1}} \mathcal{L} \\
\vdots  \\
\dfrac {\partial}{\partial w_{M}} \mathcal{L} \\
\end{bmatrix}&amp;=\begin{bmatrix}
0 \\
0 \\
\vdots  \\
0 \\
\end{bmatrix} \\
\Rightarrow \dfrac {\partial}{\partial \boldsymbol{w}} \mathcal{L} &amp;= \boldsymbol{0} \\
\end{aligned}\end{split}\]</div>
<p>のようにベクトルでの微分として表されます．あとは，上式を満たすように
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
を決めていきます．まずは<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>を求めやすくなるように，代入と式変形を行います．（下記の計算にはベクトルでの微分をはじめとして，線形代数で学んだ内容を活用しているため，計算途中がわからなくなった場合には，線形代数のパートを確認しながら進めてみてください．）</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial \boldsymbol{w}}\mathcal{L} =\dfrac {\partial }{\partial \boldsymbol{w}}\left( \gamma + \boldsymbol{\beta}^{T}\boldsymbol{w} + \boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}\right)
= \boldsymbol{0}\\
\dfrac {\partial }{\partial \boldsymbol{w}}\left( \gamma\right) +\dfrac {\partial }{\partial \boldsymbol{w}}\left( \boldsymbol{\beta}^{T}\boldsymbol{w}\right) +\dfrac {\partial }{\partial \boldsymbol{w}}\left( \boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}\right)
=\boldsymbol{0}\\
\boldsymbol{0}+\boldsymbol{\beta}+\left( \boldsymbol{A}+\boldsymbol{A}^{T}\right) \boldsymbol{w} =\boldsymbol{0}\\
-2\boldsymbol{X}^{T}\boldsymbol{t}+\left\{ \boldsymbol{X}^{T}\boldsymbol{X} + \left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{T}\right\} \boldsymbol{w}
=\boldsymbol{0}\\
-2\boldsymbol{X}^{T}\boldsymbol{t}+2\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}=\boldsymbol{0}\\
\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}=\boldsymbol{X}^{T}\boldsymbol{t}\\
\end{aligned}\end{split}\]</div>
<p>ここで，<span class="math notranslate nohighlight">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span>に逆行列が存在すると仮定して，両辺に左側から
<span class="math notranslate nohighlight">\(\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\)</span> をかけると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X} \boldsymbol{w} =\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t} \\
\boldsymbol{I}\boldsymbol{w}=\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t} \\
\boldsymbol{w}=\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}
\end{aligned}\end{split}\]</div>
<p>となり，与えられたデータセット <span class="math notranslate nohighlight">\(\boldsymbol{X}, \boldsymbol{t}\)</span>
から，最適なパラメータ <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
が求まりました．ここで，<span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span>
は単位行列です．また，式変形の際には，</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w} = \dfrac{\boldsymbol{X}^{T}\boldsymbol{t}}{\boldsymbol{X}^{T}\boldsymbol{X}}\]</div>
<p>のような分数が表れないように注意してください．これは行列の計算には割り算がないためです．そのため，逆行列を使って行列積のみで計算しています．</p>
<p>また，もうひとつよくある間違いとして，<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>を求めるために以下のような式変形をしてしまう例が挙げられます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}&amp;=\boldsymbol{X}^{T}\boldsymbol{t}\\
\left( \boldsymbol{X}^{T}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}&amp;=\left( \boldsymbol{X}^{T}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}\\
\boldsymbol{X}\boldsymbol{w}&amp;=\boldsymbol{t}\\
\boldsymbol{X}^{-1}\boldsymbol{X}\boldsymbol{w}&amp;=\boldsymbol{X}^{-1}\boldsymbol{t}\\
\boldsymbol{w}&amp;=\boldsymbol{X}^{-1}\boldsymbol{t}
\end{aligned}\end{split}\]</div>
<p>しかし，これは一般的には成立しません．その理由は，逆行列を持つための条件として，<strong>正方行列であること</strong>を満たしていないためです．一般的に，サンプル数
<span class="math notranslate nohighlight">\(N\)</span> と入力変数の数 <span class="math notranslate nohighlight">\(M+1\)</span>
は等しくないため，<span class="math notranslate nohighlight">\(\boldsymbol{X} \in \mathcal{R}^{N \times (M+1)}\)</span>
は正方行列ではなく，逆行列をもちません．それに対し，
<span class="math notranslate nohighlight">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> は
<span class="math notranslate nohighlight">\(\boldsymbol{X}^{T}\boldsymbol{X} \in \mathcal{R}^{(M+1) \times (M+1)}\)</span>
となり，サンプル数 <span class="math notranslate nohighlight">\(N\)</span>
に依存することなく，常に正方行列となります．（逆行列が求まるためにはもう少し厳密な条件がありますが，ここでは説明しません．）</p>
<p>推論の際は学習で得られたパラメータ <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> を用いて，</p>
<div class="math notranslate nohighlight">
\[y_{q} = \boldsymbol{w}^{T}\boldsymbol{x}_{q}\]</div>
<p>のように計算することで予測値が得られます．</p>
</div>
</div>
<div class="section" id="NumPyによる実装">
<h2>2.3. NumPyによる実装<a class="headerlink" href="#NumPyによる実装" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>それでは重回帰分析を例に，Pythonで線形代数を用いた実装を行っていきましょう．Pythonには<strong>NumPy</strong>とよばれる線形代数を簡単に扱えるライブラリが存在し，広く利用されています．次の章で紹介するChainerの中でもNumPyは多用されており，ディープラーニングを学ぶための第一歩として，まずはNumPyの使い方を習得することが重要です．</p>
<p>Pythonの文法に関しては把握していることを前提に進めています．具体的には，変数（数値・文字列，リスト，タプル，辞書），制御構文（for，if），関数，クラスを理解している必要があります．</p>
<p>重回帰分析では，最終的に最適なパラメータ <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> が</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w}=\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}\]</div>
<p>で表されることが分かりました．この最適なパラメータを計算するために，以下の5つを扱います．</p>
<ul class="simple">
<li>ベクトルの定義</li>
<li>行列の定義</li>
<li>転置</li>
<li>行列積</li>
<li>逆行列</li>
</ul>
<p>具体的に，以下のようなデータセットが与えられているケースを想定してみましょう．この例では，データのサンプル数<span class="math notranslate nohighlight">\(N\)</span>は<span class="math notranslate nohighlight">\(4\)</span>であり，入力データ<span class="math notranslate nohighlight">\(X\)</span>の変数の数は<span class="math notranslate nohighlight">\(2\)</span>です．そして<span class="math notranslate nohighlight">\(t\)</span>は教師データとなります．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{X} =
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 5 \\
1 &amp; 3 &amp; 4 \\
1 &amp; 5 &amp; 9
\end{bmatrix}, \
\boldsymbol{t} =
\begin{bmatrix}
1 \\ 5 \\ 6 \\ 8
\end{bmatrix}\end{split}\]</div>
<p>ここで<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>は <strong>パラメータ</strong> <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
<strong>がバイアス</strong> <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> <strong>を包含する</strong>
形式を想定しており，従って入力データ<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>の1列目には<span class="math notranslate nohighlight">\(1\)</span>が格納されています．</p>
<p>それでは実装方法について見ていきましょう．まずは，NumPyの読み込みから始めます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import numpy as np
</pre></div>
</div>
</div>
<p>ベクトルの定義は以下のように行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>t = np.array([1, 5, 6, 8])
</pre></div>
</div>
</div>
<p>ベクトルを表示してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[1 5 6 8]
</pre></div></div>
</div>
<p>行列の定義も行い，表示してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X = np.array([
    [1, 2, 3],
    [1, 2, 5],
    [1, 3, 4],
    [1, 5, 9]
])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 2 3]
 [1 2 5]
 [1 3 4]
 [1 5 9]]
</pre></div></div>
</div>
<p>ここでは<code class="docutils literal notranslate"><span class="pre">np.array</span></code>という関数を用いて，PythonのリストからNumPyの多次元配列の形式(<code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code>)への変換を行っています．</p>
<p>次に，Xの転置を行ってみましょう．<code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code>で定義されている場合，<code class="docutils literal notranslate"><span class="pre">.T</span></code>をつけるだけで転置することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(X.T)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 1 1 1]
 [2 2 3 5]
 [3 5 4 9]]
</pre></div></div>
</div>
<p>縦と横が入れ替わっていることを確認できます．</p>
<p>行列積は以下のように <code class="docutils literal notranslate"><span class="pre">np.dot</span></code>
によって実現できます．行列積を行う際には，一番目の行列の列数と，二番目の行列の行数が同じであることに注意して下さい．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>XX = np.dot(X.T, X)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(XX)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[  4  12  21]
 [ 12  42  73]
 [ 21  73 131]]
</pre></div></div>
</div>
<p>ここからさらに，<span class="math notranslate nohighlight">\(\boldsymbol{X}^{T}\boldsymbol{X}\)</span>に対する逆行列，<span class="math notranslate nohighlight">\(\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\)</span>を計算します．逆行列を求めるには，<code class="docutils literal notranslate"><span class="pre">np.linalg.inv</span></code>
を用います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>XX_inv = np.linalg.inv(XX)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(XX_inv)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[ 1.76530612 -0.39795918 -0.06122449]
 [-0.39795918  0.84693878 -0.40816327]
 [-0.06122449 -0.40816327  0.24489796]]
</pre></div></div>
</div>
<p>これで重回帰分析のために必要な演算が揃いました．</p>
<p>最適なパラメータ<span class="math notranslate nohighlight">\(\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}\)</span>を求めると，</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Xt = np.dot(X.T, t)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(Xt)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[ 20  70 124]
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>w = np.dot(XX_inv, Xt)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(w)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-0.14285714  0.71428571  0.57142857]
</pre></div></div>
</div>
<p>このようにパラメータ <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
が求まりました．NumPyを使うことで，数式をそのままプログラム上で書くことができます．</p>
</div>
<div class="section" id="Scikit-learnによる機械学習アルゴリズムの実行">
<h2>2.4. Scikit-learnによる機械学習アルゴリズムの実行<a class="headerlink" href="#Scikit-learnによる機械学習アルゴリズムの実行" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>重回帰分析であればNumPyで比較的容易に実装することができましたが，実践的に使用する機械学習アルゴリズムの多くは複雑であり，初学者が一から書くには難しい場合も少なくありません．そこで，Pythonでは<strong>Scikit-learn</strong>とよばれる機械学習用のフレームワークが公開されており，初学者でも簡単に様々な機械学習アルゴリズムを扱うことができます．</p>
<p>ここでは重回帰分析の，<strong>Scikit-learnを用いた実装方法</strong>を紹介します．データセットは先程と同様に<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>と<span class="math notranslate nohighlight">\(\boldsymbol{t}\)</span>を使用しますが，Scikit-learnにおいては，
<strong>パラメータ</strong> <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> <strong>がバイアス</strong>
<span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> <strong>を包含しない</strong>
形式を想定しており，入力データ<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>の1列目から<span class="math notranslate nohighlight">\(1\)</span>を取り除くのが一般的です．従って，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{X} =
\begin{bmatrix}
2 &amp; 3 \\
2 &amp; 5 \\
3 &amp; 4 \\
5 &amp; 9
\end{bmatrix}, \
\boldsymbol{t} =
\begin{bmatrix}
1 \\ 5 \\ 6 \\ 8
\end{bmatrix}\end{split}\]</div>
<p>が与えられているとします．</p>
<div class="section" id="Scikit-learn-基礎編">
<h3>2.4.1. Scikit-learn 基礎編<a class="headerlink" href="#Scikit-learn-基礎編" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>Scikit-learnは<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>という名前で呼び出すことができます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import sklearn
</pre></div>
</div>
</div>
<p>重回帰分析を使用する場合は以下のように呼び出します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.linear_model import LinearRegression
</pre></div>
</div>
</div>
<p>なお，使い方を調べる際には，<a class="reference external" href="http://scikit-learn.org/">公式のリファレンス</a>に加えて，実際のコード例を見るのも有用です（例えば検索エンジンで「重回帰分析
Scikit-learn」のようなキーワードで検索すればたくさんのコード例が見つかります）．</p>
<p>重回帰分析のアルゴリズムはクラスとして定義されており，実際のモデルを利用するにはインスタンス化する必要があります．
インスタンス化はクラス名の後に<code class="docutils literal notranslate"><span class="pre">()</span></code>をつければ行えます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model = LinearRegression()
</pre></div>
</div>
</div>
<p>これだけで，重回帰分析を使用するための準備が完了です．このモデルを使って，パラメータの学習は以下のように行います．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X = np.array([
    [2, 3],
    [2, 5],
    [3, 4],
    [5, 9]
])
t = np.array([1, 5, 6, 8])

model.fit(X, t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</pre></div>
</div>
</div>
<p>結果の検証は，次のように行います．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model.score(X, t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.6923076923076923
</pre></div>
</div>
</div>
<p>回帰の場合は，以下の式で示される，<strong>決定係数</strong>とよばれる指標が自動的に計算されるようになっています．</p>
<div class="math notranslate nohighlight">
\[R^{2} = 1 - \dfrac{\sum_{i}\left( t_{i} - y_{i} \right)^{2}}{\sum_{i}\left( t_{i} - \bar{t} \right)^{2}}\]</div>
<p>このように，Scikit-learnでは，簡単なインターフェースでやり取りができるようになっています．Scikit-learnの良い点は，最初にアルゴリズムを決めてしまえば，どのアルゴリズムでも，<code class="docutils literal notranslate"><span class="pre">.fit()</span></code>で学習，<code class="docutils literal notranslate"><span class="pre">.score()</span></code>で検証が行える点です．</p>
<p>また，アルゴリズムによって内容は多少異なりますが，パラメータもインスタンス変数として格納されているため，学習後に確認することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># パラメータw
model.coef_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>array([0.71428571, 0.57142857])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># バイアスb
model.intercept_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>-0.14285714285714501
</pre></div>
</div>
</div>
</div>
<div class="section" id="Scikit-learn-応用編">
<h3>2.4.2. Scikit-learn 応用編<a class="headerlink" href="#Scikit-learn-応用編" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>Scikit-learnは機械学習の実装を支援する多くの機能を兼ね備えています．本節では，サンプルデータセットの使用方法，及びデータセットの分割方法について紹介していきます．</p>
<div class="section" id="サンプルデータセットの使用">
<h4>2.4.2.1. サンプルデータセットの使用<a class="headerlink" href="#サンプルデータセットの使用" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>まずはじめにサンプルデータセットの取り扱いを紹介します．Scikit-learnでは，幾つかのデータセットが提供されています．今回はその中から，米国ボストン市郊外における地域別の物件価格のデータセットを使用することにします．</p>
<p>このデータセットには<span class="math notranslate nohighlight">\(506\)</span>件のデータが登録されており，各サンプルには対象地域の平均物件価格と，それに紐づく情報として対象地域の平均的な物件情報（一戸あたりの部屋数，築年数，雇用施設からの距離など），人口統計情報（低所得者の割合，教師あたりの生徒数など），生活環境に関する情報（犯罪発生率など）などが含まれています．このデータセットを利用する目的は，物件や人口統計などの情報を入力変数として，出力変数である平均物件価格を予測するモデルを構築することです．
入力変数は全部で13種類あり，詳細は以下の通りです．</p>
<ul class="simple">
<li>CRIM : 人口<span class="math notranslate nohighlight">\(1\)</span>人あたりの犯罪発生率</li>
<li>ZN : <span class="math notranslate nohighlight">\(25,000\)</span>平方フィート以上の住宅区画が占める割合</li>
<li>INDUS : 非小売業が占める面積の割合</li>
<li>CHAS : チャールズ川に関するダミー変数 (1 : 川沿い，0 : それ以外)</li>
<li>NOX : 窒素酸化物の濃度</li>
<li>RM : 住居あたりの平均部屋数</li>
<li>AGE : 1940年以前に建てられた物件の割合</li>
<li>DIS : 5つのボストン雇用施設からの重み付き距離</li>
<li>RAD : 都心部の幹線道路へのアクセス指数</li>
<li>TAX : $ <span class="math notranslate nohighlight">\(10,000\)</span>あたりの固定資産税の割合</li>
<li>PTRATIO : 教師1人あたりの生徒数</li>
<li>B : 黒人の比率を表す指数</li>
<li>LSTAT : 低所得者の割合</li>
</ul>
<p>それでは， <code class="docutils literal notranslate"><span class="pre">load_boston()</span></code>
関数を実行して，データセットを読み込んでみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.datasets import load_boston
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>boston = load_boston()
</pre></div>
</div>
</div>
<p>変数の<code class="docutils literal notranslate"><span class="pre">boston</span></code>には辞書型で格納されており，変数の中身を見ながら入力データと出力データに対応するものを見つけていきます．今回は<code class="docutils literal notranslate"><span class="pre">data</span></code>が入力であり，<code class="docutils literal notranslate"><span class="pre">target</span></code>が出力に対応します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X = boston[&#39;data&#39;]
t = boston[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]
 [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]
 [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]
 ...
 [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]
 [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]
 [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4
 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8
 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6
 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4
 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9
 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9
 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7
 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8
 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4
 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8
 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4
 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8
 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2
 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.
 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.
 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1
 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5
 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8
 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8
 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1
 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9
 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2
 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1
 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1
 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6
 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8
 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3
 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2
  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.
 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4
 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3
 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6
 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7
 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3
 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.
  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9
 22.  11.9]
</pre></div></div>
</div>
<p>NumPyの形式で入力データと教師データが格納されており，<code class="docutils literal notranslate"><span class="pre">.shape</span></code>を使うことで行と列の数を確認できます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(506, 13)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>t.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(506,)
</pre></div>
</div>
</div>
<p>入力データの配列<span class="math notranslate nohighlight">\(X\)</span>には，<span class="math notranslate nohighlight">\(506\)</span>件分のデータが格納されています．各サンプルは<span class="math notranslate nohighlight">\(13\)</span>次元ベクトルとして表現されており，これはそれぞれ<span class="math notranslate nohighlight">\(13\)</span>種類の入力変数を表しています．教師データ<span class="math notranslate nohighlight">\(t\)</span>には，入力変数に対応する出力変数として，平均物件価格のスカラー値が格納されています．</p>
</div>
<div class="section" id="データセットの分割">
<h4>2.4.2.2. データセットの分割<a class="headerlink" href="#データセットの分割" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>つぎに，この学習データを <strong>訓練データ</strong> と <strong>テストデータ</strong>
に分割する方法をご紹介します．もし，学習の時に使ったデータを使ってモデルの性能を評価した場合，学習データの性能が高くても学習中に見たことが無い（同じような分布からとられた）未知のデータはうまくいかない場合があります．これを
<strong>過学習</strong>
とよびます．機械学習ではこれを防ぐために学習データと別に性能を評価するテストデータを分けて評価します．このように分割して検証することを
<strong>ホールドアウト法</strong> とよびます．</p>
<p>Scikit-learnでは訓練用とテスト用を分割する機能が用意されています．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import train_test_split
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(354, 13)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_test.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(152, 13)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code>
関数の引数<code class="docutils literal notranslate"><span class="pre">test_size</span></code>は検証用に使うデータの比率であり，<span class="math notranslate nohighlight">\(0.3\)</span>と指定すると全体の<span class="math notranslate nohighlight">\(30\)</span>%がテストデータとなります．また，<code class="docutils literal notranslate"><span class="pre">random_state</span></code>は乱数のシードであり，固定のシード値を与えると，分割の再現性を確保することができます．なぜ乱数が登場するかというと，前から<span class="math notranslate nohighlight">\(70\)</span>%を訓練用，残りをテスト用とするのではなく，全体からランダムに選択した<span class="math notranslate nohighlight">\(70\)</span>%を訓練用，残り<span class="math notranslate nohighlight">\(30\)</span>%をテスト用と選択しているためです．</p>
<p>それでは，訓練データを用いて学習を行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model = LinearRegression()
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</pre></div>
</div>
</div>
<p>検証を行う場合は，訓練データとテストデータの両方に対してチェックしておくと良いでしょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 訓練データ
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.7644563391821222
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># テストデータ
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.673528086534723
</pre></div>
</div>
</div>
<p>テストデータだけでなく，訓練データでも検証することで学習に失敗している場合の問題を切り分けることができます．</p>
<p>モデルが訓練データに対して良い精度で予測できない状態を<strong>アンダーフィッティング</strong>といいます．アンダーフィッティングが起きている場合，現状の機械学習アルゴリズムでうまくデータの特徴を捉えられていないと考えられ，アルゴリズムを変更したり，入力データの特徴をより適切に表現できるような変換を考えたりするなどして改善を試みます．逆に<strong>オーバーフィッティング（過学習）</strong>の場合，アルゴリズムでデータの特徴をある程度捉えられていることは確認できているので，モデルが過学習しないように対策していきます．代表的な方法として，<strong>ハイパーパラメータ</strong>とよばれる各アルゴリズムのパラメータ学習に使われるパラメータ値を調整していくことで解決できる場合があります．このように，望ましい結果が得られない中にも，それぞれの状況を把握することで次に打つべき対策が変わってくるため，訓練データとテストデータの両方に対する検証を行うことは重要であることが分かります．</p>
<p>また，Scikit-learnでは，スケーリングも行うことができます．例えば，平均0，標準偏差1に変換するデータ正規化を行う場合の手順は以下の通りです．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># インスタンス化
scaler = StandardScaler()
</pre></div>
</div>
</div>
<p>訓練データを用いて，平均と分散を計算します．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 平均と分散を計算
scaler.fit(X_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>StandardScaler(copy=True, with_mean=True, with_std=True)
</pre></div>
</div>
</div>
<p>計算された平均，分散を用いて，訓練データ及びテストデータをスケーリングします．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 変換
X_train_s = scaler.transform(X_train)
X_test_s  = scaler.transform(X_test)
</pre></div>
</div>
</div>
<p>テストデータをスケーリングする際にも，訓練データの平均・分散を利用していることに注意しましょう．テストデータはモデルにとっては未知のデータセットであるため，訓練データとテストデータを合わせた全データの平均・分散を利用すると，それは本来知りえないテストデータの情報をモデルに与えてしまうことになります．そこで，モデルが利用可能な訓練データのみを用いてスケーリングが行われます．</p>
<p>訓練データとテストデータでは平均・分散が異なるため，訓練データの平均・分散でスケーリングされたテストデータについては，その平均が<span class="math notranslate nohighlight">\(0\)</span>，分散が<span class="math notranslate nohighlight">\(1\)</span>になるとは限らないことに注意してください．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(X_train_s)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.20416267 -0.49997924  1.54801583 ...  1.2272573   0.42454294
   3.10807269]
 [-0.38584317  0.34677427 -0.58974728 ...  0.05696346  0.40185312
  -0.66643035]
 [-0.33266283 -0.49997924  1.54801583 ...  1.2272573   0.39846135
   0.63936662]
 ...
 [-0.38147768 -0.49997924 -0.15303077 ... -0.30312696  0.39659002
  -0.30284441]
 [-0.3720831  -0.49997924 -0.59690657 ... -0.25811566  0.37588849
   0.89967717]
 [-0.38289844 -0.49997924 -1.00641779 ... -0.84326258  0.42454294
   0.31822262]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(X_test_s)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.39152624 -0.49997924 -1.12239824 ... -0.70822867  0.17086147
  -0.72160487]
 [ 0.70825498 -0.49997924  1.00534187 ...  0.77714428  0.0648977
  -0.41177872]
 [-0.38588517 -0.49997924  0.4025299  ... -0.93328518  0.38758427
  -0.27454978]
 ...
 [ 1.6177735  -0.49997924  1.00534187 ...  0.77714428  0.42454294
   2.59876943]
 [-0.34043865 -0.49997924 -0.1687812  ... -0.03305915  0.42454294
  -1.11772962]
 [-0.39601293 -0.49997924 -1.27417512 ...  0.10197476  0.39202867
  -1.02294263]]
</pre></div></div>
</div>
<p>この他，Scikit-learnはロジスティック回帰やサポートベクターマシン，ランダムフォレストなど様々な機械学習アルゴリズムをサポートしています．</p>
<p>これらについても，重回帰分析と同様にモデルをインスタンス化し，学習データを引数として<code class="docutils literal notranslate"><span class="pre">.fit()</span></code>関数で学習し，<code class="docutils literal notranslate"><span class="pre">.score()</span></code>関数で評価できるようになっています．</p>
<p>より詳しく知りたい方は<a class="reference external" href="https://scikit-learn.org/">Scikit-learn</a>サイトや解説サイトなどを参照してください．</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_Neural_Network.html" class="btn btn-neutral float-right" title="3. ニューラルネットワークの基礎" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Basic_Math_for_ML.html" class="btn btn-neutral" title="1. 機械学習に必要な数学の基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>