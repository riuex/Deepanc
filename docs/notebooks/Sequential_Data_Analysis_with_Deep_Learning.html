

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析 &mdash; メディカルAI専門コース オンライン講義資料  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="prev" title="7. 実践編: ディープラーニングを使った配列解析" href="DNA_Sequence_Data_Analysis.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>

  <meta name="description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:title" content="メディカルAI専門コース オンライン講義資料">
  <meta property="og:description" content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://japan-medical-ai.github.io/medical-ai-course-materials/">
  <meta property="og:image" content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@PreferredNetJP">
  <meta name="twitter:creator" content="@PreferredNetJP">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI専門コース オンライン講義資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 機械学習ライブラリの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. ニューラルネットワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 実践編: ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#環境構築">8.1. 環境構築</a></li>
<li class="toctree-l2"><a class="reference internal" href="#心電図(ECG)と不整脈診断について">8.2. 心電図(ECG)と不整脈診断について</a></li>
<li class="toctree-l2"><a class="reference internal" href="#使用するデータセット">8.3. 使用するデータセット</a></li>
<li class="toctree-l2"><a class="reference internal" href="#データ前処理">8.4. データ前処理</a></li>
<li class="toctree-l2"><a class="reference internal" href="#深層学習を用いた時系列データ解析">8.5. 深層学習を用いた時系列データ解析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#学習">8.5.1. 学習</a></li>
<li class="toctree-l3"><a class="reference internal" href="#評価">8.5.2. 評価</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#精度向上に向けて">8.6. 精度向上に向けて</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#クラス不均衡データへの対応">8.6.1. クラス不均衡データへの対応</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#サンプリング">8.6.1.1. サンプリング</a></li>
<li class="toctree-l4"><a class="reference internal" href="#損失関数の変更">8.6.1.2. 損失関数の変更</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ネットワーク構造の変更">8.6.2. ネットワーク構造の変更</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ノイズ除去の効果検証">8.6.3. ノイズ除去の効果検証</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#おわりに">8.7. おわりに</a></li>
<li class="toctree-l2"><a class="reference internal" href="#参考文献">8.8. 参考文献</a></li>
</ul>
</li>
</ul>

            
          
          <div style="padding-right:20px; bottom:10px;">
            <a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
              <img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png" />
              <p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
            </a>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Sequential_Data_Analysis_with_Deep_Learning.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Sequential_Data_Analysis_with_Deep_Learning.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="実践編:-ディープラーニングを使ったモニタリングデータの時系列解析">
<h1>8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析<a class="headerlink" href="#実践編:-ディープラーニングを使ったモニタリングデータの時系列解析" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>健康意識の高まりや運動人口の増加に伴って，活動量計などのウェアラブルデバイスが普及し始めています．センサーデバイスから心拍数などの情報を取得することで，リアルタイムに健康状態をモニタリングできる可能性があることから，近年ではヘルスケア領域での活用事例も増えてきています．2018年2月には，Cardiogram社とカリフォルニア大学が共同研究の成果を発表し，心拍数データに対してDeep
Learningを適用することで，高精度に糖尿病予備群を予測可能であることを報告し，大きな注目を集めました．また，Apple
Watch Series
4には心電図作成の機能が搭載されるなど，センサーデバイスも進歩を続け，より精緻な情報が取得できるようになってきています．こうした背景において，モニタリングデータを収集・解析し，健康管理に繋げていく取り組みは今後益々盛んになっていくものと考えられます．</p>
<p>本章では，心電図の信号波形データを対象として，不整脈を検出する問題に取り組みます．</p>
<div class="section" id="環境構築">
<h2>8.1. 環境構築<a class="headerlink" href="#環境構築" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>本章では, 下記のライブラリを利用します.</p>
<ul class="simple">
<li>Cupy</li>
<li>Chainer</li>
<li>Scipy</li>
<li>Matplotlib</li>
<li>Seaborn</li>
<li>Pandas</li>
<li>WFDB</li>
<li>Scikit-learn</li>
<li>Imbalanced-learn</li>
</ul>
<p>以下のセルを実行 (Shift + Enter)
して，必要なものをインストールして下さい.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!apt -y -q install tree
!pip install wfdb==2.2.1 imbalanced-learn==0.4.3
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!curl https://colab.chainer.org/install | sh -
</pre></div>
</div>
</div>
<p>インストールが完了したら以下のセルを実行して，各ライブラリのインポート，及びバージョン確認を行って下さい.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import os
import random
import numpy as np
import chainer
import scipy
import pandas as pd
import matplotlib
import seaborn as sn
import wfdb
import sklearn
import imblearn

chainer.print_runtime_info()
print(&quot;Scipy: &quot;, scipy.__version__)
print(&quot;Pandas: &quot;, pd.__version__)
print(&quot;Matplotlib: &quot;, matplotlib.__version__)
print(&quot;Seaborn: &quot;, sn.__version__)
print(&quot;WFDB: &quot;, wfdb.__version__)
print(&quot;Scikit-learn: &quot;, sklearn.__version__)
print(&quot;Imbalanced-learn: &quot;, imblearn.__version__)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform: Linux-4.14.65+-x86_64-with-Ubuntu-18.04-bionic
Chainer: 5.1.0
NumPy: 1.14.6
CuPy:
  CuPy Version          : 5.1.0
  CUDA Root             : /usr/local/cuda
  CUDA Build Version    : 9020
  CUDA Driver Version   : 9020
  CUDA Runtime Version  : 9020
  cuDNN Build Version   : 7301
  cuDNN Version         : 7301
  NCCL Build Version    : 2307
iDeep: 2.0.0.post3
Scipy:  1.1.0
Pandas:  0.22.0
Matplotlib:  2.1.2
Seaborn:  0.7.1
WFDB:  2.2.1
Scikit-learn:  0.20.1
Imbalanced-learn:  0.4.3
</pre></div></div>
</div>
<p>また，本章の実行結果を再現可能なように，4章 (4.2.4.4)
で紹介した乱数シードの固定を行います．</p>
<p>（以降の計算を行う上で必ず必要となる設定ではありません．）</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def reset_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    if chainer.cuda.available:
        chainer.cuda.cupy.random.seed(seed)

reset_seed(42)
</pre></div>
</div>
</div>
</div>
<div class="section" id="心電図(ECG)と不整脈診断について">
<h2>8.2. 心電図(ECG)と不整脈診断について<a class="headerlink" href="#心電図(ECG)と不整脈診断について" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p><strong>心電図(Electrocardiogram,
ECG)</strong>は，心筋を協調的，律動的に動かすために刺激伝導系で伝達される電気信号を体表から記録したものであり，心電図検査は不整脈や虚血性心疾患の診断において広く用いられる検査です[<a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">文献1</a>,
<a class="reference external" href="https://www.ningen-dock.jp/wp/wp-content/uploads/2013/09/d4bb55fcf01494e251d315b76738ab40.pdf">文献2</a>]．</p>
<p>標準的な心電図は，手足からとる心電図（四肢誘導）として，双極誘導（<span class="math notranslate nohighlight">\(Ⅰ\)</span>，<span class="math notranslate nohighlight">\(Ⅱ\)</span>，<span class="math notranslate nohighlight">\(Ⅲ\)</span>），及び単極誘導（<span class="math notranslate nohighlight">\(aV_R\)</span>，<span class="math notranslate nohighlight">\(aV_L\)</span>，<span class="math notranslate nohighlight">\(aV_F\)</span>）の6誘導，胸部からとる心電図（胸部誘導）として，<span class="math notranslate nohighlight">\(V_1\)</span>，<span class="math notranslate nohighlight">\(V_2\)</span>，<span class="math notranslate nohighlight">\(V_3\)</span>，<span class="math notranslate nohighlight">\(V_4\)</span>，<span class="math notranslate nohighlight">\(V_5\)</span>，<span class="math notranslate nohighlight">\(V_6\)</span>の6誘導，計12誘導から成ります．このうち，特に不整脈のスクリーニングを行う際には，<span class="math notranslate nohighlight">\(Ⅱ\)</span>誘導と<span class="math notranslate nohighlight">\(V_1\)</span>誘導に注目して診断が行われるのが一般的とされています．</p>
<p>心臓が正常な状態では，ECGにおいては規則的な波形が観測され，これを<strong>正常洞調律
(Normal sinus rhythm, NSR)</strong>といいます．</p>
<p>具体的には，以下の3つの主要な波形で構成されており，</p>
<ol class="arabic simple">
<li><strong>P波</strong>：心房の脱分極（心房の興奮）</li>
<li><strong>QRS波</strong>：心室の脱分極（心室の興奮）</li>
<li><strong>T波</strong>：心室の再分極（心室興奮の収まり）</li>
</ol>
<p>の順番で，下図のような波形が観測されます．</p>
<div class="figure" id="id16">
<img alt="正常心電図の概略図" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/monitoring/sinus_rhythm.png" />
<p class="caption"><span class="caption-text">正常心電図の概略図</span></p>
</div>
<p>([<a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">文献1</a>]より引用)</p>
<p>こうした規則的な波形に乱れが生じ，調律に異常があると判断された場合，不整脈などの疑いがあるため，診断が行われることになります．</p>
</div>
<div class="section" id="使用するデータセット">
<h2>8.3. 使用するデータセット<a class="headerlink" href="#使用するデータセット" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここでは，ECGの公開データとして有名な<a class="reference external" href="https://www.physionet.org/physiobank/database/mitdb/">MIT-BIH Arrhythmia Database
(mitdb)</a>を使用します．</p>
<p>47名の患者から収集した48レコードが登録されており，各レコードファイルには約30分間の2誘導(<span class="math notranslate nohighlight">\(II\)</span>，<span class="math notranslate nohighlight">\(V_1\)</span>)のシグナルデータが格納されています．また，各R波のピーク位置に対してアノテーションが付与されています．(データとアノテーションの詳細については<a class="reference external" href="https://www.physionet.org/physiobank/database/html/mitdbdir/intro.htm">こちら</a>を御覧ください．)</p>
<p>データベースは<a class="reference external" href="https://www.physionet.org/">PhysioNet</a>によって管理されており，ダウンロードや読み込み用のPythonパッケージが提供されているので，今回はそちらを利用してデータを入手します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>dataset_root = &#39;./dataset&#39;
download_dir = os.path.join(dataset_root, &#39;download&#39;)
</pre></div>
</div>
</div>
<p>まずはmitdbデータベースをダウンロードしましょう．
※実行時にエラーが出た場合は，再度実行して下さい．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>wfdb.dl_database(&#39;mitdb&#39;, dl_dir=download_dir)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Created local base download directory: ./dataset/download
Downloading files...
Finished downloading files
</pre></div></div>
</div>
<p>無事ダウンロードが完了すると， <code class="docutils literal notranslate"><span class="pre">Finished</span> <span class="pre">downloading</span> <span class="pre">files</span></code>
というメッセージが表示されます．</p>
<p>ファイル一覧を確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(sorted(os.listdir(download_dir)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;100.atr&#39;, &#39;100.dat&#39;, &#39;100.hea&#39;, &#39;101.atr&#39;, &#39;101.dat&#39;, &#39;101.hea&#39;, &#39;102.atr&#39;, &#39;102.dat&#39;, &#39;102.hea&#39;, &#39;103.atr&#39;, &#39;103.dat&#39;, &#39;103.hea&#39;, &#39;104.atr&#39;, &#39;104.dat&#39;, &#39;104.hea&#39;, &#39;105.atr&#39;, &#39;105.dat&#39;, &#39;105.hea&#39;, &#39;106.atr&#39;, &#39;106.dat&#39;, &#39;106.hea&#39;, &#39;107.atr&#39;, &#39;107.dat&#39;, &#39;107.hea&#39;, &#39;108.atr&#39;, &#39;108.dat&#39;, &#39;108.hea&#39;, &#39;109.atr&#39;, &#39;109.dat&#39;, &#39;109.hea&#39;, &#39;111.atr&#39;, &#39;111.dat&#39;, &#39;111.hea&#39;, &#39;112.atr&#39;, &#39;112.dat&#39;, &#39;112.hea&#39;, &#39;113.atr&#39;, &#39;113.dat&#39;, &#39;113.hea&#39;, &#39;114.atr&#39;, &#39;114.dat&#39;, &#39;114.hea&#39;, &#39;115.atr&#39;, &#39;115.dat&#39;, &#39;115.hea&#39;, &#39;116.atr&#39;, &#39;116.dat&#39;, &#39;116.hea&#39;, &#39;117.atr&#39;, &#39;117.dat&#39;, &#39;117.hea&#39;, &#39;118.atr&#39;, &#39;118.dat&#39;, &#39;118.hea&#39;, &#39;119.atr&#39;, &#39;119.dat&#39;, &#39;119.hea&#39;, &#39;121.atr&#39;, &#39;121.dat&#39;, &#39;121.hea&#39;, &#39;122.atr&#39;, &#39;122.dat&#39;, &#39;122.hea&#39;, &#39;123.atr&#39;, &#39;123.dat&#39;, &#39;123.hea&#39;, &#39;124.atr&#39;, &#39;124.dat&#39;, &#39;124.hea&#39;, &#39;200.atr&#39;, &#39;200.dat&#39;, &#39;200.hea&#39;, &#39;201.atr&#39;, &#39;201.dat&#39;, &#39;201.hea&#39;, &#39;202.atr&#39;, &#39;202.dat&#39;, &#39;202.hea&#39;, &#39;203.atr&#39;, &#39;203.dat&#39;, &#39;203.hea&#39;, &#39;205.atr&#39;, &#39;205.dat&#39;, &#39;205.hea&#39;, &#39;207.atr&#39;, &#39;207.dat&#39;, &#39;207.hea&#39;, &#39;208.atr&#39;, &#39;208.dat&#39;, &#39;208.hea&#39;, &#39;209.atr&#39;, &#39;209.dat&#39;, &#39;209.hea&#39;, &#39;210.atr&#39;, &#39;210.dat&#39;, &#39;210.hea&#39;, &#39;212.atr&#39;, &#39;212.dat&#39;, &#39;212.hea&#39;, &#39;213.atr&#39;, &#39;213.dat&#39;, &#39;213.hea&#39;, &#39;214.atr&#39;, &#39;214.dat&#39;, &#39;214.hea&#39;, &#39;215.atr&#39;, &#39;215.dat&#39;, &#39;215.hea&#39;, &#39;217.atr&#39;, &#39;217.dat&#39;, &#39;217.hea&#39;, &#39;219.atr&#39;, &#39;219.dat&#39;, &#39;219.hea&#39;, &#39;220.atr&#39;, &#39;220.dat&#39;, &#39;220.hea&#39;, &#39;221.atr&#39;, &#39;221.dat&#39;, &#39;221.hea&#39;, &#39;222.atr&#39;, &#39;222.dat&#39;, &#39;222.hea&#39;, &#39;223.atr&#39;, &#39;223.dat&#39;, &#39;223.hea&#39;, &#39;228.atr&#39;, &#39;228.dat&#39;, &#39;228.hea&#39;, &#39;230.atr&#39;, &#39;230.dat&#39;, &#39;230.hea&#39;, &#39;231.atr&#39;, &#39;231.dat&#39;, &#39;231.hea&#39;, &#39;232.atr&#39;, &#39;232.dat&#39;, &#39;232.hea&#39;, &#39;233.atr&#39;, &#39;233.dat&#39;, &#39;233.hea&#39;, &#39;234.atr&#39;, &#39;234.dat&#39;, &#39;234.hea&#39;]
</pre></div></div>
</div>
<p>ファイル名の数字はレコードIDを表しています．各レコードには3種類のファイルがあり，</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">.dat</span></code> : シグナル（バイナリ形式）</li>
<li><code class="docutils literal notranslate"><span class="pre">.atr</span></code> : アノテーション（バイナリ形式）</li>
<li><code class="docutils literal notranslate"><span class="pre">.hea</span></code> : ヘッダ（バイナリファイルの読み込みに必要）</li>
</ul>
<p>となっています．</p>
</div>
<div class="section" id="データ前処理">
<h2>8.4. データ前処理<a class="headerlink" href="#データ前処理" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ダウンロードしたファイルを読み込み，機械学習モデルへの入力形式に変換する<strong>データ前処理</strong>について説明します．</p>
<p>本節では，以下の手順で前処理を行います．</p>
<ol class="arabic simple">
<li>レコードIDを事前に 学習用 / 評価用 に分割<ul>
<li>48レコードのうち，<ul>
<li>ID =（102, 104, 107,
217）のシグナルはペースメーカーの拍動が含まれるため除外します．</li>
<li>ID =
114のシグナルは波形の一部が反転しているため，今回は除外します．</li>
<li>ID = （201,
202）は同一の患者から得られたデータのため，202を除外します．</li>
</ul>
</li>
<li>上記を除く計42レコードを，学習用とテスト用に分割します（分割方法は[<a class="reference external" href="https://ieeexplore.ieee.org/document/1306572">文献3</a>]を参考）．</li>
</ul>
</li>
<li>シグナルファイル (.dat) の読み込み<ul>
<li><span class="math notranslate nohighlight">\(Ⅱ\)</span>誘導シグナルと<span class="math notranslate nohighlight">\(V_1\)</span>誘導シグナルが格納されていますが，今回は<span class="math notranslate nohighlight">\(Ⅱ\)</span>誘導のみ利用します．</li>
<li>サンプリング周波数は360 Hz
なので，1秒間に360回のペースで数値が記録されていることになります．</li>
</ul>
</li>
<li>アノテーションファイル (.atr) の読み込み<ul>
<li>各R波ピークの位置 (positions) と，そのラベル (symbols)
を取得します．</li>
</ul>
</li>
<li>シグナルの正規化<ul>
<li>平均0，分散1になるように変換を行います．</li>
</ul>
</li>
<li>シグナルの分割 (segmentation)<ul>
<li>各R波ピークを中心として2秒間(前後1秒ずつ)の断片を切り出していきます．</li>
</ul>
</li>
<li>分割シグナルへのラベル付与<ul>
<li>各R波ピークに付与されているラベルを，下表(※)に従って集約し，今回の解析では正常拍動
(Normal)，及び心室異所性拍動 (VEB)
に対応するラベルが付与されている分割シグナルのみ学習・評価に利用します．</li>
</ul>
</li>
</ol>
<p>※ Association for the Advancement of Medical Instrumentation
(AAMI)が推奨している基準([<a class="reference external" href="https://ieeexplore.ieee.org/document/1306572">文献3</a>])で，5種類に大別して整理されています．</p>
<div class="figure" id="id17">
<img alt="AAMIの分類基準" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/monitoring/aami_standard.png" />
<p class="caption"><span class="caption-text">AAMIの分類基準</span></p>
</div>
<p>([<a class="reference external" href="https://arxiv.org/abs/1810.04121">文献4</a>]より引用)</p>
<p>まずは以下のセルを実行して，データ前処理クラスを定義しましょう．</p>
<p>前処理クラス内では，以下のメンバ関数を定義しています．</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (コンストラクタ) :
変数の初期化，学習用とテスト用への分割ルール，利用するラベルの集約ルール</li>
<li><code class="docutils literal notranslate"><span class="pre">_load_data()</span></code> : シグナル，及びアノテーションの読み込み</li>
<li><code class="docutils literal notranslate"><span class="pre">_normalize_signal()</span></code> :
<code class="docutils literal notranslate"><span class="pre">method</span></code>オプションに応じてシグナルをスケーリング</li>
<li><code class="docutils literal notranslate"><span class="pre">_segment_data()</span></code> :
読み込んだシグナルとアノテーションを，一定幅(<code class="docutils literal notranslate"><span class="pre">window_size</span></code>)で切り出し</li>
<li><code class="docutils literal notranslate"><span class="pre">preprocess_dataset()</span></code> : 学習データ，テストデータを作成</li>
<li><code class="docutils literal notranslate"><span class="pre">_preprocess_dataset_core()</span></code> :
<code class="docutils literal notranslate"><span class="pre">preprocess_datataset()</span></code>内で呼ばれるメインの処理．</li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class BaseECGDatasetPreprocessor(object):

    def __init__(
            self,
            dataset_root,
            window_size=720,  # 2 seconds
    ):
        self.dataset_root = dataset_root
        self.download_dir = os.path.join(self.dataset_root, &#39;download&#39;)
        self.window_size = window_size
        self.sample_rate = 360.
        # split list
        self.train_record_list = [
            &#39;101&#39;, &#39;106&#39;, &#39;108&#39;, &#39;109&#39;, &#39;112&#39;, &#39;115&#39;, &#39;116&#39;, &#39;118&#39;, &#39;119&#39;, &#39;122&#39;,
            &#39;124&#39;, &#39;201&#39;, &#39;203&#39;, &#39;205&#39;, &#39;207&#39;, &#39;208&#39;, &#39;209&#39;, &#39;215&#39;, &#39;220&#39;, &#39;223&#39;, &#39;230&#39;
        ]
        self.test_record_list = [
            &#39;100&#39;, &#39;103&#39;, &#39;105&#39;, &#39;111&#39;, &#39;113&#39;, &#39;117&#39;, &#39;121&#39;, &#39;123&#39;, &#39;200&#39;, &#39;210&#39;,
            &#39;212&#39;, &#39;213&#39;, &#39;214&#39;, &#39;219&#39;, &#39;221&#39;, &#39;222&#39;, &#39;228&#39;, &#39;231&#39;, &#39;232&#39;, &#39;233&#39;, &#39;234&#39;
        ]
        # annotation
        self.labels = [&#39;N&#39;, &#39;V&#39;]
        self.valid_symbols = [&#39;N&#39;, &#39;L&#39;, &#39;R&#39;, &#39;e&#39;, &#39;j&#39;, &#39;V&#39;, &#39;E&#39;]
        self.label_map = {
            &#39;N&#39;: &#39;N&#39;, &#39;L&#39;: &#39;N&#39;, &#39;R&#39;: &#39;N&#39;, &#39;e&#39;: &#39;N&#39;, &#39;j&#39;: &#39;N&#39;,
            &#39;V&#39;: &#39;V&#39;, &#39;E&#39;: &#39;V&#39;
        }

    def _load_data(
            self,
            base_record,
            channel=0  # [0, 1]
    ):
        record_name = os.path.join(self.download_dir, str(base_record))
        # read dat file
        signals, fields = wfdb.rdsamp(record_name)
        assert fields[&#39;fs&#39;] == self.sample_rate
        # read annotation file
        annotation = wfdb.rdann(record_name, &#39;atr&#39;)
        symbols = annotation.symbol
        positions = annotation.sample
        return signals[:, channel], symbols, positions

    def _normalize_signal(
            self,
            signal,
            method=&#39;std&#39;
    ):
        if method == &#39;minmax&#39;:
            # Min-Max scaling
            min_val = np.min(signal)
            max_val = np.max(signal)
            return (signal - min_val) / (max_val - min_val)
        elif method == &#39;std&#39;:
            # Zero mean and unit variance
            signal = (signal - np.mean(signal)) / np.std(signal)
            return signal
        else:
            raise ValueError(&quot;Invalid method: {}&quot;.format(method))

    def _segment_data(
            self,
            signal,
            symbols,
            positions
    ):
        X = []
        y = []
        sig_len = len(signal)
        for i in range(len(symbols)):
            start = positions[i] - self.window_size // 2
            end = positions[i] + self.window_size // 2
            if symbols[i] in self.valid_symbols and start &gt;= 0 and end &lt;= sig_len:
                segment = signal[start:end]
                assert len(segment) == self.window_size, &quot;Invalid length&quot;
                X.append(segment)
                y.append(self.labels.index(self.label_map[symbols[i]]))
        return np.array(X), np.array(y)

    def preprocess_dataset(
            self,
            normalize=True
    ):
        # preprocess training dataset
        self._preprocess_dataset_core(self.train_record_list, &quot;train&quot;, normalize)
        # preprocess test dataset
        self._preprocess_dataset_core(self.test_record_list, &quot;test&quot;, normalize)

    def _preprocess_dataset_core(
            self,
            record_list,
            mode=&quot;train&quot;,
            normalize=True
    ):
        Xs, ys = [], []
        save_dir = os.path.join(self.dataset_root, &#39;preprocessed&#39;, mode)
        for i in range(len(record_list)):
            signal, symbols, positions = self._load_data(record_list[i])
            if normalize:
                signal = self._normalize_signal(signal)
            X, y = self._segment_data(signal, symbols, positions)
            Xs.append(X)
            ys.append(y)
        os.makedirs(save_dir, exist_ok=True)
        np.save(os.path.join(save_dir, &quot;X.npy&quot;), np.vstack(Xs))
        np.save(os.path.join(save_dir, &quot;y.npy&quot;), np.concatenate(ys))

</pre></div>
</div>
</div>
<p>データ保存先のrootディレクトリ(dataset_root)を指定し，
<code class="docutils literal notranslate"><span class="pre">preprocess_dataset()</span></code> を実行することで，前処理後のデータがNumpy
Array形式で所定の場所に保存されます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>BaseECGDatasetPreprocessor(dataset_root).preprocess_dataset()
</pre></div>
</div>
</div>
<p>実行後，以下のファイルが保存されていることを確認しましょう．</p>
<ul class="simple">
<li>train/X.npy : 学習用シグナル</li>
<li>train/y.npy : 学習用ラベル</li>
<li>test/X.npy : 評価用シグナル</li>
<li>test/y.npy : 評価用ラベル</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!tree ./dataset/preprocessed
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
./dataset/preprocessed
├── test
│   ├── X.npy
│   └── y.npy
└── train
    ├── X.npy
    └── y.npy

2 directories, 4 files
</pre></div></div>
</div>
<p>次に，保存したファイルを読み込み，中身を確認してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;train&#39;, &#39;X.npy&#39;))
y_train = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;train&#39;, &#39;y.npy&#39;))
X_test = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;test&#39;, &#39;X.npy&#39;))
y_test = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;test&#39;, &#39;y.npy&#39;))
</pre></div>
</div>
</div>
<p>データセットのサンプル数はそれぞれ以下の通りです．</p>
<ul class="simple">
<li>学習用 : 47738個</li>
<li>評価用 : 45349個</li>
</ul>
<p>各シグナルデータは，2 (sec) * 360 (Hz) =
720次元ベクトルとして表現されています．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;X_train.shape = &quot;, X_train.shape, &quot; \t y_train.shape = &quot;, y_train.shape)
print(&quot;X_test.shape = &quot;, X_test.shape, &quot; \t y_test.shape = &quot;, y_test.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train.shape =  (47738, 720)    y_train.shape =  (47738,)
X_test.shape =  (45349, 720)     y_test.shape =  (45349,)
</pre></div></div>
</div>
<p>各ラベルはインデックスで表現されており，</p>
<ul class="simple">
<li>0 : 正常拍動 (Normal)</li>
<li>1 : 心室異所性拍動 (VEB)</li>
</ul>
<p>となっています．</p>
<p>学習用データセットに含まれている各ラベル毎のサンプル数をカウントしてみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>uniq_train, counts_train = np.unique(y_train, return_counts=True)
print(&quot;y_train count each labels: &quot;, dict(zip(uniq_train, counts_train)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y_train count each labels:  {0: 43995, 1: 3743}
</pre></div></div>
</div>
<p>評価用データについても同様にラベル毎のサンプル数をカウントします．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>uniq_test, counts_test = np.unique(y_test, return_counts=True)
print(&quot;y_test count each labels: &quot;, dict(zip(uniq_test, counts_test)))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
y_test count each labels:  {0: 42149, 1: 3200}
</pre></div></div>
</div>
<p>学習用データ，評価用データ共に，VEBサンプルは全体の10%未満であり，大多数は正常拍動サンプルであることが分かります．</p>
<p>次に，正常拍動，及びVEBのシグナルデータを可視化してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%matplotlib inline
import matplotlib.pyplot as plt
</pre></div>
</div>
</div>
<p>正常拍動の例を図示したものが以下になります．</p>
<p>P波 - QRS波 - T波が規則的に出現していることが確認できます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>idx_n = np.where(y_train == 0)[0]
plt.plot(X_train[idx_n[0]])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[&lt;matplotlib.lines.Line2D at 0x7f04055b3320&gt;]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_42_1.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_42_1.png" />
</div>
</div>
<p>一方でVEBの波形は規則性が乱れ，R波ピークの形状やピーク間距離も正常例とは異なる性質を示していることが分かります．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>idx_s = np.where(y_train == 1)[0]
plt.plot(X_train[idx_s[0]])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[&lt;matplotlib.lines.Line2D at 0x7f040354c4a8&gt;]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_44_1.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_44_1.png" />
</div>
</div>
<p>本章の目的は，ECGシグナル特徴をうまく捉え，新たな波形サンプルに対しても高精度に正常/異常を予測するモデルを構築することです．</p>
<p>次節では，深層学習を利用したモデル構築について説明していきます．</p>
</div>
<div class="section" id="深層学習を用いた時系列データ解析">
<h2>8.5. 深層学習を用いた時系列データ解析<a class="headerlink" href="#深層学習を用いた時系列データ解析" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="section" id="学習">
<h3>8.5.1. 学習<a class="headerlink" href="#学習" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>まず，前節で準備した前処理済みデータをChainerで読み込むためのデータセットクラスを定義します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class ECGDataset(chainer.dataset.DatasetMixin):

    def __init__(
            self,
            path
    ):
        if os.path.isfile(os.path.join(path, &#39;X.npy&#39;)):
            self.X = np.load(os.path.join(path, &#39;X.npy&#39;))
        else:
            raise FileNotFoundError(&quot;{}/X.npy not found.&quot;.format(path))
        if os.path.isfile(os.path.join(path, &#39;y.npy&#39;)):
            self.y = np.load(os.path.join(path, &#39;y.npy&#39;))
        else:
            raise FileNotFoundError(&quot;{}/y.npy not found.&quot;.format(path))

    def __len__(self):
        return len(self.X)

    def get_example(self, i):
        return self.X[None, i].astype(np.float32), self.y[i]

</pre></div>
</div>
</div>
<p>続いて，学習（＋予測）に利用するネットワーク構造を定義します．</p>
<p>今回は，画像認識タスクで有名な，CNNベースの<strong>ResNet34</strong>と同様のネットワーク構造を利用します．[<a class="reference external" href="https://arxiv.org/abs/1512.03385">文献5</a>]</p>
<p>ただし，入力シグナルは1次元配列であることから，ここでは画像解析等で一般的に利用される2D
Convolutionではなく，前章の遺伝子解析と同様，1D
Convolutionを利用します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import chainer.functions as F
import chainer.links as L
from chainer import reporter
from chainer import Variable


class BaseBlock(chainer.Chain):

    def __init__(
            self,
            channels,
            stride=1,
            dilate=1
    ):
        self.stride = stride
        super(BaseBlock, self).__init__()
        with self.init_scope():
            self.c1 = L.ConvolutionND(1, None, channels, 3, stride, dilate, dilate=dilate)
            self.c2 = L.ConvolutionND(1, None, channels, 3, 1, dilate, dilate=dilate)
            if stride &gt; 1:
                self.cd = L.ConvolutionND(1, None, channels, 1, stride, 0)
            self.b1 = L.BatchNormalization(channels)
            self.b2 = L.BatchNormalization(channels)

    def __call__(self, x):
        h = F.relu(self.b1(self.c1(x)))
        if self.stride &gt; 1:
            res = self.cd(x)
        else:
            res = x
        h = res + self.b2(self.c2(h))
        return F.relu(h)


class ResBlock(chainer.Chain):

    def __init__(
            self,
            channels,
            n_block,
            dilate=1
    ):
        self.n_block = n_block
        super(ResBlock, self).__init__()
        with self.init_scope():
            self.b0 = BaseBlock(channels, 2, dilate)
            for i in range(1, n_block):
                bx = BaseBlock(channels, 1, dilate)
                setattr(self, &#39;b{}&#39;.format(str(i)), bx)

    def __call__(self, x):
        h = self.b0(x)
        for i in range(1, self.n_block):
            h = getattr(self, &#39;b{}&#39;.format(str(i)))(h)
        return h


class ResNet34(chainer.Chain):

    def __init__(self):
        super(ResNet34, self).__init__()
        with self.init_scope():
            self.conv1 = L.ConvolutionND(1, None, 64, 7, 2, 3)
            self.bn1 = L.BatchNormalization(64)
            self.resblock0 = ResBlock(64, 3)
            self.resblock1 = ResBlock(128, 4)
            self.resblock2 = ResBlock(256, 6)
            self.resblock3 = ResBlock(512, 3)
            self.fc = L.Linear(None, 2)

    def __call__(self, x):
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.max_pooling_nd(h, 3, 2)
        for i in range(4):
            h = getattr(self, &#39;resblock{}&#39;.format(str(i)))(h)
        h = F.average(h, axis=2)
        h = self.fc(h)
        return h


class Classifier(chainer.Chain):

    def __init__(
            self,
            predictor,
            lossfun=F.softmax_cross_entropy
    ):
        super(Classifier, self).__init__()
        with self.init_scope():
            self.predictor = predictor
            self.lossfun = lossfun

    def __call__(self, *args):
        assert len(args) &gt;= 2
        x = args[:-1]
        t = args[-1]
        y = self.predictor(*x)

        # loss
        loss = self.lossfun(y, t)
        with chainer.no_backprop_mode():
            # other metrics
            accuracy = F.accuracy(y, t)
        # reporter
        reporter.report({&#39;loss&#39;: loss}, self)
        reporter.report({&#39;accuracy&#39;: accuracy}, self)

        return loss

    def predict(self, x):
        with chainer.function.no_backprop_mode(), chainer.using_config(&#39;train&#39;, False):
            x = Variable(self.xp.asarray(x, dtype=self.xp.float32))
            y = self.predictor(x)
            return y
</pre></div>
</div>
</div>
<p>学習を実行するための準備として，以下の関数を用意します．</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">create_train_dataset()</span></code>：学習用データセットを <code class="docutils literal notranslate"><span class="pre">ECGDataset</span></code>
クラスに渡す</li>
<li><code class="docutils literal notranslate"><span class="pre">create_trainer()</span></code>：学習に必要な設定を行い，Trainerオブジェクトを作成</li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer import optimizers
from chainer.optimizer import WeightDecay
from chainer.iterators import MultiprocessIterator
from chainer import training
from chainer.training import extensions
from chainer.training import triggers
from chainer.backends.cuda import get_device_from_id


def create_train_dataset(root_path):
    train_path = os.path.join(root_path, &#39;preprocessed&#39;, &#39;train&#39;)
    train_dataset = ECGDataset(train_path)

    return train_dataset


def create_trainer(
    batchsize, train_dataset, nb_epoch=1,
    device=0, lossfun=F.softmax_cross_entropy
):
    # setup model
    model = ResNet34()
    train_model = Classifier(model, lossfun=lossfun)

    # use Adam optimizer
    optimizer = optimizers.Adam(alpha=0.001)
    optimizer.setup(train_model)
    optimizer.add_hook(WeightDecay(0.0001))

    # setup iterator
    train_iter = MultiprocessIterator(train_dataset, batchsize)

    # define updater
    updater = training.StandardUpdater(train_iter, optimizer, device=device)

    # setup trainer
    stop_trigger = (nb_epoch, &#39;epoch&#39;)
    trainer = training.trainer.Trainer(updater, stop_trigger)
    logging_attributes = [
        &#39;epoch&#39;, &#39;iteration&#39;,
        &#39;main/loss&#39;, &#39;main/accuracy&#39;
    ]
    trainer.extend(
        extensions.LogReport(logging_attributes, trigger=(2000 // batchsize, &#39;iteration&#39;))
    )
    trainer.extend(
        extensions.PrintReport(logging_attributes)
    )
    trainer.extend(
        extensions.ExponentialShift(&#39;alpha&#39;, 0.75, optimizer=optimizer),
        trigger=(4000 // batchsize, &#39;iteration&#39;)
    )

    return trainer
</pre></div>
</div>
</div>
<p>これで学習の準備が整ったので，関数を呼び出してtrainerを作成します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0)
</pre></div>
</div>
</div>
<p>それでは学習を開始しましょう. (1分30秒程度で学習が完了します．)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.1307      0.868304
0           14          0.273237    0.923549
0           21          0.0950281   0.96596
0           28          0.058416    0.982701
0           35          0.0658751   0.982143
0           42          0.0506687   0.985491
0           49          0.057125    0.986607
0           56          0.063658    0.986607
0           63          0.0600915   0.981027
0           70          0.0391555   0.988281
0           77          0.0325103   0.991629
0           84          0.0344455   0.987723
0           91          0.0281526   0.989955
0           98          0.0266191   0.991629
0           105         0.0318078   0.990513
0           112         0.0304052   0.991071
0           119         0.0293185   0.993304
0           126         0.0290823   0.989397
0           133         0.019204    0.996094
0           140         0.0177221   0.994978
0           147         0.0218593   0.990513
0           154         0.019589    0.994978
0           161         0.0257332   0.991629
0           168         0.0155559   0.99442
0           175         0.0161097   0.99442
0           182         0.0212924   0.992746
CPU times: user 1min 12s, sys: 14.7 s, total: 1min 27s
Wall time: 1min 27s
</pre></div></div>
</div>
<p>学習が問題なく進めば，main/accuracyが0.99
(99%)付近まで到達していると思います．</p>
</div>
<div class="section" id="評価">
<h3>8.5.2. 評価<a class="headerlink" href="#評価" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>学習したモデルを評価用データに当てはめて識別性能を確認するため，以下の関数を用意します．</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">create_test_dataset()</span></code> : 評価用データの読み込み</li>
<li><code class="docutils literal notranslate"><span class="pre">predict()</span></code> :
推論を行い，結果の配列（正解ラベルと予測ラベル）を出力</li>
<li><code class="docutils literal notranslate"><span class="pre">print_confusion_matrix()</span></code> : 予測結果から混同行列とよばれる表を出力</li>
<li><code class="docutils literal notranslate"><span class="pre">print_scores()</span></code> : 予測結果から予測精度の評価指標を出力</li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer import cuda
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix


def create_test_dataset(root_path):
    test_path = os.path.join(root_path, &#39;preprocessed&#39;, &#39;test&#39;)
    test_dataset = ECGDataset(test_path)
    return test_dataset


def predict(trainer, test_dataset, batchsize, device=-1):
    model = trainer.updater.get_optimizer(&#39;main&#39;).target
    ys = []
    ts = []
    for i in range(len(test_dataset) // batchsize + 1):
        if i == len(test_dataset) // batchsize:
            X, t = zip(*test_dataset[i*batchsize: len(test_dataset)])
        else:
            X, t = zip(*test_dataset[i*batchsize:(i+1)*batchsize])
        X = cuda.to_gpu(np.array(X), device)
        y = model.predict(X)
        y = cuda.to_cpu(y.data.argmax(axis=1))
        ys.append(y)
        ts.append(np.array(t))
    return np.concatenate(ts), np.concatenate(ys)


def print_confusion_matrix(y_true, y_pred):
    labels = sorted(list(set(y_true)))
    target_names = [&#39;Normal&#39;, &#39;VEB&#39;]
    cmx = confusion_matrix(y_true, y_pred, labels=labels)
    df_cmx = pd.DataFrame(cmx, index=target_names, columns=target_names)
    plt.figure(figsize = (5,3))
    sn.heatmap(df_cmx, annot=True, annot_kws={&quot;size&quot;: 18}, fmt=&quot;d&quot;, cmap=&#39;Blues&#39;)
    plt.show()


def print_scores(y_true, y_pred):
    target_names = [&#39;Normal&#39;, &#39;VEB&#39;]
    print(classification_report(y_true, y_pred, target_names=target_names))
    print(&quot;accuracy: &quot;, accuracy_score(y_true, y_pred))

</pre></div>
</div>
</div>
<p>評価用データセットを用意し，</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>test_dataset = create_test_dataset(dataset_root)
</pre></div>
</div>
</div>
<p>評価用データに対して予測を行います． (17秒程度で予測が完了します)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.6 s, sys: 2.73 s, total: 18.3 s
Wall time: 18.3 s
</pre></div></div>
</div>
<p>それでは予測結果を確認していきましょう．</p>
<p>まずは， <strong>混同行列</strong>
とよばれる，予測の分類結果をまとめた表を作成します．行方向（表側）を正解ラベル，列方向（表頭）を予測ラベルとして，各項目では以下の集計値を求めています．</p>
<ul class="simple">
<li>左上 : 実際に正常拍動であるサンプルが，正常拍動と予測された数</li>
<li>右上 : 実際に正常拍動であるサンプルが，VEBと予測された数</li>
<li>左下 : 実際にVEBであるサンプルが，正常と予測された数</li>
<li>右下 : 実際にVEBであるサンプルが，VEBと予測された数</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_68_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_68_0.png" />
</div>
</div>
<p>続いて，予測結果から計算される予測精度の評価指標スコアを表示してみましょう．</p>
<p>特に，以下のスコアに注目してみてください．</p>
<ul class="simple">
<li>適合率 (Precision) : それぞれの予測診断結果 (Normal or VEB)
のうち，正しく診断できていた（正解も同じ診断結果であった）割合</li>
<li>再現率 (Recall) : それぞれの正解診断結果 (Normal or VEB)
のうち，正しく予測できていた（予測も同じ診断結果であった）割合</li>
<li>F1値 (F1-score) : 適合率と再現率の調和平均</li>
<li>正解率 (Accuracy) : 全ての診断結果 (Normal and VEB)
のうち，正しく予測できていた（予測も同じ診断結果であった）割合</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.94      0.96     42149
         VEB       0.52      0.90      0.66      3200

   micro avg       0.94      0.94      0.94     45349
   macro avg       0.76      0.92      0.81     45349
weighted avg       0.96      0.94      0.94     45349

accuracy:  0.9351694634942336
</pre></div></div>
</div>
<p>サンプル数が多い正常拍動に対する予測スコアは高い値を示す一方で，サンプル数の少ないVEBに対しては，スコアが低くなる傾向があります．今回のデータセットのように，サンプルが占めるクラスの割合が極端に偏っている不均衡データでは，こうした傾向がしばしば観測されることが知られています．</p>
<p>次節では，このようなクラス不均衡問題への対応をはじめとして，予測モデルを改善するための試行錯誤について幾つか紹介していきます．</p>
</div>
</div>
<div class="section" id="精度向上に向けて">
<h2>8.6. 精度向上に向けて<a class="headerlink" href="#精度向上に向けて" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>本節では，前節にて構築した学習器に対して，「データセット」「目的関数」「学習モデル」「前処理」といった様々な観点で工夫を行うことで，精度改善に寄与する方法を模索していきます．</p>
<p>機械学習を用いて解析を行う際には，どの工夫が精度改善に有効なのか予め分からない場合が多く，試行錯誤が必要となります．ただし，手当たり次第の方法を試すことは得策では無いので，対象とするデータセットの性質に基づいて，有効となり得る手段を検討していくことが重要となります．</p>
<p>まずはじめに，前節でも課題として挙がっていた，クラス不均衡の問題への対処法から検討していきましょう．</p>
<div class="section" id="クラス不均衡データへの対応">
<h3>8.6.1. クラス不均衡データへの対応<a class="headerlink" href="#クラス不均衡データへの対応" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>前節でも触れたように，<strong>クラス不均衡データ</strong>を用いて学習器を構築する際，大多数を占めるクラスに偏った予測結果となり，少数のクラスに対して精度が低くなってしまう場合があることが一般的に知られています．一方で，（今回のデータセットを含めて）現実世界のタスクにおいては，大多数の正常サンプルの中に含まれる少数の異常サンプルを精度良く検出することが重要であるというケースは少なくありません．こうした状況において，少数クラスの検出に注目してモデルを学習するための方策が幾つか存在します．</p>
<p>具体的には，</p>
<ol class="arabic simple">
<li><strong>サンプリング</strong><ul>
<li>不均衡データセットからサンプリングを行い，クラス比率のバランスが取れたデータセットを作成．<ul>
<li><strong>Undersampling</strong> : 大多数の正常サンプルを削減．</li>
<li><strong>Oversampling</strong> : 少数の異常サンプルを水増し．</li>
</ul>
</li>
</ul>
</li>
<li><strong>損失関数の重み調整</strong><ul>
<li>正常サンプルを異常と誤分類した際のペナルティを小さく，異常サンプルを正常と誤分類した際のペナルティを大きくする．</li>
<li>例えば，サンプル数の存在比率の逆数を重みとして利用．</li>
</ul>
</li>
<li><strong>目的関数(損失関数)の変更</strong><ul>
<li>異常サンプルに対する予測スコアを向上させるような目的関数を導入．</li>
</ul>
</li>
<li><strong>異常検知</strong><ul>
<li>正常サンプルのデータ分布を仮定し，そこから十分に逸脱したサンプルを異常とみなす．</li>
</ul>
</li>
</ol>
<p>などの方法があります．本節では，「1.サンプリング」，「3.目的関数の変更」の例を紹介していきます．</p>
<div class="section" id="サンプリング">
<h4>8.6.1.1. サンプリング<a class="headerlink" href="#サンプリング" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p><strong>Undersampling</strong>と<strong>Oversampling</strong>を組み合わせて，データセットの不均衡を解消することを考えます．</p>
<p>今回は以下のステップでサンプリングを行います．</p>
<ol class="arabic simple">
<li>Undersamplingにより，正常拍動サンプルのみ1/4に削減
(VEBサンプルは全て残す)<ul>
<li>ここでは，単純なランダムサンプリングを採用します．ランダム性があるため，分類にとって重要な（VEBサンプルとの識別境界付近にある）サンプルを削除してしまう可能性があります．</li>
<li>ランダムサンプリングの問題を緩和する手法も幾つか存在しますが，今回は使用しません．</li>
</ul>
</li>
<li>Oversamplingにより，Undersampling後の正常拍動サンプルと同数になるまでVEBサンプルを水増し<ul>
<li>SMOTE (Synthetic Minority Over-sampling TEchnique)
という手法を採用します．</li>
<li>ランダムにデータを水増しする最も単純な方法だと，過学習を引き起こしやすくなります．SMOTEでは，VEBサンプルと，その近傍VEBサンプルとの間のデータ点をランダムに生成してデータに追加していくことで，過学習の影響を緩和しています．</li>
</ul>
</li>
</ol>
<p>サンプリングを行うために， <code class="docutils literal notranslate"><span class="pre">SampledECGDataset</span></code> クラスを定義します．</p>
<p>また，そのクラスを読み込んで学習用データセットオブジェクトを作成する
<code class="docutils literal notranslate"><span class="pre">create_sampled_train_datset()</span></code> 関数を用意します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from imblearn.datasets import make_imbalance
from imblearn.over_sampling import SMOTE


class SampledECGDataset(ECGDataset):

    def __init__(
            self,
            path
    ):
        super(SampledECGDataset, self).__init__(path)
        _, counts = np.unique(self.y, return_counts=True)
        self.X, self.y = make_imbalance(
            self.X, self.y,
            sampling_strategy={0: counts[0]//4, 1: counts[1]}
        )
        smote = SMOTE(random_state=42)
        self.X, self.y = smote.fit_sample(self.X, self.y)


def create_sampled_train_dataset(root_path):
    train_path = os.path.join(root_path, &#39;preprocessed&#39;, &#39;train&#39;)
    train_dataset = SampledECGDataset(train_path)

    return train_dataset
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_sampled_train_dataset(dataset_root)
</pre></div>
</div>
</div>
<p>それでは先程と同様に，trainerを作成して学習を実行してみましょう．(1分程度で学習が完了します．)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=2, device=0)
%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.55422     0.71317
0           14          0.282043    0.902344
0           21          0.161596    0.939174
0           28          0.109996    0.960379
0           35          0.0750687   0.976004
0           42          0.0759563   0.969866
0           49          0.0531007   0.979911
0           56          0.0477525   0.984375
0           63          0.0657228   0.981585
0           70          0.0559838   0.979911
0           77          0.0410638   0.985491
0           84          0.0311444   0.989397
1           91          0.0260796   0.993304
1           98          0.0306111   0.990513
1           105         0.0267346   0.987165
1           112         0.0161048   0.995536
1           119         0.0202833   0.991629
1           126         0.0112008   0.998326
1           133         0.0164346   0.99442
1           140         0.0189331   0.992746
1           147         0.0153746   0.99442
1           154         0.0173289   0.994978
1           161         0.01027     0.996094
1           168         0.01294     0.995536
CPU times: user 55.1 s, sys: 13.4 s, total: 1min 8s
Wall time: 1min 8s
</pre></div></div>
</div>
<p>学習が完了したら，評価用データで予測を行い，精度を確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 2.91 s, total: 18 s
Wall time: 18 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_84_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_84_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       1.00      0.91      0.95     42149
         VEB       0.45      0.96      0.61      3200

   micro avg       0.91      0.91      0.91     45349
   macro avg       0.72      0.93      0.78     45349
weighted avg       0.96      0.91      0.93     45349

accuracy:  0.9144854351804891
</pre></div></div>
</div>
<p>先程の予測結果と比較して，サンプリングの効果によりVEBサンプルに対する検出精度（特にrecall）が向上しているかを確認してみて下さい．</p>
<p>(サンプリングのランダム性や，学習の初期値依存性などの影響があるため，必ず精度向上するとは限らないことにご注意下さい．)</p>
</div>
<div class="section" id="損失関数の変更">
<h4>8.6.1.2. 損失関数の変更<a class="headerlink" href="#損失関数の変更" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>続いて，<strong>損失関数を変更</strong>することで，少数の異常サンプルに対して精度向上させる方法を検討します．少数クラスの予測精度向上に注目した損失関数はこれまでに幾つも提案されていますが，今回はその中で，<strong>Focal
loss</strong> という損失関数を利用します．</p>
<p>Focal lossは，画像の物体検知手法の研究論文
[<a class="reference external" href="https://arxiv.org/abs/1708.02002">文献6</a>]
の中で提案された損失関数です．One-stage物体検知手法において，大量の候補領域の中で実際に物体が存在する領域はたかだか数個であることが多く，クラス不均衡なタスクになっており，学習がうまく進まないという問題があります．こうした問題に対処するために提案されたのがfocal
lossであり，以下の式によって記述されます．</p>
<div class="math notranslate nohighlight">
\[FL(p_t) = - (1 - p_t)^{\gamma}\log(p_t)\]</div>
<p>ここで<span class="math notranslate nohighlight">\(p_t\)</span>はSoftmax関数の出力（確率値）です．<span class="math notranslate nohighlight">\(\gamma = 0\)</span>の場合，通常のSoftmax
cross-entorpy
lossと等しくなりますが，<span class="math notranslate nohighlight">\(\gamma &gt; 0\)</span>の場合，明確に分類可能な（識別が簡単な）サンプルに対して，相対損失を小さくする効果があります．その結果，分類が難しいサンプルにより注目して学習が進んでいくことが期待されます．</p>
<p>下図は，正解クラスの予測確率値と，その際の損失の関係をプロットしており，<span class="math notranslate nohighlight">\(\gamma\)</span>の値を変化させた場合に，相対損失がどのように下がっていくかを示しています．</p>
<div class="figure" id="id18">
<img alt="正解予測確率と損失の関係" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/monitoring/focal_plot.png" />
<p class="caption"><span class="caption-text">正解予測確率と損失の関係</span></p>
</div>
<p>([<a class="reference external" href="https://arxiv.org/abs/1708.02002">文献6</a>]より引用)</p>
<p>それでは実際に，Focal loss関数を定義してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.backends.cuda import get_array_module

def focal_loss(x, t, class_num=2, gamma=0.5, eps=1e-6):
    xp = get_array_module(t)

    p = F.softmax(x)
    p = F.clip(p, x_min=eps, x_max=1-eps)
    log_p = F.log_softmax(x)
    t_onehot = xp.eye(class_num)[t.ravel()]

    loss_sce = -1 * t_onehot * log_p
    loss_focal = F.sum(loss_sce * (1. - p) ** gamma, axis=1)

    return F.mean(loss_focal)
</pre></div>
</div>
</div>
<p>前項目で実施したデータサンプリングは行わず，初期(§8.5)の学習時と同様の設定にした上で，損失関数をfocal
lossに変更します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0, lossfun=focal_loss)
</pre></div>
</div>
</div>
<p>それでは学習を開始しましょう．(1分30秒ほどで学習が完了します．)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.21684     0.876674
0           14          0.149564    0.957589
0           21          0.0536211   0.976004
0           28          0.0491485   0.979353
0           35          0.0278858   0.987165
0           42          0.0252027   0.989397
0           49          0.0311808   0.986607
0           56          0.0199696   0.989955
0           63          0.0132789   0.99442
0           70          0.0140394   0.996094
0           77          0.0144007   0.993304
0           84          0.0169302   0.992746
0           91          0.0165225   0.992746
0           98          0.0110192   0.996094
0           105         0.0177556   0.992746
0           112         0.0139628   0.994978
0           119         0.0113324   0.994978
0           126         0.010316    0.996094
0           133         0.0103831   0.995536
0           140         0.0128646   0.995536
0           147         0.0246793   0.992188
0           154         0.00787007  0.996652
0           161         0.0168202   0.994978
0           168         0.00875182  0.996652
0           175         0.015041    0.993862
0           182         0.00939075  0.99442
CPU times: user 1min 4s, sys: 13.7 s, total: 1min 17s
Wall time: 1min 17s
</pre></div></div>
</div>
<p>学習が完了したら，評価用データにて予測結果を確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 2.8 s, total: 17.9 s
Wall time: 17.9 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_97_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_97_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.95      0.97     42149
         VEB       0.57      0.93      0.71      3200

   micro avg       0.95      0.95      0.95     45349
   macro avg       0.78      0.94      0.84     45349
weighted avg       0.96      0.95      0.95     45349

accuracy:  0.9456437848684646
</pre></div></div>
</div>
<p>初期モデルの予測結果と，今回の予測結果を比較してみて下さい．</p>
<p>（余力があれば，<span class="math notranslate nohighlight">\(\gamma\)</span>の値を変化させた場合に，予測結果にどのような影響があるか確認してみて下さい．）</p>
</div>
</div>
<div class="section" id="ネットワーク構造の変更">
<h3>8.6.2. ネットワーク構造の変更<a class="headerlink" href="#ネットワーク構造の変更" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>続いて，学習に用いる<strong>ネットワーク構造を変更</strong>することを検討します．</p>
<p>ここでは，最初に用いたResNet34構造に対して以下の拡張を行います．</p>
<ol class="arabic simple">
<li>1D Convolutionを，<strong>1D Dilated Convolution</strong>に変更<ul>
<li>Dilated
Convolutionを用いることで，パラメータ数の増大を抑えながら，より広範囲の特徴を抽出可能になると期待されます（遺伝子解析の際と同様のモチベーション）．</li>
<li>広範囲の特徴が重要でないタスクの場合には，精度向上に繋がらない（または，場合によっては精度が低下する）可能性もあります．</li>
</ul>
</li>
<li>最終層の手前に全結合層を追加し，<strong>Dropout</strong>を適用<ul>
<li>Dropoutを行うことで，学習器の汎化性能が向上することを期待します．ただし複数の先行研究([<a class="reference external" href="https://arxiv.org/abs/1506.02158v6">文献7</a>]など)において，単純に畳み込み層の直後にDropoutを適用するだけでは汎化性能の向上が期待出来ないと報告されていることから，今回は全結合層に適用することにします．</li>
</ul>
</li>
</ol>
<p>それでは，上記の拡張を加えたネットワークを定義しましょう．（ResBlockクラスは，初期モデル構築時に定義済み）</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class DilatedResNet34(chainer.Chain):

    def __init__(self):
        super(DilatedResNet34, self).__init__()
        with self.init_scope():
            self.conv1 = L.ConvolutionND(1, None, 64, 7, 2, 3)
            self.bn1 = L.BatchNormalization(64)
            self.resblock0 = ResBlock(64, 3, 1)
            self.resblock1 = ResBlock(128, 4, 1)
            self.resblock2 = ResBlock(256, 6, 2)
            self.resblock3 = ResBlock(512, 3, 4)
            self.fc1 = L.Linear(None, 512)
            self.fc2 = L.Linear(None, 2)

    def __call__(self, x):
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.max_pooling_nd(h, 3, 2)
        for i in range(4):
            h = getattr(self, &#39;resblock{}&#39;.format(str(i)))(h)
        h = F.average(h, axis=2)
        h = F.dropout(self.fc1(h), 0.5)
        h = self.fc2(h)
        return h
</pre></div>
</div>
</div>
<p>初期(§8.5)の学習時と同様の設定にした上で，ネットワーク構造を
<code class="docutils literal notranslate"><span class="pre">DilatedResNet34</span></code> に変更して学習を行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def create_trainer(
    batchsize, train_dataset, nb_epoch=1,
    device=0, lossfun=F.softmax_cross_entropy
):
    # setup model
    model = DilatedResNet34()
    train_model = Classifier(model, lossfun=lossfun)

    # use Adam optimizer
    optimizer = optimizers.Adam(alpha=0.001)
    optimizer.setup(train_model)
    optimizer.add_hook(WeightDecay(0.0001))

    # setup iterator
    train_iter = MultiprocessIterator(train_dataset, batchsize)

    # define updater
    updater = training.StandardUpdater(train_iter, optimizer, device=device)

    # setup trainer
    stop_trigger = (nb_epoch, &#39;epoch&#39;)
    trainer = training.trainer.Trainer(updater, stop_trigger)
    logging_attributes = [
        &#39;epoch&#39;, &#39;iteration&#39;,
        &#39;main/loss&#39;, &#39;main/accuracy&#39;
    ]
    trainer.extend(
        extensions.LogReport(logging_attributes, trigger=(2000 // batchsize, &#39;iteration&#39;))
    )
    trainer.extend(
        extensions.PrintReport(logging_attributes)
    )
    trainer.extend(
        extensions.ExponentialShift(&#39;alpha&#39;, 0.75, optimizer=optimizer),
        trigger=(4000 // batchsize, &#39;iteration&#39;)
    )

    return trainer
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
test_dataset = create_test_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0)
</pre></div>
</div>
</div>
<p>それでは，これまでと同様に学習を開始しましょう．(1分30秒ほどで学習が完了します．)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.06941     0.860491
0           14          0.204161    0.924665
0           21          0.164622    0.932478
0           28          0.103986    0.958147
0           35          0.0782762   0.973772
0           42          0.0455737   0.988281
0           49          0.030971    0.989955
0           56          0.0431471   0.988839
0           63          0.0427462   0.985491
0           70          0.0333034   0.991629
0           77          0.0239444   0.993862
0           84          0.0325211   0.989397
0           91          0.0281632   0.991071
0           98          0.0222488   0.989955
0           105         0.0232916   0.992746
0           112         0.0210675   0.992746
0           119         0.00808897  0.997768
0           126         0.0198379   0.991629
0           133         0.0183009   0.99442
0           140         0.0103409   0.996652
0           147         0.0131249   0.995536
0           154         0.0128261   0.993862
0           161         0.0152106   0.996652
0           168         0.00777262  0.996652
0           175         0.0233488   0.993304
0           182         0.0204095   0.995536
CPU times: user 1min, sys: 14.3 s, total: 1min 14s
Wall time: 1min 14s
</pre></div></div>
</div>
<p>学習が完了したら，評価用データで予測を行い，精度を確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.1 s, sys: 2.8 s, total: 17.9 s
Wall time: 17.9 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_111_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_111_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.96      0.98     42149
         VEB       0.63      0.91      0.74      3200

   micro avg       0.96      0.96      0.96     45349
   macro avg       0.81      0.94      0.86     45349
weighted avg       0.97      0.96      0.96     45349

accuracy:  0.9553683653443296
</pre></div></div>
</div>
<p>初期モデルの予測結果と，今回の予測結果を比較してみて下さい．</p>
</div>
<div class="section" id="ノイズ除去の効果検証">
<h3>8.6.3. ノイズ除去の効果検証<a class="headerlink" href="#ノイズ除去の効果検証" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>最後に，心電図に含まれる<strong>ノイズの除去</strong>について検討します．</p>
<p>心電図波形には，以下のような外部ノイズが含まれている可能性があります．[<a class="reference external" href="http://www.iosrjournals.org/iosr-jece/papers/ICETEM/Vol.%201%20Issue%201/ECE%2006-40-44.pdf">文献8</a>]</p>
<ul class="simple">
<li>高周波<ul>
<li><strong>筋電図ノイズ</strong> (Electromyogram noise)<ul>
<li>体動により，筋肉の電気的活動が心電図に混入する場合があります．</li>
</ul>
</li>
<li><strong>電力線誘導障害</strong> (Power line interference)<ul>
<li>静電誘導により交流電流が流れ込み，心電図に混入する場合があります．</li>
<li>電源配線に電流が流れることで磁力線が発生し，電磁誘導作用により交流電流が流れ込む場合があります．</li>
</ul>
</li>
<li><strong>加算性白色ガウスノイズ</strong> (Additive white Gaussian noise)<ul>
<li>外部環境に由来する様々な要因でホワイトノイズが混入してきます．</li>
</ul>
</li>
</ul>
</li>
<li>低周波<ul>
<li><strong>基線変動</strong> (Baseline wandering)<ul>
<li>電極の装着不良，発汗，体動などの影響で，基線がゆっくり変動する場合があります．</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>心電図を解析する際は，頻脈や徐脈などの異常波形を正確に判別するために，上記のようなノイズを除去する前処理が行われるのが一般的です．</p>
<p>ノイズを除去する方法は幾つかありますが，最も単純なのは，線形フィルタを適用する方法です．今回は線形フィルタの一つであるバターワースフィルタを用いて，ノイズ除去を試してみましょう．</p>
<p><code class="docutils literal notranslate"><span class="pre">BaseECGDatasetPreprocessor</span></code> にシグナルノイズ除去の機能を追加した，
<code class="docutils literal notranslate"><span class="pre">DenoiseECGDatasetPreprocessor</span></code> クラスを定義します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from scipy.signal import butter, lfilter


class DenoiseECGDatasetPreprocessor(BaseECGDatasetPreprocessor):

    def __init__(
            self,
            dataset_root=&#39;./&#39;,
            window_size=720
    ):
        super(DenoiseECGDatasetPreprocessor, self).__init__(
        dataset_root, window_size)

    def _denoise_signal(
            self,
            signal,
            btype=&#39;low&#39;,
            cutoff_low=0.2,
            cutoff_high=25.,
            order=5
    ):
        nyquist = self.sample_rate / 2.
        if btype == &#39;band&#39;:
            cut_off = (cutoff_low / nyquist, cutoff_high / nyquist)
        elif btype == &#39;high&#39;:
            cut_off = cutoff_low / nyquist
        elif btype == &#39;low&#39;:
            cut_off = cutoff_high / nyquist
        else:
            return signal
        b, a = butter(order, cut_off, analog=False, btype=btype)
        return lfilter(b, a, signal)

    def _segment_data(
            self,
            signal,
            symbols,
            positions
    ):
        X = []
        y = []
        sig_len = len(signal)
        for i in range(len(symbols)):
            start = positions[i] - self.window_size // 2
            end = positions[i] + self.window_size // 2
            if symbols[i] in self.valid_symbols and start &gt;= 0 and end &lt;= sig_len:
                segment = signal[start:end]
                assert len(segment) == self.window_size, &quot;Invalid length&quot;
                X.append(segment)
                y.append(self.labels.index(self.label_map[symbols[i]]))
        return np.array(X), np.array(y)

    def prepare_dataset(
            self,
            denoise=False,
            normalize=True
    ):
        if not os.path.isdir(self.download_dir):
            self.download_data()

        # prepare training dataset
        self._prepare_dataset_core(self.train_record_list, &quot;train&quot;, denoise, normalize)
        # prepare test dataset
        self._prepare_dataset_core(self.test_record_list, &quot;test&quot;, denoise, normalize)

    def _prepare_dataset_core(
            self,
            record_list,
            mode=&quot;train&quot;,
            denoise=False,
            normalize=True
    ):
        Xs, ys = [], []
        save_dir = os.path.join(self.dataset_root, &#39;preprocessed&#39;, mode)
        for i in range(len(record_list)):
            signal, symbols, positions = self._load_data(record_list[i])
            if denoise:
                signal = self._denoise_signal(signal)
            if normalize:
                signal = self._normalize_signal(signal)
            X, y = self._segment_data(signal, symbols, positions)
            Xs.append(X)
            ys.append(y)
        os.makedirs(save_dir, exist_ok=True)
        np.save(os.path.join(save_dir, &quot;X.npy&quot;), np.vstack(Xs))
        np.save(os.path.join(save_dir, &quot;y.npy&quot;), np.concatenate(ys))

</pre></div>
</div>
</div>
<p>線形フィルタを適用することで，学習モデルが異常拍動のパターンを特徴として捉えやすくなる可能性があります．一方で，異常拍動を検出するにあたって重要な情報も除去されてしまう可能性があることに注意してください．</p>
<p>また，線形フィルタにおいては，その周波数特性（どの帯域の周波数成分を遮断するか）によって，幾つかの大まかな分類があります．例えば，以下のものがあります．</p>
<ul class="simple">
<li><strong>ローパスフィルタ (Low-pass filter)</strong> : 低周波成分のみ通過
(高周波成分を遮断)</li>
<li><strong>ハイパスフィルタ (High-pass filter)</strong> : 高周波成分のみ通過
(低周波成分を遮断)</li>
<li><strong>バンドパスフィルタ(Band-pass filter)</strong> : 特定の帯域成分のみ通過
(低周波，高周波成分を遮断)</li>
</ul>
<div class="figure" id="id19">
<img alt="線形フィルタの周波数特性による分類" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/monitoring/band_form.png" />
<p class="caption"><span class="caption-text">線形フィルタの周波数特性による分類</span></p>
</div>
<p>([<a class="reference external" href="https://en.wikipedia.org/wiki/Filter_%28signal_processing%29">文献9</a>]
より引用)</p>
<p>mitdbでは，予め0.1 Hz 以下の低周波と，100 Hz
以上の高周波をバンドパスフィルタによって除去済みであるため，ここではさらに，25
Hz のローパス・バターワースフィルタによって高周波ノイズを取り除きます．</p>
<p>それでは，ノイズ除去オプションを有効にして，前処理を実行してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>DenoiseECGDatasetPreprocessor(dataset_root).prepare_dataset(denoise=True)
</pre></div>
</div>
</div>
<p>実際に，高周波ノイズ除去後の波形を可視化してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train_d = np.load(os.path.join(dataset_root, &#39;preprocessed&#39;, &#39;train&#39;, &#39;X.npy&#39;))
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.subplots(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(X_train[idx_n[0]])
plt.subplot(1, 2, 2)
plt.plot(X_train_d[idx_n[0]])
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_124_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_124_0.png" />
</div>
</div>
<p>左図がフィルタリング前の波形，右図がフィルタリング後の波形です．
細かな振動が取り除かれていることが確認できると思います．</p>
<p>これまでと同様に，ノイズ除去後のデータを用いて学習を行ってみましょう．(1分30秒ほどで学習が完了します．)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = create_train_dataset(dataset_root)
test_dataset = create_test_dataset(dataset_root)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>trainer = create_trainer(256, train_dataset, nb_epoch=1, device=0)
%time trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   main/loss   main/accuracy
0           7           1.21514     0.851562
0           14          0.283428    0.951451
0           21          0.157269    0.977679
0           28          0.141721    0.962612
0           35          0.0516044   0.986049
0           42          0.068243    0.987723
0           49          0.0541831   0.987723
0           56          0.0363404   0.993304
0           63          0.0441804   0.992188
0           70          0.0271119   0.991629
0           77          0.00849581  0.99721
0           84          0.0216285   0.996652
0           91          0.034815    0.990513
0           98          0.0259185   0.991629
0           105         0.0240092   0.993862
0           112         0.00988215  0.99721
0           119         0.0122308   0.996094
0           126         0.00706834  0.998326
0           133         0.0196008   0.994978
0           140         0.0134032   0.996652
0           147         0.0330729   0.993304
0           154         0.0230272   0.993304
0           161         0.00712667  0.99721
0           168         0.0140302   0.996094
0           175         0.00879468  0.995536
0           182         0.00927302  0.99721
CPU times: user 1min, sys: 13.8 s, total: 1min 14s
Wall time: 1min 13s
</pre></div></div>
</div>
<p>学習が完了したら，評価用データで予測を行い，精度を確認してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%time y_true_test, y_pred_test = predict(trainer, test_dataset, 256, 0)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.2 s, sys: 2.81 s, total: 18 s
Wall time: 18 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_confusion_matrix(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_130_0.png" src="../_images/notebooks_Sequential_Data_Analysis_with_Deep_Learning_130_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print_scores(y_true_test, y_pred_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      Normal       0.99      0.97      0.98     42149
         VEB       0.67      0.93      0.78      3200

   micro avg       0.96      0.96      0.96     45349
   macro avg       0.83      0.95      0.88     45349
weighted avg       0.97      0.96      0.97     45349

accuracy:  0.9632185935742795
</pre></div></div>
</div>
<p>高周波のノイズを除去したことで，予測精度がどのように変わったか確認してみましょう．</p>
</div>
</div>
<div class="section" id="おわりに">
<h2>8.7. おわりに<a class="headerlink" href="#おわりに" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>本章では，ECGの公開データセットを利用して，不整脈検知の問題に取り組みました．</p>
<p>本講義内容を通じてお伝えしたかったことは，以下となります．</p>
<ol class="arabic simple">
<li>心電図を解析するにあたって必要となる最低限の知識</li>
<li>モニタリングデータを解析するための基本的な前処理手順</li>
<li>CNNベースのモデルを利用した学習器の構築</li>
<li>データセットの性質を考慮した学習方法や前処理の工夫</li>
</ol>
<p>また，精度向上に向けて様々な手法を試してみましたが，現実世界のタスクにおいては，どの工夫が有効に働くか自明で無い場合がほとんどです．従って，試行錯誤を行いながら，その問題設定に適合するやり方を模索していく必要があります．</p>
<p>さらなる取り組みとしては，例えば下記内容を検討する余地があります．</p>
<ul class="simple">
<li>情報の追加<ul>
<li><span class="math notranslate nohighlight">\(Ⅱ\)</span>誘導シグナルに加えて，<span class="math notranslate nohighlight">\(V_1\)</span>誘導シグナルも同時に入力として与えます．([<a class="reference external" href="https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_16.pdf">文献10</a>]などで実施)</li>
</ul>
</li>
<li>前処理の工夫<ul>
<li>セグメント長の変更<ul>
<li>より長時間のセグメントを入力とすることで，長期的な波形情報を抽出．([<a class="reference external" href="https://arxiv.org/abs/1810.04121">文献4</a>]では10秒のセグメントを解析に利用)</li>
<li>入力情報が増えることで，却って学習が難しくなってしまう可能性あり．</li>
</ul>
</li>
<li>リサンプリング<ul>
<li>サンプリング周波数を下げることで，長期的な波形情報を抽出．([<a class="reference external" href="https://arxiv.org/abs/1810.04121">文献4</a>]では180
Hzにダウンサンプリング)<ul>
<li>波形が粗くなることで学習に影響する可能性あり．</li>
<li>適切な前処理を行わないと，折り返し雑音と呼ばれる歪みが発生．</li>
<li>（モデルに入力する前に情報を縮小する処理は，画像解析などの分野では一般的）</li>
</ul>
</li>
</ul>
</li>
<li>ラベルの追加<ul>
<li>Normal，VEBに加えて，SVEB（上室異所性拍動）等も追加．</li>
</ul>
</li>
<li>ラベルの与え方の変更<ul>
<li>セグメント範囲内に正常以外のピークラベルが含まれる場合に優先的にそのラベルを付与する，等．</li>
</ul>
</li>
</ul>
</li>
<li>モデルの変更<ul>
<li>長期的な特徴を抽出するために，CNNの後段にRNNベースの構造(LSTMなど)を組み込む
([<a class="reference external" href="https://arxiv.org/abs/1810.04121">文献4</a>]などで実施)．</li>
</ul>
</li>
</ul>
<p>余力がある方は，是非チャレンジしてみてください．</p>
<p>また，最近では独自に収集した大規模なモニタリングデータを対象として，研究成果を発表する事例も幾つか出てきています．</p>
<ul class="simple">
<li>Cardiogram社とカリフォルニア大学の共同研究で，活動量計から心拍数データを収集し，深層学習を用いて糖尿病予備群を予測するDeepHeartを発表[<a class="reference external" href="https://arxiv.org/abs/1802.02511">文献11</a>]．</li>
<li>スタンフォード大学のAndrew
Ng.の研究室でも，独自に収集したECGレコードから<span class="math notranslate nohighlight">\(14\)</span>種類の波形クラス分類を予測するモデルを構築し，医師と比較実験を実施[<a class="reference external" href="https://arxiv.org/abs/1707.01836">文献12</a>]．</li>
</ul>
<p>デバイスの進歩によって簡単に精緻な情報が収集可能になってきていることから，こうした研究は今後益々盛んになっていくと考えられます．</p>
<p>以上で，モニタリングデータの時系列解析の章は終了となります．お疲れ様でした．</p>
</div>
<div class="section" id="参考文献">
<h2>8.8. 参考文献<a class="headerlink" href="#参考文献" title="このヘッドラインへのパーマリンク">¶</a></h2>
<ol class="arabic simple">
<li><strong>Electrocardiography</strong> Wikipedia: The Free Encyclopedia. Wikimedia
Foundation, Inc.&nbsp;22 July 2004. Web. 10 Aug.&nbsp;2004,
[<a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">Link</a>]</li>
<li><strong>心電図健診判定マニュアル</strong>, 日本人間ドック学会, 平成26年4月,
[<a class="reference external" href="https://www.ningen-dock.jp/wp/wp-content/uploads/2013/09/d4bb55fcf01494e251d315b76738ab40.pdf">Link</a>]</li>
<li><strong>Automatic classification of heartbeats using ECG morphology and
heartbeat interval features</strong>, Phillip de Chazal et al., June 2004,
[<a class="reference external" href="https://ieeexplore.ieee.org/document/1306572">Link</a>]</li>
<li><strong>Inter-Patient ECG Classification with Convolutional and Recurrent
Neural Networks</strong>, Li Guo et al., Sep 2018,
[<a class="reference external" href="https://arxiv.org/abs/1810.04121">Link</a>]</li>
<li><strong>Deep Residual Learning for Image Recognition</strong>, Kaiming He et al.,
Dec 2015, [<a class="reference external" href="https://arxiv.org/abs/1512.03385">Link</a>]</li>
<li><strong>Focal Loss for Dense Object Detection</strong>, Tsung-Yi Lin et al., Aug
2017, [<a class="reference external" href="https://arxiv.org/abs/1708.02002">Link</a>]</li>
<li><strong>Bayesian Convolutional Neural Networks with Bernoulli Approximate
Variational Inference</strong>, Yarin Gal et al., Jun 2015,
[<a class="reference external" href="https://arxiv.org/abs/1506.02158v6">Link</a>]</li>
<li><strong>Noise Analysis and Different Denoising Techniques of ECG Signal -
A Survey</strong>, Aswathy Velayudhan et al., ICETEM2016,
[<a class="reference external" href="http://www.iosrjournals.org/iosr-jece/papers/ICETEM/Vol.%201%20Issue%201/ECE%2006-40-44.pdf">Link</a>]</li>
<li><strong>Filter (signal processing)</strong>, Wikipedia: The Free Encyclopedia.
Wikimedia Foundation, Inc.&nbsp;22 July 2004. Web. 10 Aug.&nbsp;2004,
[<a class="reference external" href="https://en.wikipedia.org/wiki/Filter_%28signal_processing%29">Link</a>]</li>
<li><strong>Arrhythmia Detection from 2-lead ECG using Convolutional Denoising
Autoencoders</strong>, Keiichi Ochiai et al., KDD2018,
[<a class="reference external" href="https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_16.pdf">Link</a>]</li>
<li><strong>DeepHeart: Semi-Supervised Sequence Learning for Cardiovascular
Risk Prediction</strong>, Brandon Ballinger et al., Feb 2018,
[<a class="reference external" href="https://arxiv.org/abs/1802.02511">Link</a>]</li>
<li><strong>Cardiologist-Level Arrhythmia Detection with Convolutional Neural
Networks</strong>, Pranav Rajpurkar et al., Jul 2017,
[<a class="reference external" href="https://arxiv.org/abs/1707.01836">Link</a>]</li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="DNA_Sequence_Data_Analysis.html" class="btn btn-neutral" title="7. 実践編: ディープラーニングを使った配列解析" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>